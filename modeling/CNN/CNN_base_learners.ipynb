{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e99154b4-96a2-460b-b067-678bb4749a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\" # set before any torch imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DL_DIR = \"../../data/deep_learning\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d61da12-cc45-4132-a810-f652dcdeb478",
   "metadata": {},
   "source": [
    "### Function for setting the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa95d45-f10f-4cf3-977b-9b7ed3d4f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    \n",
    "    \n",
    "    return seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e237af3d-92ea-4cc1-9d16-05817b2e5060",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb99506-5747-460d-8932-3ad53439ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the regression split dictionary.\n",
    "with open(f'{DL_DIR}/comb_reg_dict.pkl', 'rb') as f:\n",
    "    comb_reg_dict = pickle.load(f)\n",
    "\n",
    "with open(f'{DL_DIR}/fitbit_reg_dict.pkl', 'rb') as f:\n",
    "    fitbit_reg_dict = pickle.load(f)\n",
    "\n",
    "# Load the classification split dictionary.\n",
    "with open(f'{DL_DIR}/comb_class_dict.pkl', 'rb') as f:\n",
    "    comb_class_dict = pickle.load(f)\n",
    "\n",
    "with open(f'{DL_DIR}/fitbit_class_dict.pkl', 'rb') as f:\n",
    "    fitbit_class_dict = pickle.load(f) \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa76d33-9179-4ff3-be86-05d2239cbb2a",
   "metadata": {},
   "source": [
    "### Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2919c232-f541-417d-88cb-19c33d548e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNRegression(nn.Module):\n",
    "    def __init__(self, input_channels, seq_len, dropout_prob=0.5):\n",
    "        super(CNNRegression, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=input_channels, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(16 * seq_len, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)             # shape: [batch, 16, seq_len]\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)          # apply dropout here\n",
    "        x = x.view(x.size(0), -1)      # flatten: [batch, 16*seq_len]\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CNNClassification(nn.Module):\n",
    "    def __init__(self, input_channels, seq_len, dropout_prob=0.5):\n",
    "        super(CNNClassification, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=input_channels, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(16 * seq_len, 1)  # single logit for binary classification\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)          # apply dropout here\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.fc(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742ece70-1a75-4685-b510-52e378388c74",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f4fce5-8c85-4af2-9241-142b9ca11f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subject_dataset(df, outcome_col=\"SI_mean\"):\n",
    "    \"\"\"\n",
    "    Aggregates records for each subject into a subject-level sample.\n",
    "    Excludes meta/outcome columns and returns a DataFrame with:\n",
    "      - PatientID, outcome (SI_mean or is_SI), sample_weight,\n",
    "      - X: a numpy array of predictors with shape (n_features, 39).\n",
    "    \n",
    "    This function assumes that each subject already has exactly 39 timepoints.\n",
    "    For classification (when outcome_col==\"is_SI\"), if the original DataFrame contains a SI_mean column, it is included.\n",
    "    A stratification column is created by rounding the outcome (used for later splitting).\n",
    "    \"\"\"\n",
    "    exclude_cols = [\"PatientID\", \"timepoints\", \"si_kde_weight\", \"SI_mean\", \"is_SI\", \"SI_level\"]\n",
    "    predictor_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    subject_data = []\n",
    "    for pid, group in df.groupby(\"PatientID\"):\n",
    "        group_sorted = group.sort_values(\"timepoints\")\n",
    "        # Assume each subject already has exactly 39 timepoints.\n",
    "        X = group_sorted[predictor_cols].values.T  # shape: (n_features, 39)\n",
    "        y = group_sorted[outcome_col].iloc[0]\n",
    "        weight = group_sorted[\"si_kde_weight\"].iloc[0] if \"si_kde_weight\" in group.columns else 1.0\n",
    "        record = {\"PatientID\": pid, \"X\": X, outcome_col: y, \"sample_weight\": weight}\n",
    "        if outcome_col == \"is_SI\" and \"SI_mean\" in group_sorted.columns:\n",
    "            record[\"SI_mean\"] = group_sorted[\"SI_mean\"].iloc[0]\n",
    "        subject_data.append(record)\n",
    "    subj_df = pd.DataFrame(subject_data)\n",
    "    subj_df[f\"{outcome_col}_bin\"] = np.round(subj_df[outcome_col]).astype(int)\n",
    "    return subj_df, predictor_cols\n",
    "\n",
    "def get_stratified_cv_splits(df, subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5):\n",
    "    \"\"\"\n",
    "    Performs stratified K-fold cross validation at the subject level.\n",
    "    \n",
    "    Parameters:\n",
    "      df : pandas.DataFrame\n",
    "          The original dataframe containing repeated measures.\n",
    "      subject_id : str\n",
    "          The column name for the subject ID (e.g., \"PatientID\").\n",
    "      target_var : str\n",
    "          The target variable; for regression use \"SI_mean\" and for classification use \"is_SI\".\n",
    "      n_splits : int\n",
    "          Number of folds for cross validation.\n",
    "    \n",
    "    Returns:\n",
    "      splits : list of tuples\n",
    "          A list where each element is a tuple (train_df, test_df) corresponding\n",
    "          to one fold. Each dataframe contains all rows (i.e. repeated measures) for the patients in that fold.\n",
    "    \n",
    "    Behavior:\n",
    "      - Isolates unique patient IDs and their target variable by dropping duplicates.\n",
    "      - If target_var is \"SI_mean\", creates a new column \"SI_mean_levels\" (rounded SI_mean).\n",
    "      - Uses the resulting column as the stratification column.\n",
    "      - Performs stratified K-fold CV and then subsets the original dataframe based on the patient IDs.\n",
    "    \"\"\"\n",
    "    # Create a subject-level dataframe (unique patient IDs with their target variable)\n",
    "    subject_df = df[[subject_id, target_var]].drop_duplicates(subset=[subject_id]).copy()\n",
    "    \n",
    "    # For regression: create a new column with the rounded SI_mean values.\n",
    "    if target_var == \"SI_mean\":\n",
    "        subject_df[\"SI_mean_levels\"] = subject_df[target_var].round().astype(int)\n",
    "        strat_col = \"SI_mean_levels\"\n",
    "    else:\n",
    "        strat_col = target_var  # For classification, use the target directly.\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    splits = []\n",
    "    \n",
    "    # Get the subject IDs and stratification labels\n",
    "    subjects = subject_df[subject_id].values\n",
    "    strat_labels = subject_df[strat_col].values\n",
    "    \n",
    "    # For each fold, retrieve patient IDs and then subset the original dataframe.\n",
    "    for train_idx, test_idx in skf.split(subjects, strat_labels):\n",
    "        train_patient_ids = subject_df.iloc[train_idx][subject_id].values\n",
    "        test_patient_ids  = subject_df.iloc[test_idx][subject_id].values\n",
    "        train_split = df[df[subject_id].isin(train_patient_ids)]\n",
    "        test_split  = df[df[subject_id].isin(test_patient_ids)]\n",
    "        splits.append((train_split, test_split))\n",
    "    \n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e3e55-b2f5-4c53-b724-2c40ed00b6d9",
   "metadata": {},
   "source": [
    "### SI_mean regression CNN base learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a9050b3-de3c-4b47-afdc-65b7af17a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_learner_SI_mean(data_dict, model_name, num_epochs, batch_size, use_sample_weights=False):\n",
    "    \"\"\"\n",
    "    Trains a CNN regression model to predict SI_mean.\n",
    "    Uses subject-level training and testing DataFrames provided in data_dict.\n",
    "    Each split is processed with create_subject_dataset (using outcome_col=\"SI_mean\")\n",
    "    so that each subject has an \"X\" array of shape (n_features, timepoints).\n",
    "\n",
    "    If use_sample_weights=True, sample-level weights are used during training.\n",
    "    (Weights are not used during evaluation.)\n",
    "\n",
    "    Also performs 5-fold stratified cross validation (based on PatientID and SI_mean_levels)\n",
    "    on the training set and returns the mean and standard deviation of the RMSE (both overall and per-bin),\n",
    "    and now also includes the overall MSE.\n",
    "\n",
    "    Returns two DataFrames:\n",
    "      1. metrics_df: with columns: model, data, \"1\", \"2\", \"3\", \"4\", \"5\", overall \n",
    "         for the full training and test splits.\n",
    "      2. cv_metrics_df: with columns: stat, model, \"1\", \"2\", \"3\", \"4\", \"5\", overall, mse.\n",
    "         There will be one row for the mean RMSE/MSE values (stat = \"mean\") and one row for the standard deviation\n",
    "         values (stat = \"sd\"), computed via 5-fold stratified cross validation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Process the pre-split training and testing sets.\n",
    "    # (Assumes create_subject_dataset returns one row per PatientID.)\n",
    "    train_df, _ = create_subject_dataset(data_dict['train'], outcome_col=\"SI_mean\")\n",
    "    test_df, _  = create_subject_dataset(data_dict['test'], outcome_col=\"SI_mean\")\n",
    "    \n",
    "    X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "    y_train = train_df[\"SI_mean\"].values\n",
    "    if use_sample_weights:\n",
    "        w_train = train_df[\"sample_weight\"].values\n",
    "    else:\n",
    "        w_train = np.ones_like(y_train, dtype=np.float32)\n",
    "    \n",
    "    X_test = np.stack(test_df[\"X\"].values, axis=0)\n",
    "    y_test = test_df[\"SI_mean\"].values\n",
    "    \n",
    "    input_channels = X_train.shape[1]\n",
    "    seq_len = X_train.shape[2]\n",
    "    \n",
    "    # --- Full Training on the Provided Training Set ---\n",
    "    model = CNNRegression(input_channels, seq_len).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).view(-1, 1),\n",
    "        torch.tensor(w_train, dtype=torch.float32)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    model.train()\n",
    "    for ep in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch, weight_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            weight_batch = weight_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            # Use sample weights during training if specified.\n",
    "            loss_per_sample = loss_fn(outputs, y_batch).view(-1)\n",
    "            loss = (loss_per_sample * weight_batch).mean() if use_sample_weights else loss_per_sample.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * X_batch.size(0)\n",
    "        print(f\"SI_mean - Epoch {ep+1}/{num_epochs}, Loss: {epoch_loss/len(train_dataset):.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        train_preds = model(X_train_tensor).cpu().numpy()\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        test_preds = model(X_test_tensor).cpu().numpy()\n",
    "        \n",
    "        overall_train_rmse = np.sqrt(np.mean((train_preds - y_train.reshape(-1, 1))**2))\n",
    "        overall_test_rmse  = np.sqrt(np.mean((test_preds - y_test.reshape(-1, 1))**2))\n",
    "    \n",
    "    train_bins = np.round(y_train).astype(int)\n",
    "    test_bins  = np.round(y_test).astype(int)\n",
    "    bin_rmse_train = {}\n",
    "    for b in range(1, 6):\n",
    "        idx = np.where(train_bins == b)[0]\n",
    "        bin_rmse_train[str(b)] = np.sqrt(np.mean((train_preds[idx] - y_train[idx].reshape(-1, 1))**2)) if len(idx) > 0 else np.nan\n",
    "    bin_rmse_test = {}\n",
    "    for b in range(1, 6):\n",
    "        idx = np.where(test_bins == b)[0]\n",
    "        bin_rmse_test[str(b)] = np.sqrt(np.mean((test_preds[idx] - y_test[idx].reshape(-1, 1))**2)) if len(idx) > 0 else np.nan\n",
    "    \n",
    "    train_metrics = {\"model\": model_name, \"data\": \"train\"}\n",
    "    for b in range(1, 6):\n",
    "        train_metrics[str(b)] = bin_rmse_train.get(str(b), np.nan)\n",
    "    train_metrics[\"overall\"] = overall_train_rmse\n",
    "    \n",
    "    test_metrics = {\"model\": model_name, \"data\": \"test\"}\n",
    "    for b in range(1, 6):\n",
    "        test_metrics[str(b)] = bin_rmse_test.get(str(b), np.nan)\n",
    "    test_metrics[\"overall\"] = overall_test_rmse\n",
    "    \n",
    "    metrics_df = pd.DataFrame([train_metrics, test_metrics])\n",
    "    \n",
    "    # --- 5-fold Stratified Cross Validation on the Training Set ---\n",
    "    # Use the helper function to obtain stratified splits (at the subject level).\n",
    "    cv_splits = get_stratified_cv_splits(train_df, subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5)\n",
    "    cv_results = []\n",
    "    for cv_train_df, cv_val_df in cv_splits:\n",
    "        # Build arrays from the subject-level CV splits.\n",
    "        X_cv_train = np.stack(cv_train_df[\"X\"].values, axis=0)\n",
    "        y_cv_train = cv_train_df[\"SI_mean\"].values\n",
    "        if use_sample_weights:\n",
    "            w_cv_train = cv_train_df[\"sample_weight\"].values\n",
    "        else:\n",
    "            w_cv_train = np.ones_like(y_cv_train, dtype=np.float32)\n",
    "        X_cv_val   = np.stack(cv_val_df[\"X\"].values, axis=0)\n",
    "        y_cv_val   = cv_val_df[\"SI_mean\"].values\n",
    "        \n",
    "        # Initialize and train a fresh model on this fold.\n",
    "        model_cv = CNNRegression(input_channels, seq_len).to(device)\n",
    "        optimizer_cv = optim.Adam(model_cv.parameters(), lr=0.001)\n",
    "        train_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_train, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_train, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(w_cv_train, dtype=torch.float32)\n",
    "        )\n",
    "        train_loader_cv = torch.utils.data.DataLoader(train_dataset_cv, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        \n",
    "        model_cv.train()\n",
    "        for ep in range(num_epochs):\n",
    "            for X_batch, y_batch, weight_batch in train_loader_cv:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                weight_batch = weight_batch.to(device)\n",
    "                optimizer_cv.zero_grad()\n",
    "                outputs = model_cv(X_batch)\n",
    "                # Use sample weights during training if specified.\n",
    "                loss_per_sample = loss_fn(outputs, y_batch).view(-1)\n",
    "                loss = (loss_per_sample * weight_batch).mean() if use_sample_weights else loss_per_sample.mean()\n",
    "                loss.backward()\n",
    "                optimizer_cv.step()\n",
    "        model_cv.eval()\n",
    "        with torch.no_grad():\n",
    "            X_cv_val_tensor = torch.tensor(X_cv_val, dtype=torch.float32).to(device)\n",
    "            val_preds = model_cv(X_cv_val_tensor).cpu().numpy()\n",
    "        overall_cv_rmse = np.sqrt(np.mean((val_preds - y_cv_val.reshape(-1, 1))**2))\n",
    "        overall_cv_mse = np.mean((val_preds - y_cv_val.reshape(-1, 1))**2)  # <-- Compute overall MSE\n",
    "        \n",
    "        cv_bin_rmse = {}\n",
    "        cv_bins = np.round(y_cv_val).astype(int)\n",
    "        for b in range(1, 6):\n",
    "            idx = np.where(cv_bins == b)[0]\n",
    "            cv_bin_rmse[str(b)] = np.sqrt(np.mean((val_preds[idx] - y_cv_val[idx].reshape(-1, 1))**2)) if len(idx) > 0 else np.nan\n",
    "        cv_results.append({\n",
    "            \"overall\": overall_cv_rmse,\n",
    "            \"1\": cv_bin_rmse[\"1\"],\n",
    "            \"2\": cv_bin_rmse[\"2\"],\n",
    "            \"3\": cv_bin_rmse[\"3\"],\n",
    "            \"4\": cv_bin_rmse[\"4\"],\n",
    "            \"5\": cv_bin_rmse[\"5\"],\n",
    "            \"mse\": overall_cv_mse  # <-- Add mse to each fold's results\n",
    "        })\n",
    "    \n",
    "    cv_results_df = pd.DataFrame(cv_results)\n",
    "    cv_mean = cv_results_df.mean()\n",
    "    cv_std  = cv_results_df.std()\n",
    "    \n",
    "    # Create two rows: one for mean and one for standard deviation, including mse.\n",
    "    mean_row = {\n",
    "        \"stat\": \"mean\",\n",
    "        \"model\": model_name,\n",
    "        \"1\": cv_mean[\"1\"],\n",
    "        \"2\": cv_mean[\"2\"],\n",
    "        \"3\": cv_mean[\"3\"],\n",
    "        \"4\": cv_mean[\"4\"],\n",
    "        \"5\": cv_mean[\"5\"],\n",
    "        \"overall\": cv_mean[\"overall\"],\n",
    "        \"mse\": cv_mean[\"mse\"]\n",
    "    }\n",
    "    sd_row = {\n",
    "        \"stat\": \"sd\",\n",
    "        \"model\": model_name,\n",
    "        \"1\": cv_std[\"1\"],\n",
    "        \"2\": cv_std[\"2\"],\n",
    "        \"3\": cv_std[\"3\"],\n",
    "        \"4\": cv_std[\"4\"],\n",
    "        \"5\": cv_std[\"5\"],\n",
    "        \"overall\": cv_std[\"overall\"],\n",
    "        \"mse\": cv_std[\"mse\"]\n",
    "    }\n",
    "    cv_metrics_df = pd.DataFrame([mean_row, sd_row])\n",
    "    \n",
    "    return metrics_df, cv_metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f82dd02-cf1c-45cb-b42e-fe11ebef58c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SI_mean - Epoch 1/10, Loss: 3.8071\n",
      "SI_mean - Epoch 2/10, Loss: 1.4165\n",
      "SI_mean - Epoch 3/10, Loss: 1.2454\n",
      "SI_mean - Epoch 4/10, Loss: 1.1587\n",
      "SI_mean - Epoch 5/10, Loss: 1.0834\n",
      "SI_mean - Epoch 6/10, Loss: 0.9869\n",
      "SI_mean - Epoch 7/10, Loss: 0.8913\n",
      "SI_mean - Epoch 8/10, Loss: 0.7679\n",
      "SI_mean - Epoch 9/10, Loss: 0.9233\n",
      "SI_mean - Epoch 10/10, Loss: 0.8546\n",
      "SI_mean - Epoch 1/10, Loss: 0.5542\n",
      "SI_mean - Epoch 2/10, Loss: 0.3895\n",
      "SI_mean - Epoch 3/10, Loss: 0.3796\n",
      "SI_mean - Epoch 4/10, Loss: 0.3626\n",
      "SI_mean - Epoch 5/10, Loss: 0.3479\n",
      "SI_mean - Epoch 6/10, Loss: 0.3365\n",
      "SI_mean - Epoch 7/10, Loss: 0.3308\n",
      "SI_mean - Epoch 8/10, Loss: 0.3394\n",
      "SI_mean - Epoch 9/10, Loss: 0.3125\n",
      "SI_mean - Epoch 10/10, Loss: 0.3124\n",
      "SI_mean - Epoch 1/10, Loss: 2.9696\n",
      "SI_mean - Epoch 2/10, Loss: 1.7025\n",
      "SI_mean - Epoch 3/10, Loss: 1.4634\n",
      "SI_mean - Epoch 4/10, Loss: 1.2463\n",
      "SI_mean - Epoch 5/10, Loss: 1.2906\n",
      "SI_mean - Epoch 6/10, Loss: 1.2697\n",
      "SI_mean - Epoch 7/10, Loss: 1.3252\n",
      "SI_mean - Epoch 8/10, Loss: 1.1992\n",
      "SI_mean - Epoch 9/10, Loss: 1.0198\n",
      "SI_mean - Epoch 10/10, Loss: 1.1604\n",
      "SI_mean - Epoch 1/10, Loss: 0.5929\n",
      "SI_mean - Epoch 2/10, Loss: 0.4042\n",
      "SI_mean - Epoch 3/10, Loss: 0.3907\n",
      "SI_mean - Epoch 4/10, Loss: 0.3704\n",
      "SI_mean - Epoch 5/10, Loss: 0.3621\n",
      "SI_mean - Epoch 6/10, Loss: 0.3547\n",
      "SI_mean - Epoch 7/10, Loss: 0.3529\n",
      "SI_mean - Epoch 8/10, Loss: 0.3405\n",
      "SI_mean - Epoch 9/10, Loss: 0.3425\n",
      "SI_mean - Epoch 10/10, Loss: 0.3469\n"
     ]
    }
   ],
   "source": [
    "comb_reg_base_w = base_learner_SI_mean(comb_reg_dict, num_epochs = 10, batch_size = 32, use_sample_weights=True, model_name=\"comb_base_weighted\")\n",
    "comb_reg_base_nw = base_learner_SI_mean(comb_reg_dict, num_epochs = 10, batch_size = 32, use_sample_weights=False, model_name=\"comb_base\")\n",
    "\n",
    "fitbit_reg_base_w = base_learner_SI_mean(fitbit_reg_dict, num_epochs = 10, batch_size = 32, use_sample_weights=True, model_name=\"fitbit_base_weighted\")\n",
    "fitbit_reg_base_nw = base_learner_SI_mean(fitbit_reg_dict, num_epochs = 10, batch_size = 32, use_sample_weights=False, model_name=\"fitbit_base\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1596c3d-e77c-4f2c-a99b-6db7f5f8e4da",
   "metadata": {},
   "source": [
    "### Save reg base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8df88d53-4382-481c-9cc9-299e4c116fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_base_results = pd.concat([comb_reg_base_w[0], comb_reg_base_nw[0], fitbit_reg_base_w[0], fitbit_reg_base_nw[0]], axis=0)\n",
    "reg_base_results.to_csv(\"results/reg_base_learner.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f303770a-1e5a-435c-8aa4-9935afb6d7b8",
   "metadata": {},
   "source": [
    "### Save reg validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94a0875e-cb1e-43bb-8405-2eb3775ca2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_base_val_results = pd.concat([comb_reg_base_w[1], comb_reg_base_nw[1], fitbit_reg_base_w[1], fitbit_reg_base_nw[1]], axis=0)\n",
    "reg_base_val_results.to_csv(\"results/reg_base_val_learner.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4ed9c5-70c3-4904-89f6-5cdaca5f85fd",
   "metadata": {},
   "source": [
    "### Classification Model for base learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94605348-76d5-4152-8cf8-4859b635a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_learner_is_SI(data_dict, model_name, num_epochs, batch_size, use_sample_weights=False):\n",
    "    \"\"\"\n",
    "    Trains a CNN classification model to predict a binary outcome.\n",
    "    Uses subject-level training and testing DataFrames provided in data_dict.\n",
    "    Each split is processed with create_subject_dataset (using outcome_col=\"is_SI\")\n",
    "    so that each subject has an \"X\" array of shape (n_features, timepoints).\n",
    "\n",
    "    If use_sample_weights=True, sample-level weights are used during training\n",
    "    (weights are not used during evaluation).\n",
    "\n",
    "    Also performs 5-fold stratified cross validation (based on PatientID and is_SI)\n",
    "    on the training set and returns the mean and standard deviation (sd) for the following metrics:\n",
    "      - Accuracy\n",
    "      - Sensitivity (Recall for the positive class)\n",
    "      - Specificity (Recall for the negative class)\n",
    "      - AUC (Area Under the ROC Curve)\n",
    "      - Binary Cross Entropy (BCE)\n",
    "\n",
    "    Returns two DataFrames:\n",
    "      1. metrics_df: with columns: model, data, accuracy, sensitivity, specificity, AUC \n",
    "         for the full training and test splits.\n",
    "      2. cv_metrics_df: with columns: stat, model, accuracy, sensitivity, specificity, AUC, BCE.\n",
    "         There will be one row for the mean CV metrics (stat = \"mean\") and one row for their \n",
    "         standard deviation (stat = \"sd\").\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Prepare the Data (Using your helper function) ---\n",
    "    # Assumes create_subject_dataset returns one row per PatientID.\n",
    "    train_df, _ = create_subject_dataset(data_dict['train'], outcome_col=\"is_SI\")\n",
    "    test_df, _  = create_subject_dataset(data_dict['test'], outcome_col=\"is_SI\")\n",
    "    \n",
    "    X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "    y_train = train_df[\"is_SI\"].values\n",
    "    if use_sample_weights:\n",
    "        w_train = train_df[\"sample_weight\"].values\n",
    "    else:\n",
    "        w_train = np.ones_like(y_train, dtype=np.float32)\n",
    "    \n",
    "    X_test = np.stack(test_df[\"X\"].values, axis=0)\n",
    "    y_test = test_df[\"is_SI\"].values\n",
    "    \n",
    "    input_channels = X_train.shape[1]\n",
    "    seq_len = X_train.shape[2]\n",
    "    \n",
    "    # --- Full Training on the Provided Training Set ---\n",
    "    # Assumes CNNClassification is defined and outputs logits.\n",
    "    model = CNNClassification(input_channels, seq_len).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    # Use BCEWithLogitsLoss for binary classification.\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).view(-1, 1),\n",
    "        torch.tensor(w_train, dtype=torch.float32)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    model.train()\n",
    "    for ep in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch, weight_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            weight_batch = weight_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)  # logits output\n",
    "            loss_per_sample = loss_fn(outputs, y_batch).view(-1)\n",
    "            loss = (loss_per_sample * weight_batch).mean() if use_sample_weights else loss_per_sample.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * X_batch.size(0)\n",
    "        print(f\"is_SI - Epoch {ep+1}/{num_epochs}, Loss: {epoch_loss/len(train_dataset):.4f}\")\n",
    "    \n",
    "    # --- Evaluation on Training and Test Sets ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Training set evaluation:\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        train_logits = model(X_train_tensor).cpu().numpy()\n",
    "        # Convert logits to probabilities using the sigmoid function.\n",
    "        train_probs = 1 / (1 + np.exp(-train_logits))\n",
    "        train_preds = (train_probs >= 0.5).astype(int)\n",
    "        \n",
    "        # Test set evaluation:\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        test_logits = model(X_test_tensor).cpu().numpy()\n",
    "        test_probs = 1 / (1 + np.exp(-test_logits))\n",
    "        test_preds = (test_probs >= 0.5).astype(int)\n",
    "    \n",
    "    # Helper function to compute metrics.\n",
    "    def compute_classification_metrics(y_true, y_pred, y_prob):\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        # Sensitivity: recall for positive class (assumes positive label == 1)\n",
    "        sensitivity = recall_score(y_true, y_pred, pos_label=1)\n",
    "        # Specificity: recall for negative class (compute from the confusion matrix)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "        return acc, sensitivity, specificity, auc\n",
    "    \n",
    "    # Compute metrics for training set.\n",
    "    train_acc, train_sens, train_spec, train_auc = compute_classification_metrics(\n",
    "        y_train, train_preds.flatten(), train_probs.flatten()\n",
    "    )\n",
    "    # Compute metrics for test set.\n",
    "    test_acc, test_sens, test_spec, test_auc = compute_classification_metrics(\n",
    "        y_test, test_preds.flatten(), test_probs.flatten()\n",
    "    )\n",
    "    \n",
    "    train_metrics = {\n",
    "        \"model\": model_name,\n",
    "        \"data\": \"train\",\n",
    "        \"accuracy\": train_acc,\n",
    "        \"sensitivity\": train_sens,\n",
    "        \"specificity\": train_spec,\n",
    "        \"AUC\": train_auc\n",
    "    }\n",
    "    test_metrics = {\n",
    "        \"model\": model_name,\n",
    "        \"data\": \"test\",\n",
    "        \"accuracy\": test_acc,\n",
    "        \"sensitivity\": test_sens,\n",
    "        \"specificity\": test_spec,\n",
    "        \"AUC\": test_auc\n",
    "    }\n",
    "    metrics_df = pd.DataFrame([train_metrics, test_metrics])\n",
    "    \n",
    "    # --- 5-fold Stratified Cross Validation on the Training Set ---\n",
    "    # Use your helper function to obtain stratified splits (at the subject level).\n",
    "    cv_splits = get_stratified_cv_splits(train_df, subject_id=\"PatientID\", target_var=\"is_SI\", n_splits=5)\n",
    "    cv_results = []\n",
    "    for cv_train_df, cv_val_df in cv_splits:\n",
    "        X_cv_train = np.stack(cv_train_df[\"X\"].values, axis=0)\n",
    "        y_cv_train = cv_train_df[\"is_SI\"].values\n",
    "        if use_sample_weights:\n",
    "            w_cv_train = cv_train_df[\"sample_weight\"].values\n",
    "        else:\n",
    "            w_cv_train = np.ones_like(y_cv_train, dtype=np.float32)\n",
    "        X_cv_val = np.stack(cv_val_df[\"X\"].values, axis=0)\n",
    "        y_cv_val = cv_val_df[\"is_SI\"].values\n",
    "        \n",
    "        # Train a fresh model on this CV split.\n",
    "        model_cv = CNNClassification(input_channels, seq_len).to(device)\n",
    "        optimizer_cv = optim.Adam(model_cv.parameters(), lr=0.001)\n",
    "        train_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_train, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_train, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(w_cv_train, dtype=torch.float32)\n",
    "        )\n",
    "        train_loader_cv = torch.utils.data.DataLoader(train_dataset_cv, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        \n",
    "        model_cv.train()\n",
    "        for ep in range(num_epochs):\n",
    "            for X_batch, y_batch, weight_batch in train_loader_cv:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                weight_batch = weight_batch.to(device)\n",
    "                optimizer_cv.zero_grad()\n",
    "                outputs = model_cv(X_batch)\n",
    "                loss_per_sample = loss_fn(outputs, y_batch).view(-1)\n",
    "                loss = (loss_per_sample * weight_batch).mean() if use_sample_weights else loss_per_sample.mean()\n",
    "                loss.backward()\n",
    "                optimizer_cv.step()\n",
    "        model_cv.eval()\n",
    "        with torch.no_grad():\n",
    "            X_cv_val_tensor = torch.tensor(X_cv_val, dtype=torch.float32).to(device)\n",
    "            cv_logits = model_cv(X_cv_val_tensor).cpu().numpy()\n",
    "            cv_probs = 1 / (1 + np.exp(-cv_logits))\n",
    "            cv_preds = (cv_probs >= 0.5).astype(int)\n",
    "        \n",
    "        fold_acc, fold_sens, fold_spec, fold_auc = compute_classification_metrics(\n",
    "            y_cv_val, cv_preds.flatten(), cv_probs.flatten()\n",
    "        )\n",
    "        # Compute binary cross entropy for the fold using the predicted probabilities.\n",
    "        epsilon = 1e-7\n",
    "        fold_bce = -np.mean(\n",
    "            y_cv_val * np.log(np.clip(cv_probs, epsilon, 1 - epsilon)) +\n",
    "            (1 - y_cv_val) * np.log(np.clip(1 - cv_probs, epsilon, 1 - epsilon))\n",
    "        )\n",
    "        cv_results.append({\n",
    "            \"accuracy\": fold_acc,\n",
    "            \"sensitivity\": fold_sens,\n",
    "            \"specificity\": fold_spec,\n",
    "            \"AUC\": fold_auc,\n",
    "            \"BCE\": fold_bce\n",
    "        })\n",
    "    \n",
    "    cv_results_df = pd.DataFrame(cv_results)\n",
    "    cv_mean = cv_results_df.mean()\n",
    "    cv_std  = cv_results_df.std()\n",
    "    \n",
    "    # Build the cross validation metrics DataFrame with two rows:\n",
    "    # one for the mean and one for the standard deviation.\n",
    "    mean_row = {\n",
    "        \"stat\": \"mean\",\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": cv_mean[\"accuracy\"],\n",
    "        \"sensitivity\": cv_mean[\"sensitivity\"],\n",
    "        \"specificity\": cv_mean[\"specificity\"],\n",
    "        \"AUC\": cv_mean[\"AUC\"],\n",
    "        \"BCE\": cv_mean[\"BCE\"]\n",
    "    }\n",
    "    sd_row = {\n",
    "        \"stat\": \"sd\",\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": cv_std[\"accuracy\"],\n",
    "        \"sensitivity\": cv_std[\"sensitivity\"],\n",
    "        \"specificity\": cv_std[\"specificity\"],\n",
    "        \"AUC\": cv_std[\"AUC\"],\n",
    "        \"BCE\": cv_std[\"BCE\"]\n",
    "    }\n",
    "    cv_metrics_df = pd.DataFrame([mean_row, sd_row])\n",
    "    \n",
    "    return metrics_df, cv_metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c13b828-0a16-4332-9077-5ae1a012e101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_SI - Epoch 1/10, Loss: 0.2999\n",
      "is_SI - Epoch 2/10, Loss: 0.2437\n",
      "is_SI - Epoch 3/10, Loss: 0.2380\n",
      "is_SI - Epoch 4/10, Loss: 0.2329\n",
      "is_SI - Epoch 5/10, Loss: 0.2263\n",
      "is_SI - Epoch 6/10, Loss: 0.2291\n",
      "is_SI - Epoch 7/10, Loss: 0.2271\n",
      "is_SI - Epoch 8/10, Loss: 0.2203\n",
      "is_SI - Epoch 9/10, Loss: 0.2160\n",
      "is_SI - Epoch 10/10, Loss: 0.2154\n",
      "is_SI - Epoch 1/10, Loss: 0.5632\n",
      "is_SI - Epoch 2/10, Loss: 0.5254\n",
      "is_SI - Epoch 3/10, Loss: 0.5175\n",
      "is_SI - Epoch 4/10, Loss: 0.5026\n",
      "is_SI - Epoch 5/10, Loss: 0.4924\n",
      "is_SI - Epoch 6/10, Loss: 0.4875\n",
      "is_SI - Epoch 7/10, Loss: 0.4794\n",
      "is_SI - Epoch 8/10, Loss: 0.4752\n",
      "is_SI - Epoch 9/10, Loss: 0.4615\n",
      "is_SI - Epoch 10/10, Loss: 0.4611\n",
      "is_SI - Epoch 1/10, Loss: 0.3418\n",
      "is_SI - Epoch 2/10, Loss: 0.2559\n",
      "is_SI - Epoch 3/10, Loss: 0.2482\n",
      "is_SI - Epoch 4/10, Loss: 0.2496\n",
      "is_SI - Epoch 5/10, Loss: 0.2478\n",
      "is_SI - Epoch 6/10, Loss: 0.2421\n",
      "is_SI - Epoch 7/10, Loss: 0.2389\n",
      "is_SI - Epoch 8/10, Loss: 0.2388\n",
      "is_SI - Epoch 9/10, Loss: 0.2367\n",
      "is_SI - Epoch 10/10, Loss: 0.2324\n",
      "is_SI - Epoch 1/10, Loss: 0.5600\n",
      "is_SI - Epoch 2/10, Loss: 0.5375\n",
      "is_SI - Epoch 3/10, Loss: 0.5247\n",
      "is_SI - Epoch 4/10, Loss: 0.5256\n",
      "is_SI - Epoch 5/10, Loss: 0.5181\n",
      "is_SI - Epoch 6/10, Loss: 0.5134\n",
      "is_SI - Epoch 7/10, Loss: 0.5040\n",
      "is_SI - Epoch 8/10, Loss: 0.4936\n",
      "is_SI - Epoch 9/10, Loss: 0.4940\n",
      "is_SI - Epoch 10/10, Loss: 0.4886\n"
     ]
    }
   ],
   "source": [
    "comb_class_base_w = base_learner_is_SI(comb_class_dict, model_name=\"comb_base_weighted\", num_epochs=10, batch_size=32, use_sample_weights=True)\n",
    "comb_class_base_nw = base_learner_is_SI(comb_class_dict, model_name=\"comb_base\", num_epochs = 10, batch_size = 32, use_sample_weights=False)\n",
    "\n",
    "fitbit_class_base_w = base_learner_is_SI(fitbit_class_dict, model_name=\"fitbit_base_weighted\", num_epochs=10, batch_size=32, use_sample_weights=True)\n",
    "fitbit_class_base_nw = base_learner_is_SI(fitbit_class_dict, model_name=\"fitbit_base\", num_epochs = 10, batch_size = 32, use_sample_weights=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3a8afb-f094-4645-aacd-1a7422bb7e5d",
   "metadata": {},
   "source": [
    "### Save class base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5fd91fa-943f-4ec8-909f-75f7013798bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_base_results = pd.concat([comb_class_base_w[0], comb_class_base_nw[0], fitbit_class_base_w[0], fitbit_class_base_nw[0]], axis=0)\n",
    "class_base_results.to_csv(\"results/class_base_learner.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4724977a-9a8e-46cd-8e19-d1ac91af9c95",
   "metadata": {},
   "source": [
    "### Save class val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05895e55-7eef-4d2a-a586-310082ee7910",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_base_val_results = pd.concat([comb_class_base_w[1], comb_class_base_nw[1], fitbit_class_base_w[1], fitbit_class_base_nw[1]], axis=0)\n",
    "class_base_val_results.to_csv(\"results/class_base_val_learner.tsv\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL_py_env)",
   "language": "python",
   "name": "dl_py_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
