{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e50badf-0136-4534-8845-1fc5135b0db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda with 5 GPU(s)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set deterministic behavior for CUDA (set before torch imports)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import optuna\n",
    "\n",
    "# Deterministic seed\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    return seed\n",
    "\n",
    "# Global device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device} with {torch.cuda.device_count()} GPU(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a76020-97db-4390-8213-aa888732b13b",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c5a5bc8-1d52-49cb-ac4e-21b8df6a3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the data directory as needed\n",
    "DL_DIR = \"../../data/deep_learning\"\n",
    "# Load the regression split dictionary.\n",
    "with open(f'{DL_DIR}/comb_reg_dict.pkl', 'rb') as f:\n",
    "    comb_reg_dict = pickle.load(f)\n",
    "\n",
    "with open(f'{DL_DIR}/fitbit_reg_dict.pkl', 'rb') as f:\n",
    "    fitbit_reg_dict = pickle.load(f)\n",
    "\n",
    "# Load the classification split dictionary.\n",
    "with open(f'{DL_DIR}/comb_class_dict.pkl', 'rb') as f:\n",
    "    comb_class_dict = pickle.load(f)\n",
    "\n",
    "with open(f'{DL_DIR}/fitbit_class_dict.pkl', 'rb') as f:\n",
    "    fitbit_class_dict = pickle.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4462f179-6020-4b71-b634-be8f4db06bd0",
   "metadata": {},
   "source": [
    "### Utility: compute output length after conv and pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a061a33f-40d8-43b8-870c-2f63f6114c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_output_length(L_in, kernel_size, stride, padding, dilation):\n",
    "    return (L_in + 2 * padding - dilation * (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "def pool_output_length(L_in, pool_kernel):\n",
    "    # Assume stride==kernel size for pooling\n",
    "    return L_in // pool_kernel\n",
    "\n",
    "# Build subject-level dataset (each row is one subject)\n",
    "def create_subject_dataset(df, outcome_col=\"SI_mean\", use_weights=True):\n",
    "    \"\"\"\n",
    "    Aggregates records for each subject into a subject-level sample.\n",
    "    If use_weights is False, then sample_weight is set to 1.0 for all subjects.\n",
    "    \"\"\"\n",
    "    exclude_cols = [\"PatientID\", \"timepoints\", \"si_kde_weight\", \"SI_mean\", \"is_SI\", \"SI_level\"]\n",
    "    predictor_cols = [col for col in df.columns if col not in (exclude_cols + [outcome_col])]\n",
    "    \n",
    "    subject_data = []\n",
    "    for pid, group in df.groupby(\"PatientID\"):\n",
    "        group_sorted = group.sort_values(\"timepoints\")\n",
    "        X = group_sorted[predictor_cols].values.T  # shape: (n_features, 39)\n",
    "        y = group_sorted[outcome_col].iloc[0]\n",
    "        if use_weights and \"si_kde_weight\" in group_sorted.columns:\n",
    "            weight = group_sorted[\"si_kde_weight\"].iloc[0]\n",
    "        else:\n",
    "            weight = 1.0\n",
    "        record = {\"PatientID\": pid, \"X\": X, outcome_col: y, \"sample_weight\": weight}\n",
    "        if outcome_col == \"is_SI\" and \"SI_mean\" in group_sorted.columns:\n",
    "            record[\"SI_mean\"] = group_sorted[\"SI_mean\"].iloc[0]\n",
    "        subject_data.append(record)\n",
    "    subj_df = pd.DataFrame(subject_data)\n",
    "    subj_df[f\"{outcome_col}_bin\"] = np.round(subj_df[outcome_col]).astype(int)\n",
    "    return subj_df, predictor_cols\n",
    "\n",
    "\n",
    "def save_results_pickle(result_dict, model_name, use_sample_weights=True):\n",
    "    \"\"\"\n",
    "    Saves the result pickle to a folder \"search\". If use_sample_weights is False,\n",
    "    appends '_nw' to the filename.\n",
    "    \"\"\"\n",
    "    save_folder = \"search\"\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "    if use_sample_weights:\n",
    "        filename = os.path.join(save_folder, f\"{model_name}_deep_search_results.pkl\")\n",
    "    else:\n",
    "        filename = os.path.join(save_folder, f\"{model_name}_nw_deep_search_results.pkl\")\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(result_dict, f)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "    \n",
    "def get_stratified_cv_splits(df, subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5):\n",
    "    \"\"\"\n",
    "    Performs stratified K-fold cross validation at the subject level.\n",
    "    \n",
    "    Parameters:\n",
    "      df : pandas.DataFrame\n",
    "          The original dataframe containing repeated measures.\n",
    "      subject_id : str\n",
    "          The column name for the subject ID (e.g., \"PatientID\").\n",
    "      target_var : str\n",
    "          The target variable; for regression use \"SI_mean\" and for classification use \"is_SI\".\n",
    "      n_splits : int\n",
    "          Number of folds for cross validation.\n",
    "    \n",
    "    Returns:\n",
    "      splits : list of tuples\n",
    "          A list where each element is a tuple (train_df, test_df) corresponding\n",
    "          to one fold. Each dataframe contains all rows (i.e. repeated measures) for the patients in that fold.\n",
    "    \n",
    "    Behavior:\n",
    "      - Isolates unique patient IDs and their target variable by dropping duplicates.\n",
    "      - If target_var is \"SI_mean\", creates a new column \"SI_mean_levels\" (rounded SI_mean).\n",
    "      - Uses the resulting column as the stratification column.\n",
    "      - Performs stratified K-fold CV and then subsets the original dataframe based on the patient IDs.\n",
    "    \"\"\"\n",
    "    # Create a subject-level dataframe (unique patient IDs with their target variable)\n",
    "    subject_df = df[[subject_id, target_var]].drop_duplicates(subset=[subject_id]).copy()\n",
    "    \n",
    "    # For regression: create a new column with the rounded SI_mean values.\n",
    "    if target_var == \"SI_mean\":\n",
    "        subject_df[\"SI_mean_levels\"] = subject_df[target_var].round().astype(int)\n",
    "        strat_col = \"SI_mean_levels\"\n",
    "    else:\n",
    "        strat_col = target_var  # For classification, use the target directly.\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    splits = []\n",
    "    \n",
    "    # Get the subject IDs and stratification labels\n",
    "    subjects = subject_df[subject_id].values\n",
    "    strat_labels = subject_df[strat_col].values\n",
    "    \n",
    "    # For each fold, retrieve patient IDs and then subset the original dataframe.\n",
    "    for train_idx, test_idx in skf.split(subjects, strat_labels):\n",
    "        train_patient_ids = subject_df.iloc[train_idx][subject_id].values\n",
    "        test_patient_ids  = subject_df.iloc[test_idx][subject_id].values\n",
    "        train_split = df[df[subject_id].isin(train_patient_ids)]\n",
    "        test_split  = df[df[subject_id].isin(test_patient_ids)]\n",
    "        splits.append((train_split, test_split))\n",
    "    \n",
    "    return splits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f0a97a-9c61-4052-8c49-4371f6ffcea8",
   "metadata": {},
   "source": [
    "###  Deep Search Objective Functions for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9194e5-2c31-4665-ab59-4eb34d47611a",
   "metadata": {},
   "source": [
    "Comb Deep Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1026e960-a60c-4b0f-854b-4d5cea560a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-weighted version (use_weights=False)\n",
    "def objective_regression_comb_nw(trial, data_dict):\n",
    "    \"\"\"\n",
    "    Non-weighted CNN regression for comb_reg.\n",
    "    Tuned hyperparameters are the same as in the weighted version.\n",
    "    \"\"\"\n",
    "    set_seed(42)\n",
    "    splits = get_stratified_cv_splits(\n",
    "        data_dict['train'], subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5\n",
    "    )\n",
    "    overall_rmse_list = []\n",
    "    bin_rmse_dict = {str(b): [] for b in range(1, 6)}\n",
    "    \n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 5, 10)\n",
    "    dropout_prob = trial.suggest_float(\"dropout_prob\", 0.1, 0.5)\n",
    "    n_filters_0 = trial.suggest_int(\"n_filters_0\", 8, 64, step=8)\n",
    "    kernel_size_0 = trial.suggest_int(\"kernel_size_0\", 3, 7, step=2)\n",
    "    n_conv = trial.suggest_int(\"n_conv\", 1, 3)\n",
    "    batch_size = 32\n",
    "\n",
    "    for train_split, val_split in splits:\n",
    "        # Force non-weighted subject-level dataset\n",
    "        train_df, _ = create_subject_dataset(train_split, outcome_col=\"SI_mean\", use_weights=False)\n",
    "        val_df, _   = create_subject_dataset(val_split, outcome_col=\"SI_mean\", use_weights=False)\n",
    "        \n",
    "        X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "        y_train = train_df[\"SI_mean\"].values\n",
    "        # sample_weight will be 1.0 for every subject\n",
    "        w_train = train_df[\"sample_weight\"].values  \n",
    "        X_val   = np.stack(val_df[\"X\"].values, axis=0)\n",
    "        y_val   = val_df[\"SI_mean\"].values\n",
    "        val_bins = val_df[\"SI_mean_bin\"].values\n",
    "        \n",
    "        n_subjects, input_channels, seq_len = X_train.shape\n",
    "        layers = []\n",
    "        current_channels = input_channels\n",
    "        current_seq_len = seq_len\n",
    "        for i in range(n_conv):\n",
    "            padding = (kernel_size_0 - 1) // 2\n",
    "            conv = nn.Conv1d(in_channels=current_channels,\n",
    "                             out_channels=n_filters_0,\n",
    "                             kernel_size=kernel_size_0,\n",
    "                             stride=1,\n",
    "                             padding=padding)\n",
    "            layers.extend([conv, nn.ReLU(), nn.Dropout(dropout_prob)])\n",
    "            current_channels = n_filters_0\n",
    "            current_seq_len = conv_output_length(current_seq_len, kernel_size_0, 1, padding, 1)\n",
    "        conv_net = nn.Sequential(*layers)\n",
    "        flattened_dim = current_channels * current_seq_len\n",
    "        fc_net = nn.Sequential(nn.Linear(flattened_dim, 1))\n",
    "        \n",
    "        class CombCNN(nn.Module):\n",
    "            def __init__(self, conv_net, fc_net):\n",
    "                super(CombCNN, self).__init__()\n",
    "                self.conv = conv_net\n",
    "                self.fc = fc_net\n",
    "            def forward(self, x):\n",
    "                x = self.conv(x)\n",
    "                x = x.view(x.size(0), -1)\n",
    "                return self.fc(x)\n",
    "                \n",
    "        model = CombCNN(conv_net, fc_net).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "            \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "        \n",
    "        train_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_train, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(w_train, dtype=torch.float32)\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for X_batch, y_batch, weight_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                weight_batch = weight_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = (loss_fn(outputs, y_batch).view(-1) * weight_batch).mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "            preds = model(X_val_tensor).cpu().numpy()\n",
    "        fold_rmse = np.sqrt(np.mean((preds - y_val.reshape(-1, 1))**2))\n",
    "        overall_rmse_list.append(fold_rmse)\n",
    "        for b in range(1, 6):\n",
    "            idx = np.where(val_bins == b)[0]\n",
    "            if len(idx) > 0:\n",
    "                bin_rmse = np.sqrt(np.mean((preds[idx] - y_val[idx].reshape(-1, 1))**2))\n",
    "                bin_rmse_dict[str(b)].append(bin_rmse)\n",
    "                \n",
    "    overall_mean_rmse = np.mean(overall_rmse_list)\n",
    "    overall_std_rmse = np.std(overall_rmse_list)\n",
    "    bin_mean_rmse = {str(b): np.mean(bin_rmse_dict[str(b)]) if len(bin_rmse_dict[str(b)]) > 0 else float('nan')\n",
    "                     for b in range(1, 6)}\n",
    "    bin_std_rmse  = {str(b): np.std(bin_rmse_dict[str(b)]) if len(bin_rmse_dict[str(b)]) > 0 else float('nan')\n",
    "                     for b in range(1, 6)}\n",
    "    \n",
    "    trial.set_user_attr(\"overall_mean_rmse\", overall_mean_rmse)\n",
    "    trial.set_user_attr(\"overall_std_rmse\", overall_std_rmse)\n",
    "    trial.set_user_attr(\"bin_mean_rmse\", bin_mean_rmse)\n",
    "    trial.set_user_attr(\"bin_std_rmse\", bin_std_rmse)\n",
    "    \n",
    "    return overall_mean_rmse\n",
    "\n",
    "\n",
    "def objective_regression_comb(trial, data_dict):\n",
    "    \"\"\"\n",
    "    Objective function for comb_reg.\n",
    "    Tunable hyperparameters:\n",
    "      - lr\n",
    "      - dropout_prob\n",
    "      - num_epochs\n",
    "      - n_filters_0\n",
    "      - n_cov: number of convolutional layers (1 to 3)\n",
    "      - use_regularization: whether to add weight decay\n",
    "      - dilation_0: dilation for all conv layers\n",
    "    \"\"\"\n",
    "    set_seed(42)\n",
    "    # Use the provided stratified CV splits (using repeated measures)\n",
    "    splits = get_stratified_cv_splits(data_dict['train'], subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5)\n",
    "\n",
    "    overall_rmse_list = []\n",
    "    bin_rmse_dict = {str(b): [] for b in range(1, 6)}\n",
    "    \n",
    "    # Tunable hyperparameters\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    dropout_prob = trial.suggest_float(\"dropout_prob\", 0.1, 0.5)\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 5, 10)\n",
    "    n_filters_0 = trial.suggest_int(\"n_filters_0\", 8, 64, step=8)\n",
    "    n_cov = trial.suggest_int(\"n_cov\", 1, 3)  # number of convolutional layers\n",
    "    use_regularization = trial.suggest_categorical(\"use_regularization\", [True, False])\n",
    "    dilation_0 = trial.suggest_int(\"dilation_0\", 1, 3)\n",
    "    batch_size = 32  # fixed\n",
    "\n",
    "    for train_split, val_split in splits:\n",
    "        # Build subject-level datasets for training and validation\n",
    "        train_df, _ = create_subject_dataset(train_split, outcome_col=\"SI_mean\")\n",
    "        val_df, _ = create_subject_dataset(val_split, outcome_col=\"SI_mean\")\n",
    "        \n",
    "        X_train = np.stack(train_df[\"X\"].values, axis=0)  # shape: (n_subjects, n_features, seq_len)\n",
    "        y_train = train_df[\"SI_mean\"].values\n",
    "        w_train = train_df[\"sample_weight\"].values\n",
    "        \n",
    "        X_val = np.stack(val_df[\"X\"].values, axis=0)\n",
    "        y_val = val_df[\"SI_mean\"].values\n",
    "        val_bins = val_df[\"SI_mean_bin\"].values\n",
    "        \n",
    "        n_subjects, input_channels, seq_len = X_train.shape\n",
    "\n",
    "        # Build a conv net with n_cov layers.\n",
    "        layers = []\n",
    "        current_channels = input_channels\n",
    "        current_seq_len = seq_len\n",
    "        for i in range(n_cov):\n",
    "            # For kernel size 3, set padding to preserve dimension:\n",
    "            padding = dilation_0 * ((3 - 1) // 2)\n",
    "            conv = nn.Conv1d(in_channels=current_channels,\n",
    "                             out_channels=n_filters_0,\n",
    "                             kernel_size=3,\n",
    "                             stride=1,\n",
    "                             padding=padding,\n",
    "                             dilation=dilation_0)\n",
    "            layers.append(conv)\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_prob))\n",
    "            current_channels = n_filters_0\n",
    "            current_seq_len = conv_output_length(current_seq_len, kernel_size=3, stride=1, padding=padding, dilation=dilation_0)\n",
    "        conv_net = nn.Sequential(*layers)\n",
    "        flattened_dim = current_channels * current_seq_len\n",
    "        fc_net = nn.Sequential(nn.Linear(flattened_dim, 1))\n",
    "\n",
    "        class CombCNN(nn.Module):\n",
    "            def __init__(self, conv_net, fc_net):\n",
    "                super(CombCNN, self).__init__()\n",
    "                self.conv = conv_net\n",
    "                self.fc = fc_net\n",
    "            def forward(self, x):\n",
    "                x = self.conv(x)\n",
    "                x = x.view(x.size(0), -1)\n",
    "                return self.fc(x)\n",
    "\n",
    "        model = CombCNN(conv_net, fc_net).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "\n",
    "        # If using regularization, add a small weight decay.\n",
    "        weight_decay = 1e-4 if use_regularization else 0.0\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "        train_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_train, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(w_train, dtype=torch.float32)\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for X_batch, y_batch, weight_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                weight_batch = weight_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = (loss_fn(outputs, y_batch).view(-1) * weight_batch).mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "            preds = model(X_val_tensor).cpu().numpy()\n",
    "        fold_rmse = np.sqrt(np.mean((preds - y_val.reshape(-1, 1)) ** 2))\n",
    "        overall_rmse_list.append(fold_rmse)\n",
    "\n",
    "        # Compute per-bin RMSE.\n",
    "        for b in range(1, 6):\n",
    "            idx = np.where(val_bins == b)[0]\n",
    "            if len(idx) > 0:\n",
    "                bin_rmse = np.sqrt(np.mean((preds[idx] - y_val[idx].reshape(-1, 1)) ** 2))\n",
    "                bin_rmse_dict[str(b)].append(bin_rmse)\n",
    "\n",
    "    overall_mean_rmse = np.mean(overall_rmse_list)\n",
    "    overall_std_rmse = np.std(overall_rmse_list)\n",
    "    bin_mean_rmse = {str(b): np.mean(bin_rmse_dict[str(b)]) if len(bin_rmse_dict[str(b)]) > 0 else float('nan')\n",
    "                     for b in range(1, 6)}\n",
    "    bin_std_rmse = {str(b): np.std(bin_rmse_dict[str(b)]) if len(bin_rmse_dict[str(b)]) > 0 else float('nan')\n",
    "                    for b in range(1, 6)}\n",
    "\n",
    "    trial.set_user_attr(\"overall_mean_rmse\", overall_mean_rmse)\n",
    "    trial.set_user_attr(\"overall_std_rmse\", overall_std_rmse)\n",
    "    trial.set_user_attr(\"bin_mean_rmse\", bin_mean_rmse)\n",
    "    trial.set_user_attr(\"bin_std_rmse\", bin_std_rmse)\n",
    "\n",
    "    return overall_mean_rmse\n",
    "\n",
    "\n",
    "def run_deep_search_comb_reg(data_dict, use_sample_weights, model_name, n_trials=75):\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    def print_progress(study, trial):\n",
    "        print(f\"{model_name} trial {len(study.trials)}/{n_trials}\\r\", end=\"\", flush=True)\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    optimize_fn = objective_regression_comb if use_sample_weights else objective_regression_comb_nw\n",
    "    study.optimize(lambda trial: optimize_fn(trial, data_dict),\n",
    "                   n_trials=n_trials, callbacks=[print_progress])\n",
    "\n",
    "    mean_rows = []\n",
    "    std_rows = []\n",
    "    for t in study.trials:\n",
    "        row_mean = {\n",
    "            \"model\": model_name,\n",
    "            \"type\": \"weighted\" if use_sample_weights else \"not weighted\",\n",
    "            \"1\": t.user_attrs[\"bin_mean_rmse\"].get(\"1\", float('nan')),\n",
    "            \"2\": t.user_attrs[\"bin_mean_rmse\"].get(\"2\", float('nan')),\n",
    "            \"3\": t.user_attrs[\"bin_mean_rmse\"].get(\"3\", float('nan')),\n",
    "            \"4\": t.user_attrs[\"bin_mean_rmse\"].get(\"4\", float('nan')),\n",
    "            \"5\": t.user_attrs[\"bin_mean_rmse\"].get(\"5\", float('nan')),\n",
    "            \"overall\": t.user_attrs[\"overall_mean_rmse\"],\n",
    "            \"config\": t.params\n",
    "        }\n",
    "        row_std = {\n",
    "            \"model\": model_name,\n",
    "            \"type\": \"weighted\" if use_sample_weights else \"not weighted\",\n",
    "            \"1\": t.user_attrs[\"bin_std_rmse\"].get(\"1\", float('nan')),\n",
    "            \"2\": t.user_attrs[\"bin_std_rmse\"].get(\"2\", float('nan')),\n",
    "            \"3\": t.user_attrs[\"bin_std_rmse\"].get(\"3\", float('nan')),\n",
    "            \"4\": t.user_attrs[\"bin_std_rmse\"].get(\"4\", float('nan')),\n",
    "            \"5\": t.user_attrs[\"bin_std_rmse\"].get(\"5\", float('nan')),\n",
    "            \"overall\": t.user_attrs[\"overall_std_rmse\"],\n",
    "            \"config\": t.params\n",
    "        }\n",
    "        mean_rows.append(row_mean)\n",
    "        std_rows.append(row_std)\n",
    "\n",
    "    columns = [\"model\", \"type\", \"1\", \"2\", \"3\", \"4\", \"5\", \"overall\", \"config\"]\n",
    "    mean_df = pd.DataFrame(mean_rows, columns=columns)\n",
    "    std_df = pd.DataFrame(std_rows, columns=columns)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    optimal_configuration = {\n",
    "        \"value\": best_trial.value,\n",
    "        \"params\": best_trial.params,\n",
    "        \"user_attrs\": best_trial.user_attrs\n",
    "    }\n",
    "\n",
    "    result_dict = {\n",
    "        \"mean_rmse\": mean_df,\n",
    "        \"std_rmse\": std_df,\n",
    "        \"optimal_configuration\": optimal_configuration\n",
    "    }\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993d08fc-a042-4d58-89ec-adb4563833d8",
   "metadata": {},
   "source": [
    "Fitbit Deep Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9647b83-09b2-41e7-8027-bd5a93d2484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-weighted version\n",
    "def objective_regression_fitbit_nw(trial, data_dict):\n",
    "    \"\"\"\n",
    "    Non-weighted CNN regression for fitbit_reg.\n",
    "    Tuned hyperparameters:\n",
    "      - dropout_prob\n",
    "      - n_filters_0\n",
    "      - num_epochs\n",
    "    \"\"\"\n",
    "    set_seed(42)\n",
    "    splits = get_stratified_cv_splits(\n",
    "        data_dict['train'], subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5\n",
    "    )\n",
    "    overall_rmse_list = []\n",
    "    bin_rmse_dict = {str(b): [] for b in range(1, 6)}\n",
    "    \n",
    "    dropout_prob = trial.suggest_float(\"dropout_prob\", 0.1, 0.5)\n",
    "    n_filters_0 = trial.suggest_int(\"n_filters_0\", 8, 64, step=8)\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 5, 10)\n",
    "    \n",
    "    kernel_size = 3\n",
    "    stride = 1\n",
    "    padding = (kernel_size - 1) // 2\n",
    "    batch_size = 32\n",
    "    \n",
    "    for train_split, val_split in splits:\n",
    "        train_df, _ = create_subject_dataset(train_split, outcome_col=\"SI_mean\", use_weights=False)\n",
    "        val_df, _ = create_subject_dataset(val_split, outcome_col=\"SI_mean\", use_weights=False)\n",
    "        \n",
    "        X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "        y_train = train_df[\"SI_mean\"].values\n",
    "        w_train = train_df[\"sample_weight\"].values  # should be 1.0\n",
    "        X_val = np.stack(val_df[\"X\"].values, axis=0)\n",
    "        y_val = val_df[\"SI_mean\"].values\n",
    "        val_bins = val_df[\"SI_mean_bin\"].values\n",
    "        \n",
    "        n_subjects, input_channels, seq_len = X_train.shape\n",
    "        conv = nn.Conv1d(in_channels=input_channels,\n",
    "                         out_channels=n_filters_0,\n",
    "                         kernel_size=kernel_size,\n",
    "                         stride=stride,\n",
    "                         padding=padding)\n",
    "        layers = [conv, nn.ReLU(), nn.Dropout(dropout_prob)]\n",
    "        conv_net = nn.Sequential(*layers)\n",
    "        new_seq_len = conv_output_length(seq_len, kernel_size, stride, padding, dilation=1)\n",
    "        flattened_dim = n_filters_0 * new_seq_len\n",
    "        fc_net = nn.Sequential(nn.Linear(flattened_dim, 1))\n",
    "        \n",
    "        class FitRegCNN(nn.Module):\n",
    "            def __init__(self, conv_net, fc_net):\n",
    "                super(FitRegCNN, self).__init__()\n",
    "                self.conv = conv_net\n",
    "                self.fc = fc_net\n",
    "            def forward(self, x):\n",
    "                x = self.conv(x)\n",
    "                x = x.view(x.size(0), -1)\n",
    "                return self.fc(x)\n",
    "                \n",
    "        model = FitRegCNN(conv_net, fc_net).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "            \n",
    "        lr_fixed = 1e-3\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr_fixed)\n",
    "        loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "        \n",
    "        train_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_train, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(w_train, dtype=torch.float32)\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for X_batch, y_batch, weight_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                weight_batch = weight_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = (loss_fn(outputs, y_batch).view(-1) * weight_batch).mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "            preds = model(X_val_tensor).cpu().numpy()\n",
    "        fold_rmse = np.sqrt(np.mean((preds - y_val.reshape(-1, 1))**2))\n",
    "        overall_rmse_list.append(fold_rmse)\n",
    "        for b in range(1, 6):\n",
    "            idx = np.where(val_bins == b)[0]\n",
    "            if len(idx) > 0:\n",
    "                bin_rmse = np.sqrt(np.mean((preds[idx] - y_val[idx].reshape(-1, 1))**2))\n",
    "                bin_rmse_dict[str(b)].append(bin_rmse)\n",
    "                \n",
    "    overall_mean_rmse = np.mean(overall_rmse_list)\n",
    "    overall_std_rmse = np.std(overall_rmse_list)\n",
    "    bin_mean_rmse = {str(b): np.mean(bin_rmse_dict[str(b)]) if len(bin_rmse_dict[str(b)]) > 0 else float('nan')\n",
    "                     for b in range(1, 6)}\n",
    "    bin_std_rmse  = {str(b): np.std(bin_rmse_dict[str(b)]) if len(bin_rmse_dict[str(b)]) > 0 else float('nan')\n",
    "                     for b in range(1, 6)}\n",
    "    \n",
    "    trial.set_user_attr(\"overall_mean_rmse\", overall_mean_rmse)\n",
    "    trial.set_user_attr(\"overall_std_rmse\", overall_std_rmse)\n",
    "    trial.set_user_attr(\"bin_mean_rmse\", bin_mean_rmse)\n",
    "    trial.set_user_attr(\"bin_std_rmse\", bin_std_rmse)\n",
    "    \n",
    "    return overall_mean_rmse\n",
    "\n",
    "def objective_regression_fitbit(trial, data_dict):\n",
    "    \"\"\"\n",
    "    Objective function for fitbit_reg.\n",
    "    Tunable hyperparameters:\n",
    "      - drop_out_prb (dropout probability)\n",
    "      - lr\n",
    "      - use_regularization\n",
    "      - num_epochs\n",
    "      - n_filters_0\n",
    "    \"\"\"\n",
    "    set_seed(42)\n",
    "    splits = get_stratified_cv_splits(data_dict['train'], subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5)\n",
    "\n",
    "    overall_rmse_list = []\n",
    "    bin_rmse_dict = {str(b): [] for b in range(1, 6)}\n",
    "\n",
    "    dropout_prob = trial.suggest_float(\"drop_out_prb\", 0.1, 0.5)\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    use_regularization = trial.suggest_categorical(\"use_regularization\", [True, False])\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 5, 10)\n",
    "    n_filters_0 = trial.suggest_int(\"n_filters_0\", 8, 64, step=8)\n",
    "\n",
    "    # Fixed parameters for fitbit_reg:\n",
    "    batch_size = 32\n",
    "    kernel_size = 3\n",
    "    stride = 1\n",
    "    dilation = 1\n",
    "    padding = (kernel_size - 1) // 2\n",
    "\n",
    "    for train_split, val_split in splits:\n",
    "        train_df, _ = create_subject_dataset(train_split, outcome_col=\"SI_mean\")\n",
    "        val_df, _ = create_subject_dataset(val_split, outcome_col=\"SI_mean\")\n",
    "\n",
    "        X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "        y_train = train_df[\"SI_mean\"].values\n",
    "        w_train = train_df[\"sample_weight\"].values\n",
    "\n",
    "        X_val = np.stack(val_df[\"X\"].values, axis=0)\n",
    "        y_val = val_df[\"SI_mean\"].values\n",
    "        val_bins = val_df[\"SI_mean_bin\"].values\n",
    "\n",
    "        n_subjects, input_channels, seq_len = X_train.shape\n",
    "\n",
    "        layers = []\n",
    "        conv0 = nn.Conv1d(in_channels=input_channels,\n",
    "                          out_channels=n_filters_0,\n",
    "                          kernel_size=kernel_size,\n",
    "                          stride=stride,\n",
    "                          dilation=dilation,\n",
    "                          padding=padding)\n",
    "        layers.append(conv0)\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout_prob))\n",
    "        conv_net = nn.Sequential(*layers)\n",
    "        new_seq_len = conv_output_length(seq_len, kernel_size, stride, padding, dilation)\n",
    "        flattened_dim = n_filters_0 * new_seq_len\n",
    "        fc_net = nn.Sequential(nn.Linear(flattened_dim, 1))\n",
    "\n",
    "        class FitRegCNN(nn.Module):\n",
    "            def __init__(self, conv_net, fc_net):\n",
    "                super(FitRegCNN, self).__init__()\n",
    "                self.conv = conv_net\n",
    "                self.fc = fc_net\n",
    "            def forward(self, x):\n",
    "                x = self.conv(x)\n",
    "                x = x.view(x.size(0), -1)\n",
    "                return self.fc(x)\n",
    "\n",
    "        model = FitRegCNN(conv_net, fc_net).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "\n",
    "        weight_decay = 1e-4 if use_regularization else 0.0\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "        train_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_train, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(w_train, dtype=torch.float32)\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for X_batch, y_batch, weight_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                weight_batch = weight_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = (loss_fn(outputs, y_batch).view(-1) * weight_batch).mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "            preds = model(X_val_tensor).cpu().numpy()\n",
    "        fold_rmse = np.sqrt(np.mean((preds - y_val.reshape(-1, 1)) ** 2))\n",
    "        overall_rmse_list.append(fold_rmse)\n",
    "\n",
    "        for b in range(1, 6):\n",
    "            idx = np.where(val_bins == b)[0]\n",
    "            if len(idx) > 0:\n",
    "                bin_rmse = np.sqrt(np.mean((preds[idx] - y_val[idx].reshape(-1, 1)) ** 2))\n",
    "                bin_rmse_dict[str(b)].append(bin_rmse)\n",
    "\n",
    "    overall_mean_rmse = np.mean(overall_rmse_list)\n",
    "    overall_std_rmse = np.std(overall_rmse_list)\n",
    "    bin_mean_rmse = {str(b): np.mean(bin_rmse_dict[str(b)]) if len(bin_rmse_dict[str(b)]) > 0 else float('nan')\n",
    "                     for b in range(1, 6)}\n",
    "    bin_std_rmse = {str(b): np.std(bin_rmse_dict[str(b)]) if len(bin_rmse_dict[str(b)]) > 0 else float('nan')\n",
    "                    for b in range(1, 6)}\n",
    "\n",
    "    trial.set_user_attr(\"overall_mean_rmse\", overall_mean_rmse)\n",
    "    trial.set_user_attr(\"overall_std_rmse\", overall_std_rmse)\n",
    "    trial.set_user_attr(\"bin_mean_rmse\", bin_mean_rmse)\n",
    "    trial.set_user_attr(\"bin_std_rmse\", bin_std_rmse)\n",
    "\n",
    "    return overall_mean_rmse\n",
    "\n",
    "\n",
    "def run_deep_search_fitbit_reg(data_dict, use_sample_weights, model_name, n_trials=75):\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    def print_progress(study, trial):\n",
    "        print(f\"{model_name} trial {len(study.trials)}/{n_trials}\\r\", end=\"\", flush=True)\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    optimize_fn = objective_regression_fitbit if use_sample_weights else objective_regression_fitbit_nw\n",
    "    study.optimize(lambda trial: optimize_fn(trial, data_dict),\n",
    "                   n_trials=n_trials, callbacks=[print_progress])\n",
    "\n",
    "    mean_rows = []\n",
    "    std_rows = []\n",
    "    for t in study.trials:\n",
    "        row_mean = {\n",
    "            \"model\": model_name,\n",
    "            \"type\": \"weighted\" if use_sample_weights else \"not weighted\",\n",
    "            \"1\": t.user_attrs[\"bin_mean_rmse\"].get(\"1\", float('nan')),\n",
    "            \"2\": t.user_attrs[\"bin_mean_rmse\"].get(\"2\", float('nan')),\n",
    "            \"3\": t.user_attrs[\"bin_mean_rmse\"].get(\"3\", float('nan')),\n",
    "            \"4\": t.user_attrs[\"bin_mean_rmse\"].get(\"4\", float('nan')),\n",
    "            \"5\": t.user_attrs[\"bin_mean_rmse\"].get(\"5\", float('nan')),\n",
    "            \"overall\": t.user_attrs[\"overall_mean_rmse\"],\n",
    "            \"config\": t.params\n",
    "        }\n",
    "        row_std = {\n",
    "            \"model\": model_name,\n",
    "            \"type\": \"weighted\" if use_sample_weights else \"not weighted\",\n",
    "            \"1\": t.user_attrs[\"bin_std_rmse\"].get(\"1\", float('nan')),\n",
    "            \"2\": t.user_attrs[\"bin_std_rmse\"].get(\"2\", float('nan')),\n",
    "            \"3\": t.user_attrs[\"bin_std_rmse\"].get(\"3\", float('nan')),\n",
    "            \"4\": t.user_attrs[\"bin_std_rmse\"].get(\"4\", float('nan')),\n",
    "            \"5\": t.user_attrs[\"bin_std_rmse\"].get(\"5\", float('nan')),\n",
    "            \"overall\": t.user_attrs[\"overall_std_rmse\"],\n",
    "            \"config\": t.params\n",
    "        }\n",
    "        mean_rows.append(row_mean)\n",
    "        std_rows.append(row_std)\n",
    "\n",
    "    columns = [\"model\", \"type\", \"1\", \"2\", \"3\", \"4\", \"5\", \"overall\", \"config\"]\n",
    "    mean_df = pd.DataFrame(mean_rows, columns=columns)\n",
    "    std_df = pd.DataFrame(std_rows, columns=columns)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    optimal_configuration = {\"value\": best_trial.value,\n",
    "                             \"params\": best_trial.params,\n",
    "                             \"user_attrs\": best_trial.user_attrs}\n",
    "\n",
    "    result_dict = {\"mean_rmse\": mean_df,\n",
    "                   \"std_rmse\": std_df,\n",
    "                   \"optimal_configuration\": optimal_configuration}\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9b9bbe-47d8-4441-a57f-462b123416af",
   "metadata": {},
   "source": [
    "### Run deep search for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2590d3e-7161-491e-94fa-d247b183d821",
   "metadata": {},
   "source": [
    "#### Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6217c2cd-e8e7-499e-8191-3a1fcfc04cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to search/comb_reg_deep_deep_search_results.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>type</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>overall</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>1.393811</td>\n",
       "      <td>0.870844</td>\n",
       "      <td>0.869801</td>\n",
       "      <td>1.536806</td>\n",
       "      <td>2.297815</td>\n",
       "      <td>1.332119</td>\n",
       "      <td>{'lr': 0.0005288622240373623, 'dropout_prob': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>1.085239</td>\n",
       "      <td>0.628376</td>\n",
       "      <td>1.017290</td>\n",
       "      <td>1.863034</td>\n",
       "      <td>2.829652</td>\n",
       "      <td>1.064307</td>\n",
       "      <td>{'lr': 4.896278079138998e-05, 'dropout_prob': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>1.709774</td>\n",
       "      <td>1.043727</td>\n",
       "      <td>0.540656</td>\n",
       "      <td>1.181194</td>\n",
       "      <td>2.062841</td>\n",
       "      <td>1.606519</td>\n",
       "      <td>{'lr': 8.546548868916738e-05, 'dropout_prob': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>1.425002</td>\n",
       "      <td>1.252953</td>\n",
       "      <td>1.516907</td>\n",
       "      <td>2.155115</td>\n",
       "      <td>3.191589</td>\n",
       "      <td>1.468563</td>\n",
       "      <td>{'lr': 0.007056872698036484, 'dropout_prob': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>1.598624</td>\n",
       "      <td>1.046183</td>\n",
       "      <td>0.746520</td>\n",
       "      <td>1.453205</td>\n",
       "      <td>2.050083</td>\n",
       "      <td>1.520920</td>\n",
       "      <td>{'lr': 0.0002553399922608638, 'dropout_prob': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.830737</td>\n",
       "      <td>0.513131</td>\n",
       "      <td>1.227953</td>\n",
       "      <td>2.022883</td>\n",
       "      <td>2.861890</td>\n",
       "      <td>0.860659</td>\n",
       "      <td>{'lr': 1.7223038444033673e-05, 'dropout_prob':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.248935</td>\n",
       "      <td>0.886722</td>\n",
       "      <td>1.920040</td>\n",
       "      <td>2.771035</td>\n",
       "      <td>3.790356</td>\n",
       "      <td>0.661265</td>\n",
       "      <td>{'lr': 1.0093409334112497e-05, 'dropout_prob':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.408769</td>\n",
       "      <td>0.679306</td>\n",
       "      <td>1.666788</td>\n",
       "      <td>2.502117</td>\n",
       "      <td>3.518366</td>\n",
       "      <td>0.653402</td>\n",
       "      <td>{'lr': 1.2190224984562657e-05, 'dropout_prob':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>1.540365</td>\n",
       "      <td>0.876328</td>\n",
       "      <td>0.541653</td>\n",
       "      <td>1.361522</td>\n",
       "      <td>2.039991</td>\n",
       "      <td>1.446638</td>\n",
       "      <td>{'lr': 2.3097356466409334e-05, 'dropout_prob':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.389965</td>\n",
       "      <td>0.696407</td>\n",
       "      <td>1.689656</td>\n",
       "      <td>2.526496</td>\n",
       "      <td>3.543914</td>\n",
       "      <td>0.650105</td>\n",
       "      <td>{'lr': 1.2089984414346552e-05, 'dropout_prob':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            model      type         1         2         3         4         5  \\\n",
       "0   comb_reg_deep  weighted  1.393811  0.870844  0.869801  1.536806  2.297815   \n",
       "1   comb_reg_deep  weighted  1.085239  0.628376  1.017290  1.863034  2.829652   \n",
       "2   comb_reg_deep  weighted  1.709774  1.043727  0.540656  1.181194  2.062841   \n",
       "3   comb_reg_deep  weighted  1.425002  1.252953  1.516907  2.155115  3.191589   \n",
       "4   comb_reg_deep  weighted  1.598624  1.046183  0.746520  1.453205  2.050083   \n",
       "..            ...       ...       ...       ...       ...       ...       ...   \n",
       "70  comb_reg_deep  weighted  0.830737  0.513131  1.227953  2.022883  2.861890   \n",
       "71  comb_reg_deep  weighted  0.248935  0.886722  1.920040  2.771035  3.790356   \n",
       "72  comb_reg_deep  weighted  0.408769  0.679306  1.666788  2.502117  3.518366   \n",
       "73  comb_reg_deep  weighted  1.540365  0.876328  0.541653  1.361522  2.039991   \n",
       "74  comb_reg_deep  weighted  0.389965  0.696407  1.689656  2.526496  3.543914   \n",
       "\n",
       "     overall                                             config  \n",
       "0   1.332119  {'lr': 0.0005288622240373623, 'dropout_prob': ...  \n",
       "1   1.064307  {'lr': 4.896278079138998e-05, 'dropout_prob': ...  \n",
       "2   1.606519  {'lr': 8.546548868916738e-05, 'dropout_prob': ...  \n",
       "3   1.468563  {'lr': 0.007056872698036484, 'dropout_prob': 0...  \n",
       "4   1.520920  {'lr': 0.0002553399922608638, 'dropout_prob': ...  \n",
       "..       ...                                                ...  \n",
       "70  0.860659  {'lr': 1.7223038444033673e-05, 'dropout_prob':...  \n",
       "71  0.661265  {'lr': 1.0093409334112497e-05, 'dropout_prob':...  \n",
       "72  0.653402  {'lr': 1.2190224984562657e-05, 'dropout_prob':...  \n",
       "73  1.446638  {'lr': 2.3097356466409334e-05, 'dropout_prob':...  \n",
       "74  0.650105  {'lr': 1.2089984414346552e-05, 'dropout_prob':...  \n",
       "\n",
       "[75 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run deep search for COMB dataset (n_trials determines how many steps/trials are executed)\n",
    "results_comb_reg = run_deep_search_comb_reg(comb_reg_dict, use_sample_weights=True, model_name=\"comb_reg_deep\", n_trials=75)\n",
    "save_results_pickle(results_comb_reg, \"comb_reg_deep\")\n",
    "results_comb_reg[\"mean_rmse\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c957cf7-51cf-405f-bf70-d290d2d020c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to search/fitbit_reg_deep_deep_search_results.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>type</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>overall</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>1.393811</td>\n",
       "      <td>0.870844</td>\n",
       "      <td>0.869801</td>\n",
       "      <td>1.536806</td>\n",
       "      <td>2.297815</td>\n",
       "      <td>1.332119</td>\n",
       "      <td>{'lr': 0.0005288622240373623, 'dropout_prob': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>1.085239</td>\n",
       "      <td>0.628376</td>\n",
       "      <td>1.017290</td>\n",
       "      <td>1.863034</td>\n",
       "      <td>2.829652</td>\n",
       "      <td>1.064307</td>\n",
       "      <td>{'lr': 4.896278079138998e-05, 'dropout_prob': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>1.709774</td>\n",
       "      <td>1.043727</td>\n",
       "      <td>0.540656</td>\n",
       "      <td>1.181194</td>\n",
       "      <td>2.062841</td>\n",
       "      <td>1.606519</td>\n",
       "      <td>{'lr': 8.546548868916738e-05, 'dropout_prob': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>1.425002</td>\n",
       "      <td>1.252953</td>\n",
       "      <td>1.516907</td>\n",
       "      <td>2.155115</td>\n",
       "      <td>3.191589</td>\n",
       "      <td>1.468563</td>\n",
       "      <td>{'lr': 0.007056872698036484, 'dropout_prob': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>1.598624</td>\n",
       "      <td>1.046183</td>\n",
       "      <td>0.746520</td>\n",
       "      <td>1.453205</td>\n",
       "      <td>2.050083</td>\n",
       "      <td>1.520920</td>\n",
       "      <td>{'lr': 0.0002553399922608638, 'dropout_prob': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.830737</td>\n",
       "      <td>0.513131</td>\n",
       "      <td>1.227953</td>\n",
       "      <td>2.022883</td>\n",
       "      <td>2.861890</td>\n",
       "      <td>0.860659</td>\n",
       "      <td>{'lr': 1.7223038444033673e-05, 'dropout_prob':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.248935</td>\n",
       "      <td>0.886722</td>\n",
       "      <td>1.920040</td>\n",
       "      <td>2.771035</td>\n",
       "      <td>3.790356</td>\n",
       "      <td>0.661265</td>\n",
       "      <td>{'lr': 1.0093409334112497e-05, 'dropout_prob':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.408769</td>\n",
       "      <td>0.679306</td>\n",
       "      <td>1.666788</td>\n",
       "      <td>2.502117</td>\n",
       "      <td>3.518366</td>\n",
       "      <td>0.653402</td>\n",
       "      <td>{'lr': 1.2190224984562657e-05, 'dropout_prob':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>1.540365</td>\n",
       "      <td>0.876328</td>\n",
       "      <td>0.541653</td>\n",
       "      <td>1.361522</td>\n",
       "      <td>2.039991</td>\n",
       "      <td>1.446638</td>\n",
       "      <td>{'lr': 2.3097356466409334e-05, 'dropout_prob':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.389965</td>\n",
       "      <td>0.696407</td>\n",
       "      <td>1.689656</td>\n",
       "      <td>2.526496</td>\n",
       "      <td>3.543914</td>\n",
       "      <td>0.650105</td>\n",
       "      <td>{'lr': 1.2089984414346552e-05, 'dropout_prob':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            model      type         1         2         3         4         5  \\\n",
       "0   comb_reg_deep  weighted  1.393811  0.870844  0.869801  1.536806  2.297815   \n",
       "1   comb_reg_deep  weighted  1.085239  0.628376  1.017290  1.863034  2.829652   \n",
       "2   comb_reg_deep  weighted  1.709774  1.043727  0.540656  1.181194  2.062841   \n",
       "3   comb_reg_deep  weighted  1.425002  1.252953  1.516907  2.155115  3.191589   \n",
       "4   comb_reg_deep  weighted  1.598624  1.046183  0.746520  1.453205  2.050083   \n",
       "..            ...       ...       ...       ...       ...       ...       ...   \n",
       "70  comb_reg_deep  weighted  0.830737  0.513131  1.227953  2.022883  2.861890   \n",
       "71  comb_reg_deep  weighted  0.248935  0.886722  1.920040  2.771035  3.790356   \n",
       "72  comb_reg_deep  weighted  0.408769  0.679306  1.666788  2.502117  3.518366   \n",
       "73  comb_reg_deep  weighted  1.540365  0.876328  0.541653  1.361522  2.039991   \n",
       "74  comb_reg_deep  weighted  0.389965  0.696407  1.689656  2.526496  3.543914   \n",
       "\n",
       "     overall                                             config  \n",
       "0   1.332119  {'lr': 0.0005288622240373623, 'dropout_prob': ...  \n",
       "1   1.064307  {'lr': 4.896278079138998e-05, 'dropout_prob': ...  \n",
       "2   1.606519  {'lr': 8.546548868916738e-05, 'dropout_prob': ...  \n",
       "3   1.468563  {'lr': 0.007056872698036484, 'dropout_prob': 0...  \n",
       "4   1.520920  {'lr': 0.0002553399922608638, 'dropout_prob': ...  \n",
       "..       ...                                                ...  \n",
       "70  0.860659  {'lr': 1.7223038444033673e-05, 'dropout_prob':...  \n",
       "71  0.661265  {'lr': 1.0093409334112497e-05, 'dropout_prob':...  \n",
       "72  0.653402  {'lr': 1.2190224984562657e-05, 'dropout_prob':...  \n",
       "73  1.446638  {'lr': 2.3097356466409334e-05, 'dropout_prob':...  \n",
       "74  0.650105  {'lr': 1.2089984414346552e-05, 'dropout_prob':...  \n",
       "\n",
       "[75 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run deep search for FITBIT dataset\n",
    "results_fitbit_reg = run_deep_search_fitbit_reg(fitbit_reg_dict, use_sample_weights=True, model_name=\"fitbit_reg_deep\", n_trials=75)\n",
    "save_results_pickle(results_fitbit_reg, \"fitbit_reg_deep\")\n",
    "results_fitbit_reg[\"mean_rmse\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadc2186-e11d-423c-b65f-4c946b302322",
   "metadata": {},
   "source": [
    "### Unweighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e41c5b8-8906-46c0-ba65-68f23e0f13b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to search/comb_reg_deep_nw_deep_search_results.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>type</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>overall</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.219079</td>\n",
       "      <td>0.772274</td>\n",
       "      <td>1.724489</td>\n",
       "      <td>2.688882</td>\n",
       "      <td>3.686512</td>\n",
       "      <td>0.607270</td>\n",
       "      <td>{'lr': 0.0001445914452794981, 'num_epochs': 7,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.227203</td>\n",
       "      <td>0.745711</td>\n",
       "      <td>1.702541</td>\n",
       "      <td>2.681855</td>\n",
       "      <td>3.660913</td>\n",
       "      <td>0.601519</td>\n",
       "      <td>{'lr': 6.0557504510913714e-05, 'num_epochs': 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.415848</td>\n",
       "      <td>0.799626</td>\n",
       "      <td>1.659092</td>\n",
       "      <td>2.647804</td>\n",
       "      <td>3.455087</td>\n",
       "      <td>0.682556</td>\n",
       "      <td>{'lr': 0.007148172666773336, 'num_epochs': 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.231826</td>\n",
       "      <td>0.765090</td>\n",
       "      <td>1.712052</td>\n",
       "      <td>2.630040</td>\n",
       "      <td>3.649265</td>\n",
       "      <td>0.603878</td>\n",
       "      <td>{'lr': 0.00012275979082630953, 'num_epochs': 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.325820</td>\n",
       "      <td>0.711415</td>\n",
       "      <td>1.633101</td>\n",
       "      <td>2.667351</td>\n",
       "      <td>3.506835</td>\n",
       "      <td>0.625045</td>\n",
       "      <td>{'lr': 0.0016120799177080705, 'num_epochs': 6,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.222728</td>\n",
       "      <td>0.767614</td>\n",
       "      <td>1.734866</td>\n",
       "      <td>2.676031</td>\n",
       "      <td>3.664791</td>\n",
       "      <td>0.606583</td>\n",
       "      <td>{'lr': 3.9226215913848744e-05, 'num_epochs': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.262880</td>\n",
       "      <td>0.706867</td>\n",
       "      <td>1.641748</td>\n",
       "      <td>2.626062</td>\n",
       "      <td>3.615995</td>\n",
       "      <td>0.597381</td>\n",
       "      <td>{'lr': 0.00019525882860361605, 'num_epochs': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.262867</td>\n",
       "      <td>0.704961</td>\n",
       "      <td>1.643646</td>\n",
       "      <td>2.619193</td>\n",
       "      <td>3.614400</td>\n",
       "      <td>0.597078</td>\n",
       "      <td>{'lr': 0.00015336843667400446, 'num_epochs': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.254887</td>\n",
       "      <td>0.727488</td>\n",
       "      <td>1.676276</td>\n",
       "      <td>2.627156</td>\n",
       "      <td>3.676336</td>\n",
       "      <td>0.602054</td>\n",
       "      <td>{'lr': 0.00011369846686143195, 'num_epochs': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.263074</td>\n",
       "      <td>0.704656</td>\n",
       "      <td>1.644507</td>\n",
       "      <td>2.617591</td>\n",
       "      <td>3.610906</td>\n",
       "      <td>0.596927</td>\n",
       "      <td>{'lr': 0.00014972691998344572, 'num_epochs': 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            model          type         1         2         3         4  \\\n",
       "0   comb_reg_deep  not weighted  0.219079  0.772274  1.724489  2.688882   \n",
       "1   comb_reg_deep  not weighted  0.227203  0.745711  1.702541  2.681855   \n",
       "2   comb_reg_deep  not weighted  0.415848  0.799626  1.659092  2.647804   \n",
       "3   comb_reg_deep  not weighted  0.231826  0.765090  1.712052  2.630040   \n",
       "4   comb_reg_deep  not weighted  0.325820  0.711415  1.633101  2.667351   \n",
       "..            ...           ...       ...       ...       ...       ...   \n",
       "70  comb_reg_deep  not weighted  0.222728  0.767614  1.734866  2.676031   \n",
       "71  comb_reg_deep  not weighted  0.262880  0.706867  1.641748  2.626062   \n",
       "72  comb_reg_deep  not weighted  0.262867  0.704961  1.643646  2.619193   \n",
       "73  comb_reg_deep  not weighted  0.254887  0.727488  1.676276  2.627156   \n",
       "74  comb_reg_deep  not weighted  0.263074  0.704656  1.644507  2.617591   \n",
       "\n",
       "           5   overall                                             config  \n",
       "0   3.686512  0.607270  {'lr': 0.0001445914452794981, 'num_epochs': 7,...  \n",
       "1   3.660913  0.601519  {'lr': 6.0557504510913714e-05, 'num_epochs': 7...  \n",
       "2   3.455087  0.682556  {'lr': 0.007148172666773336, 'num_epochs': 6, ...  \n",
       "3   3.649265  0.603878  {'lr': 0.00012275979082630953, 'num_epochs': 8...  \n",
       "4   3.506835  0.625045  {'lr': 0.0016120799177080705, 'num_epochs': 6,...  \n",
       "..       ...       ...                                                ...  \n",
       "70  3.664791  0.606583  {'lr': 3.9226215913848744e-05, 'num_epochs': 1...  \n",
       "71  3.615995  0.597381  {'lr': 0.00019525882860361605, 'num_epochs': 1...  \n",
       "72  3.614400  0.597078  {'lr': 0.00015336843667400446, 'num_epochs': 1...  \n",
       "73  3.676336  0.602054  {'lr': 0.00011369846686143195, 'num_epochs': 1...  \n",
       "74  3.610906  0.596927  {'lr': 0.00014972691998344572, 'num_epochs': 1...  \n",
       "\n",
       "[75 rows x 9 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_comb_reg = run_deep_search_comb_reg(comb_reg_dict, use_sample_weights=False, model_name=\"comb_reg_deep\", n_trials=75)\n",
    "save_results_pickle(results_comb_reg, \"comb_reg_deep_nw\")\n",
    "results_comb_reg[\"mean_rmse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83d79d61-eab7-45c4-84ff-3de0e62be435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to search/fitbit_reg_deep_nw_deep_search_results.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>type</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>overall</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fitbit_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.301444</td>\n",
       "      <td>0.772690</td>\n",
       "      <td>1.660792</td>\n",
       "      <td>2.683249</td>\n",
       "      <td>3.657459</td>\n",
       "      <td>0.630615</td>\n",
       "      <td>{'dropout_prob': 0.3326082417076779, 'n_filter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fitbit_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.327491</td>\n",
       "      <td>0.711077</td>\n",
       "      <td>1.665755</td>\n",
       "      <td>2.582421</td>\n",
       "      <td>3.602582</td>\n",
       "      <td>0.622867</td>\n",
       "      <td>{'dropout_prob': 0.3068599720375237, 'n_filter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fitbit_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.292815</td>\n",
       "      <td>0.734227</td>\n",
       "      <td>1.704416</td>\n",
       "      <td>2.666346</td>\n",
       "      <td>3.608171</td>\n",
       "      <td>0.621249</td>\n",
       "      <td>{'dropout_prob': 0.49178864075773265, 'n_filte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fitbit_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.355127</td>\n",
       "      <td>0.723047</td>\n",
       "      <td>1.658068</td>\n",
       "      <td>2.631583</td>\n",
       "      <td>3.655334</td>\n",
       "      <td>0.641610</td>\n",
       "      <td>{'dropout_prob': 0.24133697341394322, 'n_filte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fitbit_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.366435</td>\n",
       "      <td>0.742917</td>\n",
       "      <td>1.670036</td>\n",
       "      <td>2.575252</td>\n",
       "      <td>3.600745</td>\n",
       "      <td>0.647544</td>\n",
       "      <td>{'dropout_prob': 0.12301782875485451, 'n_filte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>fitbit_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.364246</td>\n",
       "      <td>0.723072</td>\n",
       "      <td>1.632760</td>\n",
       "      <td>2.610837</td>\n",
       "      <td>3.644825</td>\n",
       "      <td>0.644291</td>\n",
       "      <td>{'dropout_prob': 0.3186437999665475, 'n_filter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>fitbit_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.277437</td>\n",
       "      <td>0.764302</td>\n",
       "      <td>1.708684</td>\n",
       "      <td>2.607566</td>\n",
       "      <td>3.617004</td>\n",
       "      <td>0.616972</td>\n",
       "      <td>{'dropout_prob': 0.4629268393784858, 'n_filter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>fitbit_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.276947</td>\n",
       "      <td>0.763884</td>\n",
       "      <td>1.706084</td>\n",
       "      <td>2.609960</td>\n",
       "      <td>3.610045</td>\n",
       "      <td>0.616613</td>\n",
       "      <td>{'dropout_prob': 0.45369056974403743, 'n_filte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>fitbit_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.278165</td>\n",
       "      <td>0.763893</td>\n",
       "      <td>1.707058</td>\n",
       "      <td>2.604729</td>\n",
       "      <td>3.611156</td>\n",
       "      <td>0.616743</td>\n",
       "      <td>{'dropout_prob': 0.46894321051414145, 'n_filte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>fitbit_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.258224</td>\n",
       "      <td>0.778918</td>\n",
       "      <td>1.759647</td>\n",
       "      <td>2.714607</td>\n",
       "      <td>3.623247</td>\n",
       "      <td>0.624672</td>\n",
       "      <td>{'dropout_prob': 0.49899119723462176, 'n_filte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              model          type         1         2         3         4  \\\n",
       "0   fitbit_reg_deep  not weighted  0.301444  0.772690  1.660792  2.683249   \n",
       "1   fitbit_reg_deep  not weighted  0.327491  0.711077  1.665755  2.582421   \n",
       "2   fitbit_reg_deep  not weighted  0.292815  0.734227  1.704416  2.666346   \n",
       "3   fitbit_reg_deep  not weighted  0.355127  0.723047  1.658068  2.631583   \n",
       "4   fitbit_reg_deep  not weighted  0.366435  0.742917  1.670036  2.575252   \n",
       "..              ...           ...       ...       ...       ...       ...   \n",
       "70  fitbit_reg_deep  not weighted  0.364246  0.723072  1.632760  2.610837   \n",
       "71  fitbit_reg_deep  not weighted  0.277437  0.764302  1.708684  2.607566   \n",
       "72  fitbit_reg_deep  not weighted  0.276947  0.763884  1.706084  2.609960   \n",
       "73  fitbit_reg_deep  not weighted  0.278165  0.763893  1.707058  2.604729   \n",
       "74  fitbit_reg_deep  not weighted  0.258224  0.778918  1.759647  2.714607   \n",
       "\n",
       "           5   overall                                             config  \n",
       "0   3.657459  0.630615  {'dropout_prob': 0.3326082417076779, 'n_filter...  \n",
       "1   3.602582  0.622867  {'dropout_prob': 0.3068599720375237, 'n_filter...  \n",
       "2   3.608171  0.621249  {'dropout_prob': 0.49178864075773265, 'n_filte...  \n",
       "3   3.655334  0.641610  {'dropout_prob': 0.24133697341394322, 'n_filte...  \n",
       "4   3.600745  0.647544  {'dropout_prob': 0.12301782875485451, 'n_filte...  \n",
       "..       ...       ...                                                ...  \n",
       "70  3.644825  0.644291  {'dropout_prob': 0.3186437999665475, 'n_filter...  \n",
       "71  3.617004  0.616972  {'dropout_prob': 0.4629268393784858, 'n_filter...  \n",
       "72  3.610045  0.616613  {'dropout_prob': 0.45369056974403743, 'n_filte...  \n",
       "73  3.611156  0.616743  {'dropout_prob': 0.46894321051414145, 'n_filte...  \n",
       "74  3.623247  0.624672  {'dropout_prob': 0.49899119723462176, 'n_filte...  \n",
       "\n",
       "[75 rows x 9 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_fitbit_reg = run_deep_search_fitbit_reg(fitbit_reg_dict, use_sample_weights=False, model_name=\"fitbit_reg_deep\", n_trials=75)\n",
    "save_results_pickle(results_fitbit_reg, \"fitbit_reg_deep_nw\")\n",
    "results_fitbit_reg[\"mean_rmse\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e6def1-1794-4260-8b85-8b6f8a36fea9",
   "metadata": {},
   "source": [
    "###  Deep Search Objective Functions for classifcation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8389af99-d0f7-4501-837d-69afa45c334a",
   "metadata": {},
   "source": [
    "### Classification Deep Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b2345-220e-4ab9-b45e-b73b13141bb0",
   "metadata": {},
   "source": [
    "Comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a187162-c2f7-4cf8-970f-6c3ea50ef294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-weighted version\n",
    "def objective_classification_comb_nw(trial, data_dict):\n",
    "    \"\"\"\n",
    "    Non-weighted CNN classification for comb_class.\n",
    "    Tuned hyperparameters:\n",
    "      - batch_size\n",
    "      - n_filters_0\n",
    "    \"\"\"\n",
    "    set_seed(42)\n",
    "    splits = get_stratified_cv_splits(\n",
    "        data_dict['train'], subject_id=\"PatientID\", target_var=\"is_SI\", n_splits=5\n",
    "    )\n",
    "    overall_acc_list = []\n",
    "    overall_sens_list = []\n",
    "    overall_spec_list = []\n",
    "    \n",
    "    batch_size = trial.suggest_int(\"batch_size\", 16, 64, step=16)\n",
    "    n_filters_0 = trial.suggest_int(\"n_filters_0\", 8, 64, step=8)\n",
    "    \n",
    "    dropout_prob = 0.2\n",
    "    lr = 1e-3\n",
    "    num_epochs = 10\n",
    "    \n",
    "    for train_split, val_split in splits:\n",
    "        train_df, _ = create_subject_dataset(train_split, outcome_col=\"is_SI\", use_weights=False)\n",
    "        val_df, _ = create_subject_dataset(val_split, outcome_col=\"is_SI\", use_weights=False)\n",
    "        \n",
    "        X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "        y_train = train_df[\"is_SI\"].values.astype(np.float32)\n",
    "        X_val   = np.stack(val_df[\"X\"].values, axis=0)\n",
    "        y_val   = val_df[\"is_SI\"].values.astype(np.float32)\n",
    "        \n",
    "        conv = nn.Conv1d(in_channels=X_train.shape[1],\n",
    "                         out_channels=n_filters_0,\n",
    "                         kernel_size=3,\n",
    "                         stride=1,\n",
    "                         padding=1)\n",
    "        layers = [conv, nn.ReLU(), nn.Dropout(dropout_prob)]\n",
    "        conv_net = nn.Sequential(*layers)\n",
    "        flattened_dim = n_filters_0 * X_train.shape[2]\n",
    "        fc_net = nn.Sequential(nn.Linear(flattened_dim, 1))\n",
    "        \n",
    "        class CombClassCNN(nn.Module):\n",
    "            def __init__(self, conv_net, fc_net):\n",
    "                super(CombClassCNN, self).__init__()\n",
    "                self.conv = conv_net\n",
    "                self.fc = fc_net\n",
    "            def forward(self, x):\n",
    "                x = self.conv(x)\n",
    "                x = x.view(x.size(0), -1)\n",
    "                return self.fc(x)\n",
    "                \n",
    "        model = CombClassCNN(conv_net, fc_net).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "            \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        train_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = loss_fn(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "            logits = model(X_val_tensor)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().reshape(-1)\n",
    "            preds = (probs >= 0.5).astype(np.float32)\n",
    "        fold_acc = np.mean(preds == y_val)\n",
    "        TP = np.sum((preds == 1) & (y_val == 1))\n",
    "        FN = np.sum((preds == 0) & (y_val == 1))\n",
    "        sensitivity = TP/(TP+FN) if (TP+FN) > 0 else np.nan\n",
    "        TN = np.sum((preds == 0) & (y_val == 0))\n",
    "        FP = np.sum((preds == 1) & (y_val == 0))\n",
    "        specificity = TN/(TN+FP) if (TN+FP) > 0 else np.nan\n",
    "        \n",
    "        overall_acc_list.append(fold_acc)\n",
    "        overall_sens_list.append(sensitivity)\n",
    "        overall_spec_list.append(specificity)\n",
    "        \n",
    "    mean_acc = np.mean(overall_acc_list)\n",
    "    std_acc = np.std(overall_acc_list)\n",
    "    mean_sens = np.mean(overall_sens_list)\n",
    "    std_sens = np.std(overall_sens_list)\n",
    "    mean_spec = np.mean(overall_spec_list)\n",
    "    std_spec = np.std(overall_spec_list)\n",
    "    \n",
    "    trial.set_user_attr(\"overall_acc_mean\", mean_acc)\n",
    "    trial.set_user_attr(\"overall_acc_std\", std_acc)\n",
    "    trial.set_user_attr(\"sensitivity_mean\", mean_sens)\n",
    "    trial.set_user_attr(\"sensitivity_std\", std_sens)\n",
    "    trial.set_user_attr(\"specificity_mean\", mean_spec)\n",
    "    trial.set_user_attr(\"specificity_std\", std_spec)\n",
    "    \n",
    "    return 1 - mean_acc\n",
    "\n",
    "def objective_classification_comb(trial, data_dict):\n",
    "    \"\"\"\n",
    "    Objective function for comb_class.\n",
    "    Tunable hyperparameters:\n",
    "      - lr\n",
    "      - dropout_prob\n",
    "      - num_epochs\n",
    "      - batch_size\n",
    "      - n_filters_0\n",
    "    \"\"\"\n",
    "    set_seed(42)\n",
    "    splits = get_stratified_cv_splits(data_dict['train'], subject_id=\"PatientID\", target_var=\"is_SI\", n_splits=5)\n",
    "\n",
    "    overall_acc_list = []\n",
    "    overall_sens_list = []\n",
    "    overall_spec_list = []\n",
    "\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    dropout_prob = trial.suggest_float(\"dropout_prob\", 0.1, 0.5)\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 5, 10)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 16, 64, step=16)\n",
    "    n_filters_0 = trial.suggest_int(\"n_filters_0\", 8, 64, step=8)\n",
    "\n",
    "    for train_split, val_split in splits:\n",
    "        train_df, _ = create_subject_dataset(train_split, outcome_col=\"is_SI\")\n",
    "        val_df, _ = create_subject_dataset(val_split, outcome_col=\"is_SI\")\n",
    "\n",
    "        X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "        y_train = train_df[\"is_SI\"].values.astype(np.float32)\n",
    "\n",
    "        X_val = np.stack(val_df[\"X\"].values, axis=0)\n",
    "        y_val = val_df[\"is_SI\"].values.astype(np.float32)\n",
    "\n",
    "        conv = nn.Conv1d(in_channels=X_train.shape[1],\n",
    "                         out_channels=n_filters_0,\n",
    "                         kernel_size=3,\n",
    "                         stride=1,\n",
    "                         padding=1)\n",
    "        layers = [conv, nn.ReLU(), nn.Dropout(dropout_prob)]\n",
    "        conv_net = nn.Sequential(*layers)\n",
    "        flattened_dim = n_filters_0 * X_train.shape[2]\n",
    "        fc_net = nn.Sequential(nn.Linear(flattened_dim, 1))\n",
    "\n",
    "        class CombClassCNN(nn.Module):\n",
    "            def __init__(self, conv_net, fc_net):\n",
    "                super(CombClassCNN, self).__init__()\n",
    "                self.conv = conv_net\n",
    "                self.fc = fc_net\n",
    "            def forward(self, x):\n",
    "                x = self.conv(x)\n",
    "                x = x.view(x.size(0), -1)\n",
    "                return self.fc(x)\n",
    "\n",
    "        model = CombClassCNN(conv_net, fc_net).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        train_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = loss_fn(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "            logits = model(X_val_tensor)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().reshape(-1)\n",
    "            preds = (probs >= 0.5).astype(np.float32)\n",
    "        fold_acc = np.mean(preds == y_val)\n",
    "        TP = np.sum((preds == 1) & (y_val == 1))\n",
    "        FN = np.sum((preds == 0) & (y_val == 1))\n",
    "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "        TN = np.sum((preds == 0) & (y_val == 0))\n",
    "        FP = np.sum((preds == 1) & (y_val == 0))\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
    "\n",
    "        overall_acc_list.append(fold_acc)\n",
    "        overall_sens_list.append(sensitivity)\n",
    "        overall_spec_list.append(specificity)\n",
    "\n",
    "    mean_acc = np.mean(overall_acc_list)\n",
    "    std_acc = np.std(overall_acc_list)\n",
    "    mean_sens = np.mean(overall_sens_list)\n",
    "    std_sens = np.std(overall_sens_list)\n",
    "    mean_spec = np.mean(overall_spec_list)\n",
    "    std_spec = np.std(overall_spec_list)\n",
    "\n",
    "    trial.set_user_attr(\"overall_acc_mean\", mean_acc)\n",
    "    trial.set_user_attr(\"overall_acc_std\", std_acc)\n",
    "    trial.set_user_attr(\"sensitivity_mean\", mean_sens)\n",
    "    trial.set_user_attr(\"sensitivity_std\", std_sens)\n",
    "    trial.set_user_attr(\"specificity_mean\", mean_spec)\n",
    "    trial.set_user_attr(\"specificity_std\", std_spec)\n",
    "\n",
    "    return 1 - mean_acc  # we minimize 1 - accuracy\n",
    "\n",
    "\n",
    "def run_deep_search_comb_class(data_dict, use_sample_weights, model_name, n_trials=75):\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    def print_progress(study, trial):\n",
    "        print(f\"{model_name} trial {len(study.trials)}/{n_trials}\\r\", end=\"\", flush=True)\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    optimize_fn = objective_classification_comb if use_sample_weights else objective_classification_comb_nw\n",
    "    study.optimize(lambda trial: optimize_fn(trial, data_dict),\n",
    "                   n_trials=n_trials, callbacks=[print_progress])\n",
    "\n",
    "    mean_rows = []\n",
    "    std_rows = []\n",
    "    for t in study.trials:\n",
    "        row_mean = {\n",
    "            \"model\": model_name,\n",
    "            \"type\": \"weighted\",\n",
    "            \"accuracy\": t.user_attrs[\"overall_acc_mean\"],\n",
    "            \"sensitivity\": t.user_attrs[\"sensitivity_mean\"],\n",
    "            \"specificity\": t.user_attrs[\"specificity_mean\"],\n",
    "            \"config\": t.params\n",
    "        }\n",
    "        row_std = {\n",
    "            \"model\": model_name,\n",
    "            \"type\": \"weighted\",\n",
    "            \"accuracy\": t.user_attrs[\"overall_acc_std\"],\n",
    "            \"sensitivity\": t.user_attrs[\"sensitivity_std\"],\n",
    "            \"specificity\": t.user_attrs[\"specificity_std\"],\n",
    "            \"config\": t.params\n",
    "        }\n",
    "        mean_rows.append(row_mean)\n",
    "        std_rows.append(row_std)\n",
    "\n",
    "    columns = [\"model\", \"type\", \"accuracy\", \"sensitivity\", \"specificity\", \"config\"]\n",
    "    mean_df = pd.DataFrame(mean_rows, columns=columns)\n",
    "    std_df = pd.DataFrame(std_rows, columns=columns)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    optimal_configuration = {\n",
    "        \"value\": best_trial.value,\n",
    "        \"params\": best_trial.params,\n",
    "        \"user_attrs\": best_trial.user_attrs\n",
    "    }\n",
    "\n",
    "    result_dict = {\n",
    "        \"mean_metrics\": mean_df,\n",
    "        \"std_metrics\": std_df,\n",
    "        \"optimal_configuration\": optimal_configuration\n",
    "    }\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3481d059-4d6b-4ecc-8c05-92c7904c146b",
   "metadata": {},
   "source": [
    "comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc16c008-bcd4-4e5d-a142-2c9bca72acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-weighted version\n",
    "def objective_classification_fitbit_nw(trial, data_dict):\n",
    "    \"\"\"\n",
    "    Non-weighted CNN classification for fitbit_class.\n",
    "    Tuned hyperparameters:\n",
    "      - num_epochs\n",
    "      - lr\n",
    "    \"\"\"\n",
    "    set_seed(42)\n",
    "    splits = get_stratified_cv_splits(\n",
    "        data_dict['train'], subject_id=\"PatientID\", target_var=\"is_SI\", n_splits=5\n",
    "    )\n",
    "    overall_acc_list = []\n",
    "    overall_sens_list = []\n",
    "    overall_spec_list = []\n",
    "    \n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 5, 10)\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    \n",
    "    n_filters_0 = 32  # fixed default\n",
    "    dropout_prob = 0.2\n",
    "    batch_size = 32\n",
    "    \n",
    "    for train_split, val_split in splits:\n",
    "        train_df, _ = create_subject_dataset(train_split, outcome_col=\"is_SI\", use_weights=False)\n",
    "        val_df, _ = create_subject_dataset(val_split, outcome_col=\"is_SI\", use_weights=False)\n",
    "        \n",
    "        X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "        y_train = train_df[\"is_SI\"].values.astype(np.float32)\n",
    "        X_val   = np.stack(val_df[\"X\"].values, axis=0)\n",
    "        y_val   = val_df[\"is_SI\"].values.astype(np.float32)\n",
    "        \n",
    "        conv = nn.Conv1d(in_channels=X_train.shape[1],\n",
    "                         out_channels=n_filters_0,\n",
    "                         kernel_size=3,\n",
    "                         stride=1,\n",
    "                         padding=1)\n",
    "        layers = [conv, nn.ReLU(), nn.Dropout(dropout_prob)]\n",
    "        conv_net = nn.Sequential(*layers)\n",
    "        flattened_dim = n_filters_0 * X_train.shape[2]\n",
    "        fc_net = nn.Sequential(nn.Linear(flattened_dim, 1))\n",
    "        \n",
    "        class FitbitClassCNN(nn.Module):\n",
    "            def __init__(self, conv_net, fc_net):\n",
    "                super(FitbitClassCNN, self).__init__()\n",
    "                self.conv = conv_net\n",
    "                self.fc = fc_net\n",
    "            def forward(self, x):\n",
    "                x = self.conv(x)\n",
    "                x = x.view(x.size(0), -1)\n",
    "                return self.fc(x)\n",
    "                \n",
    "        model = FitbitClassCNN(conv_net, fc_net).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "            \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        train_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = loss_fn(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "            logits = model(X_val_tensor)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().reshape(-1)\n",
    "            preds = (probs >= 0.5).astype(np.float32)\n",
    "        fold_acc = np.mean(preds == y_val)\n",
    "        TP = np.sum((preds == 1) & (y_val == 1))\n",
    "        FN = np.sum((preds == 0) & (y_val == 1))\n",
    "        sensitivity = TP/(TP+FN) if (TP+FN) > 0 else np.nan\n",
    "        TN = np.sum((preds == 0) & (y_val == 0))\n",
    "        FP = np.sum((preds == 1) & (y_val == 0))\n",
    "        specificity = TN/(TN+FP) if (TN+FP) > 0 else np.nan\n",
    "        \n",
    "        overall_acc_list.append(fold_acc)\n",
    "        overall_sens_list.append(sensitivity)\n",
    "        overall_spec_list.append(specificity)\n",
    "        \n",
    "    mean_acc = np.mean(overall_acc_list)\n",
    "    std_acc = np.std(overall_acc_list)\n",
    "    mean_sens = np.mean(overall_sens_list)\n",
    "    std_sens = np.std(overall_sens_list)\n",
    "    mean_spec = np.mean(overall_spec_list)\n",
    "    std_spec = np.std(overall_spec_list)\n",
    "    \n",
    "    trial.set_user_attr(\"overall_acc_mean\", mean_acc)\n",
    "    trial.set_user_attr(\"overall_acc_std\", std_acc)\n",
    "    trial.set_user_attr(\"sensitivity_mean\", mean_sens)\n",
    "    trial.set_user_attr(\"sensitivity_std\", std_sens)\n",
    "    trial.set_user_attr(\"specificity_mean\", mean_spec)\n",
    "    trial.set_user_attr(\"specificity_std\", std_spec)\n",
    "    \n",
    "    return 1 - mean_acc\n",
    "\n",
    "def objective_classification_fitbit(trial, data_dict):\n",
    "    \"\"\"\n",
    "    Objective function for fitbit_class.\n",
    "    Tunable hyperparameters:\n",
    "      - lr\n",
    "      - n_filters_0\n",
    "    Other parameters (dropout, kernel size, etc.) are fixed.\n",
    "    \"\"\"\n",
    "    set_seed(42)\n",
    "    splits = get_stratified_cv_splits(data_dict['train'], subject_id=\"PatientID\", target_var=\"is_SI\", n_splits=5)\n",
    "\n",
    "    overall_acc_list = []\n",
    "    overall_sens_list = []\n",
    "    overall_spec_list = []\n",
    "\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    n_filters_0 = trial.suggest_int(\"n_filters_0\", 8, 64, step=8)\n",
    "\n",
    "    # Fixed parameters:\n",
    "    dropout_prob = 0.2\n",
    "    kernel_size = 3\n",
    "    batch_size = 32\n",
    "\n",
    "    for train_split, val_split in splits:\n",
    "        train_df, _ = create_subject_dataset(train_split, outcome_col=\"is_SI\")\n",
    "        val_df, _ = create_subject_dataset(val_split, outcome_col=\"is_SI\")\n",
    "\n",
    "        X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "        y_train = train_df[\"is_SI\"].values.astype(np.float32)\n",
    "\n",
    "        X_val = np.stack(val_df[\"X\"].values, axis=0)\n",
    "        y_val = val_df[\"is_SI\"].values.astype(np.float32)\n",
    "\n",
    "        conv = nn.Conv1d(in_channels=X_train.shape[1],\n",
    "                         out_channels=n_filters_0,\n",
    "                         kernel_size=kernel_size,\n",
    "                         stride=1,\n",
    "                         padding=(kernel_size - 1) // 2)\n",
    "        layers = [conv, nn.ReLU(), nn.Dropout(dropout_prob)]\n",
    "        conv_net = nn.Sequential(*layers)\n",
    "        flattened_dim = n_filters_0 * X_train.shape[2]\n",
    "        fc_net = nn.Sequential(nn.Linear(flattened_dim, 1))\n",
    "\n",
    "        class FitbitClassCNN(nn.Module):\n",
    "            def __init__(self, conv_net, fc_net):\n",
    "                super(FitbitClassCNN, self).__init__()\n",
    "                self.conv = conv_net\n",
    "                self.fc = fc_net\n",
    "            def forward(self, x):\n",
    "                x = self.conv(x)\n",
    "                x = x.view(x.size(0), -1)\n",
    "                return self.fc(x)\n",
    "\n",
    "        model = FitbitClassCNN(conv_net, fc_net).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        train_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(5):  # fixed number of epochs\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = loss_fn(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "            logits = model(X_val_tensor)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().reshape(-1)\n",
    "            preds = (probs >= 0.5).astype(np.float32)\n",
    "        fold_acc = np.mean(preds == y_val)\n",
    "        TP = np.sum((preds == 1) & (y_val == 1))\n",
    "        FN = np.sum((preds == 0) & (y_val == 1))\n",
    "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "        TN = np.sum((preds == 0) & (y_val == 0))\n",
    "        FP = np.sum((preds == 1) & (y_val == 0))\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
    "\n",
    "        overall_acc_list.append(fold_acc)\n",
    "        overall_sens_list.append(sensitivity)\n",
    "        overall_spec_list.append(specificity)\n",
    "\n",
    "    mean_acc = np.mean(overall_acc_list)\n",
    "    std_acc = np.std(overall_acc_list)\n",
    "    mean_sens = np.mean(overall_sens_list)\n",
    "    std_sens = np.std(overall_sens_list)\n",
    "    mean_spec = np.mean(overall_spec_list)\n",
    "    std_spec = np.std(overall_spec_list)\n",
    "\n",
    "    trial.set_user_attr(\"overall_acc_mean\", mean_acc)\n",
    "    trial.set_user_attr(\"overall_acc_std\", std_acc)\n",
    "    trial.set_user_attr(\"sensitivity_mean\", mean_sens)\n",
    "    trial.set_user_attr(\"sensitivity_std\", std_sens)\n",
    "    trial.set_user_attr(\"specificity_mean\", mean_spec)\n",
    "    trial.set_user_attr(\"specificity_std\", std_spec)\n",
    "\n",
    "    return 1 - mean_acc\n",
    "\n",
    "\n",
    "def run_deep_search_fitbit_class(data_dict, use_sample_weights, model_name, n_trials=75):\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    def print_progress(study, trial):\n",
    "        print(f\"{model_name} trial {len(study.trials)}/{n_trials}\\r\", end=\"\", flush=True)\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    optimize_fn = objective_classification_fitbit if use_sample_weights else objective_classification_fitbit_nw\n",
    "    study.optimize(lambda trial: optimize_fn(trial, data_dict),\n",
    "                   n_trials=n_trials, callbacks=[print_progress])\n",
    "\n",
    "    mean_rows = []\n",
    "    std_rows = []\n",
    "    for t in study.trials:\n",
    "        row_mean = {\n",
    "            \"model\": model_name,\n",
    "            \"type\": \"weighted\",\n",
    "            \"accuracy\": t.user_attrs[\"overall_acc_mean\"],\n",
    "            \"sensitivity\": t.user_attrs[\"sensitivity_mean\"],\n",
    "            \"specificity\": t.user_attrs[\"specificity_mean\"],\n",
    "            \"config\": t.params\n",
    "        }\n",
    "        row_std = {\n",
    "            \"model\": model_name,\n",
    "            \"type\": \"weighted\",\n",
    "            \"accuracy\": t.user_attrs[\"overall_acc_std\"],\n",
    "            \"sensitivity\": t.user_attrs[\"sensitivity_std\"],\n",
    "            \"specificity\": t.user_attrs[\"specificity_std\"],\n",
    "            \"config\": t.params\n",
    "        }\n",
    "        mean_rows.append(row_mean)\n",
    "        std_rows.append(row_std)\n",
    "\n",
    "    columns = [\"model\", \"type\", \"accuracy\", \"sensitivity\", \"specificity\", \"config\"]\n",
    "    mean_df = pd.DataFrame(mean_rows, columns=columns)\n",
    "    std_df = pd.DataFrame(std_rows, columns=columns)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    optimal_configuration = {\n",
    "        \"value\": best_trial.value,\n",
    "        \"params\": best_trial.params,\n",
    "        \"user_attrs\": best_trial.user_attrs\n",
    "    }\n",
    "\n",
    "    result_dict = {\n",
    "        \"mean_metrics\": mean_df,\n",
    "        \"std_metrics\": std_df,\n",
    "        \"optimal_configuration\": optimal_configuration\n",
    "    }\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b516c6-5192-40bc-9f75-4944736e7ee7",
   "metadata": {},
   "source": [
    "### Run Deep Search for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0c7bca-f884-49b2-a352-2a17da4aca13",
   "metadata": {},
   "source": [
    "### Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad2a3e2e-a6c3-4dc0-9894-64516b0a5ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to search/comb_class_deep_deep_search_results.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.763958</td>\n",
       "      <td>0.065828</td>\n",
       "      <td>0.982418</td>\n",
       "      <td>{'lr': 0.00356660993595182, 'dropout_prob': 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.748869</td>\n",
       "      <td>0.143010</td>\n",
       "      <td>0.938462</td>\n",
       "      <td>{'lr': 0.0005955331235282746, 'dropout_prob': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.762279</td>\n",
       "      <td>0.004706</td>\n",
       "      <td>0.999267</td>\n",
       "      <td>{'lr': 1.7974344160643104e-05, 'dropout_prob':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.762279</td>\n",
       "      <td>0.016471</td>\n",
       "      <td>0.995604</td>\n",
       "      <td>{'lr': 0.0006468836215398885, 'dropout_prob': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.759484</td>\n",
       "      <td>0.032722</td>\n",
       "      <td>0.986813</td>\n",
       "      <td>{'lr': 0.0013520870967612822, 'dropout_prob': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.761720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'lr': 7.289082720609982e-05, 'dropout_prob': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.765629</td>\n",
       "      <td>0.018769</td>\n",
       "      <td>0.999267</td>\n",
       "      <td>{'lr': 0.00010213087804239786, 'dropout_prob':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.766183</td>\n",
       "      <td>0.037483</td>\n",
       "      <td>0.994139</td>\n",
       "      <td>{'lr': 0.00014350779165287232, 'dropout_prob':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.766186</td>\n",
       "      <td>0.025773</td>\n",
       "      <td>0.997802</td>\n",
       "      <td>{'lr': 0.00012117497511970936, 'dropout_prob':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.764512</td>\n",
       "      <td>0.021149</td>\n",
       "      <td>0.997070</td>\n",
       "      <td>{'lr': 0.00015636370236471572, 'dropout_prob':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              model      type  accuracy  sensitivity  specificity  \\\n",
       "0   comb_class_deep  weighted  0.763958     0.065828     0.982418   \n",
       "1   comb_class_deep  weighted  0.748869     0.143010     0.938462   \n",
       "2   comb_class_deep  weighted  0.762279     0.004706     0.999267   \n",
       "3   comb_class_deep  weighted  0.762279     0.016471     0.995604   \n",
       "4   comb_class_deep  weighted  0.759484     0.032722     0.986813   \n",
       "..              ...       ...       ...          ...          ...   \n",
       "70  comb_class_deep  weighted  0.761720     0.000000     1.000000   \n",
       "71  comb_class_deep  weighted  0.765629     0.018769     0.999267   \n",
       "72  comb_class_deep  weighted  0.766183     0.037483     0.994139   \n",
       "73  comb_class_deep  weighted  0.766186     0.025773     0.997802   \n",
       "74  comb_class_deep  weighted  0.764512     0.021149     0.997070   \n",
       "\n",
       "                                               config  \n",
       "0   {'lr': 0.00356660993595182, 'dropout_prob': 0....  \n",
       "1   {'lr': 0.0005955331235282746, 'dropout_prob': ...  \n",
       "2   {'lr': 1.7974344160643104e-05, 'dropout_prob':...  \n",
       "3   {'lr': 0.0006468836215398885, 'dropout_prob': ...  \n",
       "4   {'lr': 0.0013520870967612822, 'dropout_prob': ...  \n",
       "..                                                ...  \n",
       "70  {'lr': 7.289082720609982e-05, 'dropout_prob': ...  \n",
       "71  {'lr': 0.00010213087804239786, 'dropout_prob':...  \n",
       "72  {'lr': 0.00014350779165287232, 'dropout_prob':...  \n",
       "73  {'lr': 0.00012117497511970936, 'dropout_prob':...  \n",
       "74  {'lr': 0.00015636370236471572, 'dropout_prob':...  \n",
       "\n",
       "[75 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_comb_class = run_deep_search_comb_class(comb_class_dict, use_sample_weights=True, model_name=\"comb_class_deep\", n_trials=75)\n",
    "save_results_pickle(results_comb_class, \"comb_class_deep\")\n",
    "results_comb_class[\"mean_metrics\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca28ccf-1577-48fb-a387-20d3c3982c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fitbit_class = run_deep_search_fitbit_class(fitbit_class_dict, use_sample_weights=True, model_name=\"fitbit_class_deep\", n_trials=75)\n",
    "save_results_pickle(results_fitbit_class, \"fitbit_class_deep\")\n",
    "results_fitbit_class[\"mean_metrics\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461dd4b2-05bb-4eef-a7c7-3fce52436b12",
   "metadata": {},
   "source": [
    "### Unweighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2bfdbf41-5fa8-4577-b15b-50ead2621446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to search/comb_class_deep_nw_deep_search_results.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.763390</td>\n",
       "      <td>0.065609</td>\n",
       "      <td>0.981685</td>\n",
       "      <td>{'batch_size': 48, 'n_filters_0': 8}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.709254</td>\n",
       "      <td>0.182763</td>\n",
       "      <td>0.873993</td>\n",
       "      <td>{'batch_size': 32, 'n_filters_0': 64}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.734371</td>\n",
       "      <td>0.168618</td>\n",
       "      <td>0.911355</td>\n",
       "      <td>{'batch_size': 48, 'n_filters_0': 32}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.753904</td>\n",
       "      <td>0.112449</td>\n",
       "      <td>0.954579</td>\n",
       "      <td>{'batch_size': 64, 'n_filters_0': 16}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.746647</td>\n",
       "      <td>0.163776</td>\n",
       "      <td>0.928938</td>\n",
       "      <td>{'batch_size': 64, 'n_filters_0': 24}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.182408</td>\n",
       "      <td>0.906960</td>\n",
       "      <td>{'batch_size': 48, 'n_filters_0': 24}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.763390</td>\n",
       "      <td>0.065609</td>\n",
       "      <td>0.981685</td>\n",
       "      <td>{'batch_size': 48, 'n_filters_0': 8}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.763390</td>\n",
       "      <td>0.065609</td>\n",
       "      <td>0.981685</td>\n",
       "      <td>{'batch_size': 48, 'n_filters_0': 8}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.763390</td>\n",
       "      <td>0.065609</td>\n",
       "      <td>0.981685</td>\n",
       "      <td>{'batch_size': 48, 'n_filters_0': 8}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.763390</td>\n",
       "      <td>0.065609</td>\n",
       "      <td>0.981685</td>\n",
       "      <td>{'batch_size': 48, 'n_filters_0': 8}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              model      type  accuracy  sensitivity  specificity  \\\n",
       "0   comb_class_deep  weighted  0.763390     0.065609     0.981685   \n",
       "1   comb_class_deep  weighted  0.709254     0.182763     0.873993   \n",
       "2   comb_class_deep  weighted  0.734371     0.168618     0.911355   \n",
       "3   comb_class_deep  weighted  0.753904     0.112449     0.954579   \n",
       "4   comb_class_deep  weighted  0.746647     0.163776     0.928938   \n",
       "..              ...       ...       ...          ...          ...   \n",
       "70  comb_class_deep  weighted  0.734375     0.182408     0.906960   \n",
       "71  comb_class_deep  weighted  0.763390     0.065609     0.981685   \n",
       "72  comb_class_deep  weighted  0.763390     0.065609     0.981685   \n",
       "73  comb_class_deep  weighted  0.763390     0.065609     0.981685   \n",
       "74  comb_class_deep  weighted  0.763390     0.065609     0.981685   \n",
       "\n",
       "                                   config  \n",
       "0    {'batch_size': 48, 'n_filters_0': 8}  \n",
       "1   {'batch_size': 32, 'n_filters_0': 64}  \n",
       "2   {'batch_size': 48, 'n_filters_0': 32}  \n",
       "3   {'batch_size': 64, 'n_filters_0': 16}  \n",
       "4   {'batch_size': 64, 'n_filters_0': 24}  \n",
       "..                                    ...  \n",
       "70  {'batch_size': 48, 'n_filters_0': 24}  \n",
       "71   {'batch_size': 48, 'n_filters_0': 8}  \n",
       "72   {'batch_size': 48, 'n_filters_0': 8}  \n",
       "73   {'batch_size': 48, 'n_filters_0': 8}  \n",
       "74   {'batch_size': 48, 'n_filters_0': 8}  \n",
       "\n",
       "[75 rows x 6 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_comb_class = run_deep_search_comb_class(comb_class_dict, use_sample_weights=False, model_name=\"comb_class_deep\", n_trials=75)\n",
    "save_results_pickle(results_comb_class, \"comb_class_deep_nw\")\n",
    "results_comb_class[\"mean_metrics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b13f4a49-c1c4-4bc7-aa06-999ad23314f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to search/fitbit_class_deep_nw_deep_search_results.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.701416</td>\n",
       "      <td>0.185062</td>\n",
       "      <td>0.863004</td>\n",
       "      <td>{'num_epochs': 10, 'lr': 0.0024939966767557526}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.761162</td>\n",
       "      <td>0.002353</td>\n",
       "      <td>0.998535</td>\n",
       "      <td>{'num_epochs': 5, 'lr': 1.5009952197841239e-05}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.730477</td>\n",
       "      <td>0.112230</td>\n",
       "      <td>0.923810</td>\n",
       "      <td>{'num_epochs': 6, 'lr': 0.0033768338913100727}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.762277</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'num_epochs': 6, 'lr': 0.00014935673948754337}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.738270</td>\n",
       "      <td>0.096033</td>\n",
       "      <td>0.939194</td>\n",
       "      <td>{'num_epochs': 5, 'lr': 0.004765277633376154}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.763955</td>\n",
       "      <td>0.023502</td>\n",
       "      <td>0.995604</td>\n",
       "      <td>{'num_epochs': 9, 'lr': 0.00023162552300880473}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.761163</td>\n",
       "      <td>0.060985</td>\n",
       "      <td>0.980220</td>\n",
       "      <td>{'num_epochs': 9, 'lr': 0.00045748783190070806}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.763955</td>\n",
       "      <td>0.030534</td>\n",
       "      <td>0.993407</td>\n",
       "      <td>{'num_epochs': 9, 'lr': 0.0003002825055670167}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.761165</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.997070</td>\n",
       "      <td>{'num_epochs': 8, 'lr': 0.00019952281945488572}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.755581</td>\n",
       "      <td>0.089029</td>\n",
       "      <td>0.964103</td>\n",
       "      <td>{'num_epochs': 9, 'lr': 0.0006090482579264076}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                model      type  accuracy  sensitivity  specificity  \\\n",
       "0   fitbit_class_deep  weighted  0.701416     0.185062     0.863004   \n",
       "1   fitbit_class_deep  weighted  0.761162     0.002353     0.998535   \n",
       "2   fitbit_class_deep  weighted  0.730477     0.112230     0.923810   \n",
       "3   fitbit_class_deep  weighted  0.762277     0.002326     1.000000   \n",
       "4   fitbit_class_deep  weighted  0.738270     0.096033     0.939194   \n",
       "..                ...       ...       ...          ...          ...   \n",
       "70  fitbit_class_deep  weighted  0.763955     0.023502     0.995604   \n",
       "71  fitbit_class_deep  weighted  0.761163     0.060985     0.980220   \n",
       "72  fitbit_class_deep  weighted  0.763955     0.030534     0.993407   \n",
       "73  fitbit_class_deep  weighted  0.761165     0.007031     0.997070   \n",
       "74  fitbit_class_deep  weighted  0.755581     0.089029     0.964103   \n",
       "\n",
       "                                             config  \n",
       "0   {'num_epochs': 10, 'lr': 0.0024939966767557526}  \n",
       "1   {'num_epochs': 5, 'lr': 1.5009952197841239e-05}  \n",
       "2    {'num_epochs': 6, 'lr': 0.0033768338913100727}  \n",
       "3   {'num_epochs': 6, 'lr': 0.00014935673948754337}  \n",
       "4     {'num_epochs': 5, 'lr': 0.004765277633376154}  \n",
       "..                                              ...  \n",
       "70  {'num_epochs': 9, 'lr': 0.00023162552300880473}  \n",
       "71  {'num_epochs': 9, 'lr': 0.00045748783190070806}  \n",
       "72   {'num_epochs': 9, 'lr': 0.0003002825055670167}  \n",
       "73  {'num_epochs': 8, 'lr': 0.00019952281945488572}  \n",
       "74   {'num_epochs': 9, 'lr': 0.0006090482579264076}  \n",
       "\n",
       "[75 rows x 6 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_fitbit_class = run_deep_search_fitbit_class(fitbit_class_dict, use_sample_weights=False, model_name=\"fitbit_class_deep\", n_trials=75)\n",
    "save_results_pickle(results_fitbit_class, \"fitbit_class_deep_nw\")\n",
    "results_fitbit_class[\"mean_metrics\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5729c0ea-e7ae-45ac-a4fa-aca8505476ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL_py_env)",
   "language": "python",
   "name": "dl_py_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
