{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad7aab01-c799-40c3-bbdb-cfb2b14be896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda with 5 GPU(s)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from captum.attr import IntegratedGradients\n",
    "import math\n",
    "\n",
    "# Set deterministic behavior for CUDA (set before torch imports)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # for multi-GPU\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    return seed\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device} with {torch.cuda.device_count()} GPU(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647ec8de-bb3a-4b01-8729-1a8d2729050f",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1bd439-3db4-4faa-8f12-435ff961441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the data directory as needed\n",
    "DL_DIR = \"../../data/deep_learning\"\n",
    "# Load the regression split dictionary.\n",
    "with open(f'{DL_DIR}/comb_reg_dict.pkl', 'rb') as f:\n",
    "    comb_reg_dict = pickle.load(f)\n",
    "\n",
    "with open(f'{DL_DIR}/fitbit_reg_dict.pkl', 'rb') as f:\n",
    "    fitbit_reg_dict = pickle.load(f)\n",
    "\n",
    "# Load the classification split dictionary.\n",
    "with open(f'{DL_DIR}/comb_class_dict.pkl', 'rb') as f:\n",
    "    comb_class_dict = pickle.load(f)\n",
    "\n",
    "with open(f'{DL_DIR}/fitbit_class_dict.pkl', 'rb') as f:\n",
    "    fitbit_class_dict = pickle.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07425ba7-e84c-44ab-86a9-a63fd7798cb1",
   "metadata": {},
   "source": [
    "### Best results Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d35fbd6-6a09-4056-948a-75512ed97d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in each pickle file individually (weighted)\n",
    "with open(\"search/fitbit_reg_deep_deep_search_results.pkl\", \"rb\") as f:\n",
    "    fitbit_reg_deep_results = pickle.load(f)\n",
    "\n",
    "with open(\"search/fitbit_class_deep_deep_search_results.pkl\", \"rb\") as f:\n",
    "    fitbit_class_deep_results = pickle.load(f)\n",
    "\n",
    "with open(\"search/comb_reg_deep_deep_search_results.pkl\", \"rb\") as f:\n",
    "    comb_reg_deep_results = pickle.load(f)\n",
    "\n",
    "with open(\"search/comb_class_deep_deep_search_results.pkl\", \"rb\") as f:\n",
    "    comb_class_deep_results = pickle.load(f)\n",
    "    \n",
    "    \n",
    "# Read in each pickle file individually (unweighted\n",
    "with open(\"search/fitbit_reg_deep_nw_deep_search_results.pkl\", \"rb\") as f:\n",
    "    fitbit_reg_deep_nw_results = pickle.load(f)\n",
    "\n",
    "with open(\"search/fitbit_class_deep_nw_deep_search_results.pkl\", \"rb\") as f:\n",
    "    fitbit_class_deep_nw_results = pickle.load(f)\n",
    "\n",
    "with open(\"search/comb_reg_deep_nw_deep_search_results.pkl\", \"rb\") as f:\n",
    "    comb_reg_deep_nw_results = pickle.load(f)\n",
    "\n",
    "with open(\"search/comb_class_deep_nw_deep_search_results.pkl\", \"rb\") as f:\n",
    "    comb_class_deep_nw_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb601608-54ad-49e7-a823-bf66e072af7a",
   "metadata": {},
   "source": [
    "### Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ba10f91-1acc-447d-a5c6-5ecb8b80ed03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 0.6534586661808931,\n",
       " 'params': {'drop_out_prb': 0.1234638549488152,\n",
       "  'lr': 1.0975595054006004e-05,\n",
       "  'use_regularization': True,\n",
       "  'num_epochs': 10,\n",
       "  'n_filters_0': 48},\n",
       " 'user_attrs': {'overall_mean_rmse': 0.6534586661808931,\n",
       "  'overall_std_rmse': 0.01648839042257811,\n",
       "  'bin_mean_rmse': {'1': 0.3450005299177086,\n",
       "   '2': 0.7880174590693902,\n",
       "   '3': 1.6688681135098693,\n",
       "   '4': 2.653461974891114,\n",
       "   '5': 3.7602703131715374},\n",
       "  'bin_std_rmse': {'1': 0.07033304786203862,\n",
       "   '2': 0.09080233647333263,\n",
       "   '3': 0.12889111377806875,\n",
       "   '4': 0.0939790601669137,\n",
       "   '5': 0.09452101007598172}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitbit_reg_deep_results[\"optimal_configuration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5453065f-da7b-427a-afcc-850270e17908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 0.23325811923250495,\n",
       " 'params': {'lr': 0.000565904221438412, 'n_filters_0': 24},\n",
       " 'user_attrs': {'overall_acc_mean': 0.766741880767495,\n",
       "  'overall_acc_std': 0.005703636802401684,\n",
       "  'sensitivity_mean': 0.035102599179206564,\n",
       "  'sensitivity_std': 0.03215625233012081,\n",
       "  'specificity_mean': 0.9956043956043956,\n",
       "  'specificity_std': 0.0035889959601218594}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitbit_class_deep_results[\"optimal_configuration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90055ce1-e324-4877-9331-254b624dcf52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 0.6447002631150113,\n",
       " 'params': {'lr': 1.1216699156320343e-05,\n",
       "  'dropout_prob': 0.10662333833500955,\n",
       "  'num_epochs': 10,\n",
       "  'n_filters_0': 56,\n",
       "  'n_cov': 1,\n",
       "  'use_regularization': False,\n",
       "  'dilation_0': 2},\n",
       " 'user_attrs': {'overall_mean_rmse': 0.6447002631150113,\n",
       "  'overall_std_rmse': 0.015244285170082817,\n",
       "  'bin_mean_rmse': {'1': 0.33430442785050146,\n",
       "   '2': 0.7534966556675069,\n",
       "   '3': 1.7620385953470161,\n",
       "   '4': 2.603179206530158,\n",
       "   '5': 3.620745750753796},\n",
       "  'bin_std_rmse': {'1': 0.06546720474385422,\n",
       "   '2': 0.06416182634837153,\n",
       "   '3': 0.08561986220277645,\n",
       "   '4': 0.15541045665538414,\n",
       "   '5': 0.09100139025395973}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_reg_deep_results[\"optimal_configuration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0dabcf8-74a9-4bf0-88d0-6e523ffdff8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 0.2310188138995659,\n",
       " 'params': {'lr': 0.00015704157833575886,\n",
       "  'dropout_prob': 0.45497081070937473,\n",
       "  'num_epochs': 8,\n",
       "  'batch_size': 48,\n",
       "  'n_filters_0': 56},\n",
       " 'user_attrs': {'overall_acc_mean': 0.7689811861004341,\n",
       "  'overall_acc_std': 0.006157065666334226,\n",
       "  'sensitivity_mean': 0.05157318741450069,\n",
       "  'sensitivity_std': 0.020542440809382948,\n",
       "  'specificity_mean': 0.9934065934065934,\n",
       "  'specificity_std': 0.004859523502352217}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_class_deep_results[\"optimal_configuration\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40d8fdc-e9da-4abd-9de3-ec9620ac9c8c",
   "metadata": {},
   "source": [
    "### Unweighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eade6ce9-f55e-42f6-8f59-2505ec29f717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 0.6162078713426811,\n",
       " 'params': {'dropout_prob': 0.4461520705008476,\n",
       "  'n_filters_0': 8,\n",
       "  'num_epochs': 5},\n",
       " 'user_attrs': {'overall_mean_rmse': 0.6162078713426811,\n",
       "  'overall_std_rmse': 0.021693254860636075,\n",
       "  'bin_mean_rmse': {'1': 0.2777239457846413,\n",
       "   '2': 0.762233443104348,\n",
       "   '3': 1.7028998849745907,\n",
       "   '4': 2.6041541764919733,\n",
       "   '5': 3.6145699780957417},\n",
       "  'bin_std_rmse': {'1': 0.03366632293350492,\n",
       "   '2': 0.04805768605116488,\n",
       "   '3': 0.08803748023481245,\n",
       "   '4': 0.09431819128946899,\n",
       "   '5': 0.1769821398233237}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitbit_reg_deep_nw_results[\"optimal_configuration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11551618-0f45-4f59-8064-b547c2c822fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 0.23437076920682842,\n",
       " 'params': {'num_epochs': 9, 'lr': 0.00025685423297033865},\n",
       " 'user_attrs': {'overall_acc_mean': 0.7656292307931716,\n",
       "  'overall_acc_std': 0.005674767020963522,\n",
       "  'sensitivity_mean': 0.03053351573187415,\n",
       "  'sensitivity_std': 0.0253574253673536,\n",
       "  'specificity_mean': 0.9956043956043956,\n",
       "  'specificity_std': 0.005383493940182791}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitbit_class_deep_nw_results[\"optimal_configuration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "958906c3-cef5-40e0-a0d5-021c7df98a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 0.5967643614168212,\n",
       " 'params': {'lr': 0.00013555143635027785,\n",
       "  'num_epochs': 10,\n",
       "  'dropout_prob': 0.13884004660199356,\n",
       "  'n_filters_0': 16,\n",
       "  'kernel_size_0': 3,\n",
       "  'n_conv': 3},\n",
       " 'user_attrs': {'overall_mean_rmse': 0.5967643614168212,\n",
       "  'overall_std_rmse': 0.02198441850555824,\n",
       "  'bin_mean_rmse': {'1': 0.2611028774320149,\n",
       "   '2': 0.7061489981085595,\n",
       "   '3': 1.6480903044230897,\n",
       "   '4': 2.6172805704761606,\n",
       "   '5': 3.6091912234001255},\n",
       "  'bin_std_rmse': {'1': 0.02387025088679633,\n",
       "   '2': 0.03572499616431226,\n",
       "   '3': 0.09207538787709463,\n",
       "   '4': 0.1302750589462002,\n",
       "   '5': 0.1720195899149569}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_reg_deep_nw_results[\"optimal_configuration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0631c156-05c9-4590-853f-e5231ad4b95c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 0.23661007453976746,\n",
       " 'params': {'batch_size': 48, 'n_filters_0': 8},\n",
       " 'user_attrs': {'overall_acc_mean': 0.7633899254602325,\n",
       "  'overall_acc_std': 0.0034470777500334817,\n",
       "  'sensitivity_mean': 0.06560875512995895,\n",
       "  'sensitivity_std': 0.03019082113868996,\n",
       "  'specificity_mean': 0.9816849816849818,\n",
       "  'specificity_std': 0.013105160307691077}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_class_deep_nw_results[\"optimal_configuration\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934e7d80-cbf2-4c21-ab32-559a5bd2c1e2",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec77271e-b7ed-46fb-a9c1-0350d78a2ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_output_length(L_in, kernel_size, stride, padding, dilation):\n",
    "    return (L_in + 2 * padding - dilation * (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "def pool_output_length(L_in, pool_kernel):\n",
    "    return L_in // pool_kernel  # assume stride equals kernel size\n",
    "\n",
    "# Build subject-level dataset (each row is one subject)\n",
    "def create_subject_dataset(df, outcome_col=\"SI_mean\", use_weights=True):\n",
    "    \"\"\"\n",
    "    Aggregates records for each subject into a subject-level sample.\n",
    "    If use_weights is False, then sample_weight is set to 1.0 for all subjects.\n",
    "    \"\"\"\n",
    "    exclude_cols = [\"PatientID\", \"timepoints\", \"si_kde_weight\", \"SI_mean\", \"is_SI\", \"SI_level\"]\n",
    "    predictor_cols = [col for col in df.columns if col not in (exclude_cols + [outcome_col])]\n",
    "    \n",
    "    subject_data = []\n",
    "    for pid, group in df.groupby(\"PatientID\"):\n",
    "        group_sorted = group.sort_values(\"timepoints\")\n",
    "        X = group_sorted[predictor_cols].values.T  # shape: (n_features, 39)\n",
    "        y = group_sorted[outcome_col].iloc[0]\n",
    "        if use_weights and \"si_kde_weight\" in group_sorted.columns:\n",
    "            weight = group_sorted[\"si_kde_weight\"].iloc[0]\n",
    "        else:\n",
    "            weight = 1.0\n",
    "        record = {\"PatientID\": pid, \"X\": X, outcome_col: y, \"sample_weight\": weight}\n",
    "        if outcome_col == \"is_SI\" and \"SI_mean\" in group_sorted.columns:\n",
    "            record[\"SI_mean\"] = group_sorted[\"SI_mean\"].iloc[0]\n",
    "        subject_data.append(record)\n",
    "    subj_df = pd.DataFrame(subject_data)\n",
    "    subj_df[f\"{outcome_col}_bin\"] = np.round(subj_df[outcome_col]).astype(int)\n",
    "    return subj_df, predictor_cols\n",
    "\n",
    "def compute_rmse(y_true, y_pred):\n",
    "    return math.sqrt(np.mean((y_pred - y_true)**2))\n",
    "\n",
    "def compute_classification_metrics(y_true, y_pred):\n",
    "    acc = np.mean(y_pred == y_true)\n",
    "    TP = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    FN = np.sum((y_pred == 0) & (y_true == 1))\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "    TN = np.sum((y_pred == 0) & (y_true == 0))\n",
    "    FP = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
    "    return acc, sensitivity, specificity\n",
    "\n",
    "def save_results_pickle(result_dict, model_name):\n",
    "    save_folder = \"search\"\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "    filename = os.path.join(save_folder, f\"{model_name}_deep_search_results.pkl\")\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(result_dict, f)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "\n",
    "# Helper to compute regression performance\n",
    "def compute_regression_perf(y_true, y_pred):\n",
    "    perf = {}\n",
    "    perf[\"overall\"] = compute_rmse(y_true, y_pred)\n",
    "    levels = [1, 2, 3, 4, 5]\n",
    "    y_levels = np.round(y_true).astype(int)\n",
    "    for level in levels:\n",
    "        inds = np.where(y_levels == level)[0]\n",
    "        if len(inds) > 0:\n",
    "            perf[str(level)] = compute_rmse(y_true[inds], y_pred[inds])\n",
    "        else:\n",
    "            perf[str(level)] = np.nan\n",
    "    return perf\n",
    "\n",
    "\n",
    "def get_stratified_cv_splits(df, subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5):\n",
    "    \"\"\"\n",
    "    Performs stratified K-fold cross validation at the subject level.\n",
    "    \n",
    "    Parameters:\n",
    "      df : pandas.DataFrame\n",
    "          The original dataframe containing repeated measures.\n",
    "      subject_id : str\n",
    "          The column name for the subject ID (e.g., \"PatientID\").\n",
    "      target_var : str\n",
    "          The target variable; for regression use \"SI_mean\" and for classification use \"is_SI\".\n",
    "      n_splits : int\n",
    "          Number of folds for cross validation.\n",
    "    \n",
    "    Returns:\n",
    "      splits : list of tuples\n",
    "          A list where each element is a tuple (train_df, test_df) corresponding\n",
    "          to one fold. Each dataframe contains all rows (i.e. repeated measures) for the patients in that fold.\n",
    "    \n",
    "    Behavior:\n",
    "      - Isolates unique patient IDs and their target variable by dropping duplicates.\n",
    "      - If target_var is \"SI_mean\", creates a new column \"SI_mean_levels\" (rounded SI_mean).\n",
    "      - Uses the resulting column as the stratification column.\n",
    "      - Performs stratified K-fold CV and then subsets the original dataframe based on the patient IDs.\n",
    "    \"\"\"\n",
    "    # Create a subject-level dataframe (unique patient IDs with their target variable)\n",
    "    subject_df = df[[subject_id, target_var]].drop_duplicates(subset=[subject_id]).copy()\n",
    "    \n",
    "    # For regression: create a new column with the rounded SI_mean values.\n",
    "    if target_var == \"SI_mean\":\n",
    "        subject_df[\"SI_mean_levels\"] = subject_df[target_var].round().astype(int)\n",
    "        strat_col = \"SI_mean_levels\"\n",
    "    else:\n",
    "        strat_col = target_var  # For classification, use the target directly.\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    splits = []\n",
    "    \n",
    "    # Get the subject IDs and stratification labels\n",
    "    subjects = subject_df[subject_id].values\n",
    "    strat_labels = subject_df[strat_col].values\n",
    "    \n",
    "    # For each fold, retrieve patient IDs and then subset the original dataframe.\n",
    "    for train_idx, test_idx in skf.split(subjects, strat_labels):\n",
    "        train_patient_ids = subject_df.iloc[train_idx][subject_id].values\n",
    "        test_patient_ids  = subject_df.iloc[test_idx][subject_id].values\n",
    "        train_split = df[df[subject_id].isin(train_patient_ids)]\n",
    "        test_split  = df[df[subject_id].isin(test_patient_ids)]\n",
    "        splits.append((train_split, test_split))\n",
    "    \n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1766e61-90f9-4976-a8dc-cec521be9662",
   "metadata": {},
   "source": [
    "### Model Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f0a2a85-646a-4cef-a2ed-cb7ee79a95ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Regression: Fitbit Regression using Integrated Gradients\n",
    "def run_fitbit_regression_best(fitbit_reg_dict):\n",
    "    set_seed(42)\n",
    "    # Updated hyperparameters from deep search for fitbit_reg:\n",
    "    params = {\n",
    "        \"batch_size\": 32,\n",
    "        \"num_epochs\": 10,\n",
    "        \"lr\": 1.0975595054006004e-05,\n",
    "        \"use_regularization\": True,\n",
    "        \"n_conv\": 3,\n",
    "        \"n_hidden\": 2,\n",
    "        \"n_filters_0\": 48,\n",
    "        \"kernel_size_0\": 7,\n",
    "        \"dropout_prob\": 0.1234638549488152,\n",
    "        \"use_pool_0\": True,\n",
    "        \"stride_0\": 1,\n",
    "        \"dilation_0\": 1\n",
    "    }\n",
    "    \n",
    "    # Create subject-level datasets from full train and test splits.\n",
    "    train_df, predictor_cols = create_subject_dataset(fitbit_reg_dict['train'], outcome_col=\"SI_mean\")\n",
    "    test_df, _ = create_subject_dataset(fitbit_reg_dict['test'], outcome_col=\"SI_mean\")\n",
    "    \n",
    "    X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "    y_train = train_df[\"SI_mean\"].values.astype(np.float32)\n",
    "    sw_train = train_df[\"sample_weight\"].values.astype(np.float32)\n",
    "    \n",
    "    X_test = np.stack(test_df[\"X\"].values, axis=0)\n",
    "    y_test = test_df[\"SI_mean\"].values.astype(np.float32)\n",
    "    sw_test = test_df[\"sample_weight\"].values.astype(np.float32)\n",
    "    \n",
    "    n_subjects, input_channels, seq_len = X_train.shape\n",
    "    \n",
    "    # Build convolutional network.\n",
    "    kernel_size = params[\"kernel_size_0\"]\n",
    "    stride = params[\"stride_0\"]\n",
    "    dilation = params[\"dilation_0\"]\n",
    "    padding = ((kernel_size - 1) // 2) * dilation\n",
    "    conv = nn.Conv1d(in_channels=input_channels, \n",
    "                     out_channels=params[\"n_filters_0\"],\n",
    "                     kernel_size=kernel_size, \n",
    "                     stride=stride, \n",
    "                     dilation=dilation, \n",
    "                     padding=padding)\n",
    "    relu = nn.ReLU()\n",
    "    layers = [conv, relu, nn.Dropout(params[\"dropout_prob\"])]\n",
    "    if params[\"use_pool_0\"]:\n",
    "        layers.append(nn.MaxPool1d(kernel_size=2))\n",
    "        conv_out_len = pool_output_length(seq_len, 2)\n",
    "    else:\n",
    "        conv_out_len = seq_len\n",
    "    conv_net = nn.Sequential(*layers)\n",
    "    flattened_dim = params[\"n_filters_0\"] * conv_out_len\n",
    "    \n",
    "    # Build fully connected network.\n",
    "    fc_layers = []\n",
    "    in_features = flattened_dim\n",
    "    for _ in range(params[\"n_hidden\"]):\n",
    "        fc_layers.append(nn.Linear(in_features, 64))\n",
    "        fc_layers.append(nn.ReLU())\n",
    "        fc_layers.append(nn.Dropout(params[\"dropout_prob\"]))\n",
    "        in_features = 64\n",
    "    fc_layers.append(nn.Linear(in_features, 1))\n",
    "    fc_net = nn.Sequential(*fc_layers)\n",
    "    \n",
    "    class FitRegCNN(nn.Module):\n",
    "        def __init__(self, conv_net, fc_net):\n",
    "            super(FitRegCNN, self).__init__()\n",
    "            self.conv = conv_net\n",
    "            self.fc = fc_net\n",
    "        def forward(self, x):\n",
    "            x = self.conv(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            return self.fc(x)\n",
    "    \n",
    "    model = FitRegCNN(conv_net, fc_net).to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "    loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).view(-1, 1),\n",
    "        torch.tensor(sw_train, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    test_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.float32),\n",
    "        torch.tensor(y_test, dtype=torch.float32).view(-1, 1),\n",
    "        torch.tensor(sw_test, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    # Train the model on the full training set.\n",
    "    model.train()\n",
    "    for epoch in range(params[\"num_epochs\"]):\n",
    "        for X_batch, y_batch, w_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            w_batch = w_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = (loss_fn(outputs, y_batch).view(-1) * w_batch.view(-1)).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Helper function to get predictions.\n",
    "    def get_preds(loader, model):\n",
    "        preds, truths = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch, _ in loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                preds.append(outputs.cpu().numpy())\n",
    "                truths.append(y_batch.cpu().numpy())\n",
    "        preds = np.concatenate(preds).flatten()\n",
    "        truths = np.concatenate(truths).flatten()\n",
    "        return truths, preds\n",
    "    \n",
    "    model.eval()\n",
    "    y_train_true, y_train_pred = get_preds(train_loader, model)\n",
    "    y_test_true, y_test_pred = get_preds(test_loader, model)\n",
    "    \n",
    "    perf_train = compute_regression_perf(y_train_true, y_train_pred)\n",
    "    perf_test  = compute_regression_perf(y_test_true, y_test_pred)\n",
    "    \n",
    "    perf_train_df = pd.DataFrame({\n",
    "        \"model\": [\"fitbit_regression\"],\n",
    "        \"type\": [\"train\"],\n",
    "        \"1\": [perf_train[\"1\"]],\n",
    "        \"2\": [perf_train[\"2\"]],\n",
    "        \"3\": [perf_train[\"3\"]],\n",
    "        \"4\": [perf_train[\"4\"]],\n",
    "        \"5\": [perf_train[\"5\"]],\n",
    "        \"overall\": [perf_train[\"overall\"]]\n",
    "    })\n",
    "    perf_test_df = pd.DataFrame({\n",
    "        \"model\": [\"fitbit_regression\"],\n",
    "        \"type\": [\"test\"],\n",
    "        \"1\": [perf_test[\"1\"]],\n",
    "        \"2\": [perf_test[\"2\"]],\n",
    "        \"3\": [perf_test[\"3\"]],\n",
    "        \"4\": [perf_test[\"4\"]],\n",
    "        \"5\": [perf_test[\"5\"]],\n",
    "        \"overall\": [perf_test[\"overall\"]]\n",
    "    })\n",
    "    performance_df = pd.concat([perf_train_df, perf_test_df], ignore_index=True)\n",
    "    \n",
    "    # Compute Integrated Gradients for SHAP.\n",
    "    set_seed(42)\n",
    "    model_for_attr = model.module if hasattr(model, 'module') else model\n",
    "    ig = IntegratedGradients(model_for_attr)\n",
    "    input_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    baseline = torch.zeros_like(input_tensor)\n",
    "    attributions = ig.attribute(input_tensor, baselines=baseline, n_steps=50)\n",
    "    mean_attr = attributions.mean(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    sd_attr = attributions.abs().std(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    if len(predictor_cols) != len(mean_attr):\n",
    "        min_len = min(len(predictor_cols), len(mean_attr))\n",
    "        predictor_names = predictor_cols[:min_len]\n",
    "        mean_attr = mean_attr[:min_len]\n",
    "        sd_attr = sd_attr[:min_len]\n",
    "    else:\n",
    "        predictor_names = predictor_cols\n",
    "    shap_df = pd.DataFrame({\n",
    "        \"predictor\": predictor_names,\n",
    "        \"mean_abs_integrated_gradients\": mean_attr,\n",
    "        \"sd_abs_integrated_gradients\": sd_attr\n",
    "    })\n",
    "    \n",
    "    #################################################################\n",
    "    # CROSS-VALIDATION (CV) on the training data for regression\n",
    "    cv_splits = get_stratified_cv_splits(fitbit_reg_dict['train'], subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5)\n",
    "    cv_fold_metrics = []  # to store each fold's performance dictionary\n",
    "    \n",
    "    for fold, (cv_train_raw, cv_val_raw) in enumerate(cv_splits, start=1):\n",
    "        # Create subject-level datasets for this CV fold.\n",
    "        cv_train_df, _ = create_subject_dataset(cv_train_raw, outcome_col=\"SI_mean\")\n",
    "        cv_val_df, _ = create_subject_dataset(cv_val_raw, outcome_col=\"SI_mean\")\n",
    "        X_cv_train = np.stack(cv_train_df[\"X\"].values, axis=0)\n",
    "        y_cv_train = cv_train_df[\"SI_mean\"].values.astype(np.float32)\n",
    "        sw_cv_train = cv_train_df[\"sample_weight\"].values.astype(np.float32)\n",
    "        \n",
    "        X_cv_val = np.stack(cv_val_df[\"X\"].values, axis=0)\n",
    "        y_cv_val = cv_val_df[\"SI_mean\"].values.astype(np.float32)\n",
    "        sw_cv_val = cv_val_df[\"sample_weight\"].values.astype(np.float32)\n",
    "        \n",
    "        n_cv, input_channels_cv, seq_len_cv = X_cv_train.shape\n",
    "        \n",
    "        # Build a new model instance with the same architecture.\n",
    "        kernel_size = params[\"kernel_size_0\"]\n",
    "        stride = params[\"stride_0\"]\n",
    "        dilation = params[\"dilation_0\"]\n",
    "        padding = ((kernel_size - 1) // 2) * dilation\n",
    "        conv_cv = nn.Conv1d(in_channels=input_channels_cv, \n",
    "                            out_channels=params[\"n_filters_0\"],\n",
    "                            kernel_size=kernel_size, \n",
    "                            stride=stride, \n",
    "                            dilation=dilation, \n",
    "                            padding=padding)\n",
    "        relu_cv = nn.ReLU()\n",
    "        layers_cv = [conv_cv, relu_cv, nn.Dropout(params[\"dropout_prob\"])]\n",
    "        if params[\"use_pool_0\"]:\n",
    "            layers_cv.append(nn.MaxPool1d(kernel_size=2))\n",
    "            conv_out_len_cv = pool_output_length(seq_len_cv, 2)\n",
    "        else:\n",
    "            conv_out_len_cv = seq_len_cv\n",
    "        conv_net_cv = nn.Sequential(*layers_cv)\n",
    "        flattened_dim_cv = params[\"n_filters_0\"] * conv_out_len_cv\n",
    "        \n",
    "        fc_layers_cv = []\n",
    "        in_features_cv = flattened_dim_cv\n",
    "        for _ in range(params[\"n_hidden\"]):\n",
    "            fc_layers_cv.append(nn.Linear(in_features_cv, 64))\n",
    "            fc_layers_cv.append(nn.ReLU())\n",
    "            fc_layers_cv.append(nn.Dropout(params[\"dropout_prob\"]))\n",
    "            in_features_cv = 64\n",
    "        fc_layers_cv.append(nn.Linear(in_features_cv, 1))\n",
    "        fc_net_cv = nn.Sequential(*fc_layers_cv)\n",
    "        \n",
    "        class FitRegCNN_CV(nn.Module):\n",
    "            def __init__(self, conv_net, fc_net):\n",
    "                super(FitRegCNN_CV, self).__init__()\n",
    "                self.conv = conv_net\n",
    "                self.fc = fc_net\n",
    "            def forward(self, x):\n",
    "                x = self.conv(x)\n",
    "                x = x.view(x.size(0), -1)\n",
    "                return self.fc(x)\n",
    "        \n",
    "        model_cv = FitRegCNN_CV(conv_net_cv, fc_net_cv).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model_cv = nn.DataParallel(model_cv)\n",
    "        \n",
    "        optimizer_cv = optim.Adam(model_cv.parameters(), lr=params[\"lr\"])\n",
    "        loss_fn_cv = nn.MSELoss(reduction=\"none\")\n",
    "        \n",
    "        train_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_train, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_train, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(sw_cv_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        val_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_val, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_val, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(sw_cv_val, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader_cv = torch.utils.data.DataLoader(train_dataset_cv, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "        val_loader_cv = torch.utils.data.DataLoader(val_dataset_cv, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "        \n",
    "        model_cv.train()\n",
    "        for epoch in range(params[\"num_epochs\"]):\n",
    "            for X_batch, y_batch, w_batch in train_loader_cv:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                w_batch = w_batch.to(device)\n",
    "                optimizer_cv.zero_grad()\n",
    "                outputs = model_cv(X_batch)\n",
    "                loss = (loss_fn_cv(outputs, y_batch).view(-1) * w_batch.view(-1)).mean()\n",
    "                loss.backward()\n",
    "                optimizer_cv.step()\n",
    "        \n",
    "        model_cv.eval()\n",
    "        y_val_true_cv, y_val_pred_cv = get_preds(val_loader_cv, model_cv)\n",
    "        # Compute overall MSE for this fold.\n",
    "        mse_cv = np.mean((y_val_true_cv - y_val_pred_cv) ** 2)\n",
    "        fold_perf = compute_regression_perf(y_val_true_cv, y_val_pred_cv)\n",
    "        fold_perf[\"mse\"] = mse_cv\n",
    "        cv_fold_metrics.append(fold_perf)\n",
    "    \n",
    "    # Aggregate CV metrics over folds (including mse).\n",
    "    keys = [\"1\", \"2\", \"3\", \"4\", \"5\", \"overall\", \"mse\"]\n",
    "    mean_metrics = {k: np.mean([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    sd_metrics   = {k: np.std([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    cv_val_df = pd.DataFrame({\n",
    "        \"stat\": [\"mean\", \"sd\"],\n",
    "        \"model\": [\"fitbit_regression\", \"fitbit_regression\"],\n",
    "        \"1\": [mean_metrics[\"1\"], sd_metrics[\"1\"]],\n",
    "        \"2\": [mean_metrics[\"2\"], sd_metrics[\"2\"]],\n",
    "        \"3\": [mean_metrics[\"3\"], sd_metrics[\"3\"]],\n",
    "        \"4\": [mean_metrics[\"4\"], sd_metrics[\"4\"]],\n",
    "        \"5\": [mean_metrics[\"5\"], sd_metrics[\"5\"]],\n",
    "        \"overall\": [mean_metrics[\"overall\"], sd_metrics[\"overall\"]],\n",
    "        \"mse\": [mean_metrics[\"mse\"], sd_metrics[\"mse\"]]\n",
    "    })\n",
    "    \n",
    "    return performance_df, shap_df, cv_val_df\n",
    "\n",
    "\n",
    "############################################\n",
    "# Regression: Comb Regression using Integrated Gradients\n",
    "def run_comb_regression_best(comb_dict):\n",
    "    set_seed(42)\n",
    "    # Updated hyperparameters from deep search for comb_reg:\n",
    "    params = {\n",
    "        \"lr\": 1.1216699156320343e-05,\n",
    "        \"dropout_prob\": 0.10662333833500955,\n",
    "        \"num_epochs\": 10,\n",
    "        \"n_filters_0\": 56,\n",
    "        \"n_cov\": 1,\n",
    "        \"use_regularization\": False,\n",
    "        \"dilation_0\": 2,\n",
    "        \"use_pool_0\": True\n",
    "    }\n",
    "    train_df, predictor_cols = create_subject_dataset(comb_dict['train'], outcome_col=\"SI_mean\")\n",
    "    test_df, _ = create_subject_dataset(comb_dict['test'], outcome_col=\"SI_mean\")\n",
    "    \n",
    "    X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "    y_train = train_df[\"SI_mean\"].values.astype(np.float32)\n",
    "    sw_train = train_df[\"sample_weight\"].values.astype(np.float32)\n",
    "    \n",
    "    X_test = np.stack(test_df[\"X\"].values, axis=0)\n",
    "    y_test = test_df[\"SI_mean\"].values.astype(np.float32)\n",
    "    sw_test = test_df[\"sample_weight\"].values.astype(np.float32)\n",
    "    \n",
    "    n_subjects, input_channels, seq_len = X_train.shape\n",
    "\n",
    "    # Build convolutional network.\n",
    "    kernel_size = 3\n",
    "    stride = 1\n",
    "    dilation = params[\"dilation_0\"]\n",
    "    padding = ((kernel_size - 1) // 2) * dilation\n",
    "    conv = nn.Conv1d(in_channels=input_channels,\n",
    "                     out_channels=params[\"n_filters_0\"],\n",
    "                     kernel_size=kernel_size,\n",
    "                     stride=stride,\n",
    "                     dilation=dilation,\n",
    "                     padding=padding)\n",
    "    relu = nn.ReLU()\n",
    "    layers = [conv, relu, nn.Dropout(params[\"dropout_prob\"])]\n",
    "    if params[\"use_pool_0\"]:\n",
    "        layers.append(nn.MaxPool1d(kernel_size=2))\n",
    "        conv_out_len = pool_output_length(seq_len, 2)\n",
    "    else:\n",
    "        conv_out_len = seq_len\n",
    "    conv_net = nn.Sequential(*layers)\n",
    "    flattened_dim = params[\"n_filters_0\"] * conv_out_len\n",
    "    \n",
    "    # For comb regression, a direct mapping is used.\n",
    "    fc_net = nn.Linear(flattened_dim, 1)\n",
    "    \n",
    "    class CombRegCNN(nn.Module):\n",
    "        def __init__(self, conv_net, fc_net):\n",
    "            super(CombRegCNN, self).__init__()\n",
    "            self.conv = conv_net\n",
    "            self.fc = fc_net\n",
    "        def forward(self, x):\n",
    "            x = self.conv(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            return self.fc(x)\n",
    "    \n",
    "    model = CombRegCNN(conv_net, fc_net).to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "    loss_fn = nn.MSELoss(reduction='none')\n",
    "    num_epochs = params[\"num_epochs\"]\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).view(-1, 1),\n",
    "        torch.tensor(sw_train, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    test_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.float32),\n",
    "        torch.tensor(y_test, dtype=torch.float32).view(-1, 1),\n",
    "        torch.tensor(sw_test, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for X_batch, y_batch, w_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            w_batch = w_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = (loss_fn(outputs, y_batch).view(-1) * w_batch.view(-1)).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def get_preds(loader, model):\n",
    "        preds, truths = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch, _ in loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                preds.append(outputs.cpu().numpy())\n",
    "                truths.append(y_batch.cpu().numpy())\n",
    "        preds = np.concatenate(preds).flatten()\n",
    "        truths = np.concatenate(truths).flatten()\n",
    "        return truths, preds\n",
    "\n",
    "    model.eval()\n",
    "    y_train_true, y_train_pred = get_preds(train_loader, model)\n",
    "    y_test_true, y_test_pred = get_preds(test_loader, model)\n",
    "    \n",
    "    perf_train = compute_regression_perf(y_train_true, y_train_pred)\n",
    "    perf_test  = compute_regression_perf(y_test_true, y_test_pred)\n",
    "    \n",
    "    perf_train_df = pd.DataFrame({\n",
    "        \"model\": [\"comb_regression\"],\n",
    "        \"type\": [\"train\"],\n",
    "        \"1\": [perf_train[\"1\"]],\n",
    "        \"2\": [perf_train[\"2\"]],\n",
    "        \"3\": [perf_train[\"3\"]],\n",
    "        \"4\": [perf_train[\"4\"]],\n",
    "        \"5\": [perf_train[\"5\"]],\n",
    "        \"overall\": [perf_train[\"overall\"]]\n",
    "    })\n",
    "    perf_test_df = pd.DataFrame({\n",
    "        \"model\": [\"comb_regression\"],\n",
    "        \"type\": [\"test\"],\n",
    "        \"1\": [perf_test[\"1\"]],\n",
    "        \"2\": [perf_test[\"2\"]],\n",
    "        \"3\": [perf_test[\"3\"]],\n",
    "        \"4\": [perf_test[\"4\"]],\n",
    "        \"5\": [perf_test[\"5\"]],\n",
    "        \"overall\": [perf_test[\"overall\"]]\n",
    "    })\n",
    "    performance_df = pd.concat([perf_train_df, perf_test_df], ignore_index=True)\n",
    "    \n",
    "    # Compute Integrated Gradients for SHAP.\n",
    "    set_seed(42)\n",
    "    model_for_attr = model.module if hasattr(model, 'module') else model\n",
    "    ig = IntegratedGradients(model_for_attr)\n",
    "    input_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    baseline = torch.zeros_like(input_tensor)\n",
    "    attributions = ig.attribute(input_tensor, baselines=baseline, n_steps=50)\n",
    "    mean_attr = attributions.mean(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    sd_attr = attributions.abs().std(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    if len(predictor_cols) != len(mean_attr):\n",
    "        min_len = min(len(predictor_cols), len(mean_attr))\n",
    "        predictor_names = predictor_cols[:min_len]\n",
    "        mean_attr = mean_attr[:min_len]\n",
    "        sd_attr = sd_attr[:min_len]\n",
    "    else:\n",
    "        predictor_names = predictor_cols\n",
    "    shap_df = pd.DataFrame({\n",
    "        \"predictor\": predictor_names,\n",
    "        \"mean_abs_integrated_gradients\": mean_attr,\n",
    "        \"sd_abs_integrated_gradients\": sd_attr\n",
    "    })\n",
    "    \n",
    "    #################################################################\n",
    "    # CROSS-VALIDATION for comb regression\n",
    "    cv_splits = get_stratified_cv_splits(comb_dict['train'], subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5)\n",
    "    cv_fold_metrics = []\n",
    "    \n",
    "    for fold, (cv_train_raw, cv_val_raw) in enumerate(cv_splits, start=1):\n",
    "        cv_train_df, _ = create_subject_dataset(cv_train_raw, outcome_col=\"SI_mean\")\n",
    "        cv_val_df, _ = create_subject_dataset(cv_val_raw, outcome_col=\"SI_mean\")\n",
    "        X_cv_train = np.stack(cv_train_df[\"X\"].values, axis=0)\n",
    "        y_cv_train = cv_train_df[\"SI_mean\"].values.astype(np.float32)\n",
    "        sw_cv_train = cv_train_df[\"sample_weight\"].values.astype(np.float32)\n",
    "        \n",
    "        X_cv_val = np.stack(cv_val_df[\"X\"].values, axis=0)\n",
    "        y_cv_val = cv_val_df[\"SI_mean\"].values.astype(np.float32)\n",
    "        sw_cv_val = cv_val_df[\"sample_weight\"].values.astype(np.float32)\n",
    "        \n",
    "        n_cv, input_channels_cv, seq_len_cv = X_cv_train.shape\n",
    "        kernel_size = 3\n",
    "        stride = 1\n",
    "        dilation = params[\"dilation_0\"]\n",
    "        padding = ((kernel_size - 1) // 2) * dilation\n",
    "        conv_cv = nn.Conv1d(in_channels=input_channels_cv,\n",
    "                            out_channels=params[\"n_filters_0\"],\n",
    "                            kernel_size=kernel_size,\n",
    "                            stride=stride,\n",
    "                            dilation=dilation,\n",
    "                            padding=padding)\n",
    "        relu_cv = nn.ReLU()\n",
    "        layers_cv = [conv_cv, relu_cv, nn.Dropout(params[\"dropout_prob\"])]\n",
    "        if params[\"use_pool_0\"]:\n",
    "            layers_cv.append(nn.MaxPool1d(kernel_size=2))\n",
    "            conv_out_len_cv = pool_output_length(seq_len_cv, 2)\n",
    "        else:\n",
    "            conv_out_len_cv = seq_len_cv\n",
    "        conv_net_cv = nn.Sequential(*layers_cv)\n",
    "        flattened_dim_cv = params[\"n_filters_0\"] * conv_out_len_cv\n",
    "        \n",
    "        fc_net_cv = nn.Linear(flattened_dim_cv, 1)\n",
    "        \n",
    "        class CombRegCNN_CV(nn.Module):\n",
    "            def __init__(self, conv_net, fc_net):\n",
    "                super(CombRegCNN_CV, self).__init__()\n",
    "                self.conv = conv_net\n",
    "                self.fc = fc_net\n",
    "            def forward(self, x):\n",
    "                x = self.conv(x)\n",
    "                x = x.view(x.size(0), -1)\n",
    "                return self.fc(x)\n",
    "        \n",
    "        model_cv = CombRegCNN_CV(conv_net_cv, fc_net_cv).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model_cv = nn.DataParallel(model_cv)\n",
    "        optimizer_cv = optim.Adam(model_cv.parameters(), lr=params[\"lr\"])\n",
    "        loss_fn_cv = nn.MSELoss(reduction='none')\n",
    "        \n",
    "        train_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_train, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_train, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(sw_cv_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        val_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_val, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_val, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(sw_cv_val, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader_cv = torch.utils.data.DataLoader(train_dataset_cv, batch_size=32, shuffle=True)\n",
    "        val_loader_cv = torch.utils.data.DataLoader(val_dataset_cv, batch_size=32, shuffle=False)\n",
    "        \n",
    "        model_cv.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for X_batch, y_batch, w_batch in train_loader_cv:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                w_batch = w_batch.to(device)\n",
    "                optimizer_cv.zero_grad()\n",
    "                outputs = model_cv(X_batch)\n",
    "                loss = (loss_fn_cv(outputs, y_batch).view(-1) * w_batch.view(-1)).mean()\n",
    "                loss.backward()\n",
    "                optimizer_cv.step()\n",
    "        \n",
    "        model_cv.eval()\n",
    "        y_val_true_cv, y_val_pred_cv = get_preds(val_loader_cv, model_cv)\n",
    "        # Compute overall MSE for this fold.\n",
    "        mse_cv = np.mean((y_val_true_cv - y_val_pred_cv) ** 2)\n",
    "        fold_perf = compute_regression_perf(y_val_true_cv, y_val_pred_cv)\n",
    "        fold_perf[\"mse\"] = mse_cv\n",
    "        cv_fold_metrics.append(fold_perf)\n",
    "    \n",
    "    keys = [\"1\", \"2\", \"3\", \"4\", \"5\", \"overall\", \"mse\"]\n",
    "    mean_metrics = {k: np.mean([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    sd_metrics = {k: np.std([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    cv_val_df = pd.DataFrame({\n",
    "        \"stat\": [\"mean\", \"sd\"],\n",
    "        \"model\": [\"comb_regression\", \"comb_regression\"],\n",
    "        \"1\": [mean_metrics[\"1\"], sd_metrics[\"1\"]],\n",
    "        \"2\": [mean_metrics[\"2\"], sd_metrics[\"2\"]],\n",
    "        \"3\": [mean_metrics[\"3\"], sd_metrics[\"3\"]],\n",
    "        \"4\": [mean_metrics[\"4\"], sd_metrics[\"4\"]],\n",
    "        \"5\": [mean_metrics[\"5\"], sd_metrics[\"5\"]],\n",
    "        \"overall\": [mean_metrics[\"overall\"], sd_metrics[\"overall\"]],\n",
    "        \"mse\": [mean_metrics[\"mse\"], sd_metrics[\"mse\"]]\n",
    "    })\n",
    "    \n",
    "    return performance_df, shap_df, cv_val_df\n",
    "\n",
    "\n",
    "############################################\n",
    "# Classification: Comb Classification using Integrated Gradients\n",
    "def run_comb_classification_best(comb_dict):\n",
    "    set_seed(42)\n",
    "    # Updated hyperparameters from deep search for comb_class:\n",
    "    params = {\n",
    "        \"lr\": 0.00015704157833575886,\n",
    "        \"dropout_prob\": 0.45497081070937473,\n",
    "        \"num_epochs\": 8,\n",
    "        \"batch_size\": 48,\n",
    "        \"n_filters_0\": 56\n",
    "    }\n",
    "    train_df, predictor_cols = create_subject_dataset(comb_dict['train'], outcome_col=\"is_SI\")\n",
    "    test_df, _ = create_subject_dataset(comb_dict['test'], outcome_col=\"is_SI\")\n",
    "    \n",
    "    X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "    y_train = train_df[\"is_SI\"].values.astype(np.float32)\n",
    "    X_test = np.stack(test_df[\"X\"].values, axis=0)\n",
    "    y_test = test_df[\"is_SI\"].values.astype(np.float32)\n",
    "    \n",
    "    n_subjects, input_channels, seq_len = X_train.shape\n",
    "    \n",
    "    kernel_size = 3\n",
    "    stride = 1\n",
    "    dilation = 1\n",
    "    padding = ((kernel_size - 1) // 2) * dilation\n",
    "    conv = nn.Conv1d(in_channels=input_channels, out_channels=params[\"n_filters_0\"],\n",
    "                     kernel_size=kernel_size, stride=stride, dilation=dilation, padding=padding)\n",
    "    relu = nn.ReLU()\n",
    "    layers = [conv, relu, nn.Dropout(params[\"dropout_prob\"])]\n",
    "    conv_net = nn.Sequential(*layers)\n",
    "    flattened_dim = params[\"n_filters_0\"] * conv_output_length(seq_len, kernel_size, stride, padding, dilation)\n",
    "    \n",
    "    fc_net = nn.Sequential(nn.Linear(flattened_dim, 1))\n",
    "    \n",
    "    class CombClassCNN(nn.Module):\n",
    "        def __init__(self, conv_net, fc_net):\n",
    "            super(CombClassCNN, self).__init__()\n",
    "            self.conv = conv_net\n",
    "            self.fc = fc_net\n",
    "        def forward(self, x):\n",
    "            x = self.conv(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            return self.fc(x)\n",
    "    \n",
    "    model = CombClassCNN(conv_net, fc_net).to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    num_epochs = params[\"num_epochs\"]\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    test_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.float32),\n",
    "        torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def get_preds(loader, model):\n",
    "        preds, truths = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "                truths.append(y_batch.cpu().numpy())\n",
    "        preds = np.concatenate(preds).flatten()\n",
    "        truths = np.concatenate(truths).flatten()\n",
    "        return truths, preds\n",
    "\n",
    "    model.eval()\n",
    "    y_train_true, y_train_pred = get_preds(train_loader, model)\n",
    "    y_test_true, y_test_pred = get_preds(test_loader, model)\n",
    "    \n",
    "    train_auc = roc_auc_score(y_train_true, y_train_pred)\n",
    "    test_auc = roc_auc_score(y_test_true, y_test_pred)\n",
    "    y_train_bin = (y_train_pred >= 0.5).astype(np.float32)\n",
    "    y_test_bin = (y_test_pred >= 0.5).astype(np.float32)\n",
    "    TP_train = np.sum((y_train_bin == 1) & (y_train_true == 1))\n",
    "    FN_train = np.sum((y_train_bin == 0) & (y_train_true == 1))\n",
    "    TN_train = np.sum((y_train_bin == 0) & (y_train_true == 0))\n",
    "    FP_train = np.sum((y_train_bin == 1) & (y_train_true == 0))\n",
    "    train_acc = np.mean(y_train_bin == y_train_true)\n",
    "    train_sens = TP_train / (TP_train + FN_train) if (TP_train + FN_train) > 0 else np.nan\n",
    "    train_spec = TN_train / (TN_train + FP_train) if (TN_train + FP_train) > 0 else np.nan\n",
    "\n",
    "    TP_test = np.sum((y_test_bin == 1) & (y_test_true == 1))\n",
    "    FN_test = np.sum((y_test_bin == 0) & (y_test_true == 1))\n",
    "    TN_test = np.sum((y_test_bin == 0) & (y_test_true == 0))\n",
    "    FP_test = np.sum((y_test_bin == 1) & (y_test_true == 0))\n",
    "    test_acc = np.mean(y_test_bin == y_test_true)\n",
    "    test_sens = TP_test / (TP_test + FN_test) if (TP_test + FN_test) > 0 else np.nan\n",
    "    test_spec = TN_test / (TN_test + FP_test) if (TN_test + FP_test) > 0 else np.nan\n",
    "\n",
    "    perf_train_df = pd.DataFrame({\n",
    "        \"model\": [\"comb_classification\"],\n",
    "        \"type\": [\"train\"],\n",
    "        \"AUC\": [train_auc],\n",
    "        \"accuracy\": [train_acc],\n",
    "        \"sensitivity\": [train_sens],\n",
    "        \"specificity\": [train_spec]\n",
    "    })\n",
    "    perf_test_df = pd.DataFrame({\n",
    "        \"model\": [\"comb_classification\"],\n",
    "        \"type\": [\"test\"],\n",
    "        \"AUC\": [test_auc],\n",
    "        \"accuracy\": [test_acc],\n",
    "        \"sensitivity\": [test_sens],\n",
    "        \"specificity\": [test_spec]\n",
    "    })\n",
    "    performance_df = pd.concat([perf_train_df, perf_test_df], ignore_index=True)\n",
    "    \n",
    "    # Compute Integrated Gradients for SHAP.\n",
    "    set_seed(42)\n",
    "    model_for_attr = model.module if hasattr(model, 'module') else model\n",
    "    ig = IntegratedGradients(model_for_attr)\n",
    "    input_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    baseline = torch.zeros_like(input_tensor)\n",
    "    attributions = ig.attribute(input_tensor, baselines=baseline, n_steps=50)\n",
    "    mean_attr = attributions.mean(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    sd_attr = attributions.abs().std(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    if len(predictor_cols) != len(mean_attr):\n",
    "        min_len = min(len(predictor_cols), len(mean_attr))\n",
    "        predictor_names = predictor_cols[:min_len]\n",
    "        mean_attr = mean_attr[:min_len]\n",
    "        sd_attr = sd_attr[:min_len]\n",
    "    else:\n",
    "        predictor_names = predictor_cols\n",
    "    shap_df = pd.DataFrame({\n",
    "        \"predictor\": predictor_names,\n",
    "        \"mean_abs_integrated_gradients\": mean_attr,\n",
    "        \"sd_abs_integrated_gradients\": sd_attr\n",
    "    })\n",
    "    \n",
    "    #################################################################\n",
    "    # CROSS-VALIDATION for comb classification\n",
    "    cv_splits = get_stratified_cv_splits(comb_dict['train'], subject_id=\"PatientID\", target_var=\"is_SI\", n_splits=5)\n",
    "    cv_fold_metrics = []\n",
    "    \n",
    "    for fold, (cv_train_raw, cv_val_raw) in enumerate(cv_splits, start=1):\n",
    "        cv_train_df, _ = create_subject_dataset(cv_train_raw, outcome_col=\"is_SI\")\n",
    "        cv_val_df, _ = create_subject_dataset(cv_val_raw, outcome_col=\"is_SI\")\n",
    "        X_cv_train = np.stack(cv_train_df[\"X\"].values, axis=0)\n",
    "        y_cv_train = cv_train_df[\"is_SI\"].values.astype(np.float32)\n",
    "        X_cv_val = np.stack(cv_val_df[\"X\"].values, axis=0)\n",
    "        y_cv_val = cv_val_df[\"is_SI\"].values.astype(np.float32)\n",
    "        \n",
    "        n_cv, input_channels_cv, seq_len_cv = X_cv_train.shape\n",
    "        kernel_size = 3\n",
    "        stride = 1\n",
    "        dilation = 1\n",
    "        padding = ((kernel_size - 1) // 2)* dilation\n",
    "        conv_cv = nn.Conv1d(in_channels=input_channels_cv,\n",
    "                            out_channels=params[\"n_filters_0\"],\n",
    "                            kernel_size=kernel_size, stride=stride, dilation=dilation, padding=padding)\n",
    "        relu_cv = nn.ReLU()\n",
    "        layers_cv = [conv_cv, relu_cv, nn.Dropout(params[\"dropout_prob\"])]\n",
    "        conv_net_cv = nn.Sequential(*layers_cv)\n",
    "        flattened_dim_cv = params[\"n_filters_0\"] * conv_output_length(seq_len_cv, kernel_size, stride, padding, dilation)\n",
    "        \n",
    "        fc_net_cv = nn.Sequential(nn.Linear(flattened_dim_cv, 1))\n",
    "        \n",
    "        class CombClassCNN_CV(nn.Module):\n",
    "            def __init__(self, conv_net, fc_net):\n",
    "                super(CombClassCNN_CV, self).__init__()\n",
    "                self.conv = conv_net\n",
    "                self.fc = fc_net\n",
    "            def forward(self, x):\n",
    "                x = self.conv(x)\n",
    "                x = x.view(x.size(0), -1)\n",
    "                return self.fc(x)\n",
    "        \n",
    "        model_cv = CombClassCNN_CV(conv_net_cv, fc_net_cv).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model_cv = nn.DataParallel(model_cv)\n",
    "        optimizer_cv = optim.Adam(model_cv.parameters(), lr=params[\"lr\"])\n",
    "        loss_fn_cv = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        train_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_train, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        val_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_val, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_val, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader_cv = torch.utils.data.DataLoader(train_dataset_cv, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "        val_loader_cv = torch.utils.data.DataLoader(val_dataset_cv, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "        \n",
    "        model_cv.train()\n",
    "        for epoch in range(params[\"num_epochs\"]):\n",
    "            for X_batch, y_batch in train_loader_cv:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                optimizer_cv.zero_grad()\n",
    "                outputs = model_cv(X_batch)\n",
    "                loss = loss_fn_cv(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer_cv.step()\n",
    "        \n",
    "        model_cv.eval()\n",
    "        y_val_true_cv, y_val_pred_cv = get_preds(val_loader_cv, model_cv)\n",
    "        auc_cv = roc_auc_score(y_val_true_cv, y_val_pred_cv)\n",
    "        y_val_bin = (y_val_pred_cv >= 0.5).astype(np.float32)\n",
    "        TP = np.sum((y_val_bin == 1) & (y_val_true_cv == 1))\n",
    "        FN = np.sum((y_val_bin == 0) & (y_val_true_cv == 1))\n",
    "        TN = np.sum((y_val_bin == 0) & (y_val_true_cv == 0))\n",
    "        FP = np.sum((y_val_bin == 1) & (y_val_true_cv == 0))\n",
    "        acc = np.mean(y_val_bin == y_val_true_cv)\n",
    "        sens = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "        spec = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
    "        # Compute binary cross entropy (with epsilon for numerical stability)\n",
    "        epsilon = 1e-12\n",
    "        bce_cv = np.mean(- (y_val_true_cv * np.log(y_val_pred_cv + epsilon) +\n",
    "                            (1 - y_val_true_cv) * np.log(1 - y_val_pred_cv + epsilon)))\n",
    "        cv_fold_metrics.append({\n",
    "            \"AUC\": auc_cv,\n",
    "            \"accuracy\": acc,\n",
    "            \"sensitivity\": sens,\n",
    "            \"specificity\": spec,\n",
    "            \"bce\": bce_cv\n",
    "        })\n",
    "    \n",
    "    mean_metrics = {k: np.mean([fold[k] for fold in cv_fold_metrics]) for k in [\"AUC\", \"accuracy\", \"sensitivity\", \"specificity\", \"bce\"]}\n",
    "    sd_metrics = {k: np.std([fold[k] for fold in cv_fold_metrics]) for k in [\"AUC\", \"accuracy\", \"sensitivity\", \"specificity\", \"bce\"]}\n",
    "    cv_val_df = pd.DataFrame({\n",
    "        \"stat\": [\"mean\", \"sd\"],\n",
    "        \"model\": [\"comb_classification\", \"comb_classification\"],\n",
    "        \"AUC\": [mean_metrics[\"AUC\"], sd_metrics[\"AUC\"]],\n",
    "        \"accuracy\": [mean_metrics[\"accuracy\"], sd_metrics[\"accuracy\"]],\n",
    "        \"sensitivity\": [mean_metrics[\"sensitivity\"], sd_metrics[\"sensitivity\"]],\n",
    "        \"specificity\": [mean_metrics[\"specificity\"], sd_metrics[\"specificity\"]],\n",
    "        \"bce\": [mean_metrics[\"bce\"], sd_metrics[\"bce\"]]\n",
    "    })\n",
    "    \n",
    "    return performance_df, shap_df, cv_val_df\n",
    "\n",
    "\n",
    "############################################\n",
    "# Classification: Fitbit Classification using Integrated Gradients\n",
    "def run_fitbit_classification_best(fitbit_dict):\n",
    "    set_seed(42)\n",
    "    # Updated hyperparameters from deep search for fitbit_class:\n",
    "    params = {\n",
    "        \"lr\": 0.000565904221438412,\n",
    "        \"dropout_prob\": 0.45160954145010296,\n",
    "        \"num_epochs\": 8,\n",
    "        \"n_filters_0\": 24,\n",
    "        \"kernel_size_0\": 3\n",
    "    }\n",
    "    train_df, predictor_cols = create_subject_dataset(fitbit_dict['train'], outcome_col=\"is_SI\")\n",
    "    test_df, _ = create_subject_dataset(fitbit_dict['test'], outcome_col=\"is_SI\")\n",
    "    \n",
    "    X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "    y_train = train_df[\"is_SI\"].values.astype(np.float32)\n",
    "    X_test = np.stack(test_df[\"X\"].values, axis=0)\n",
    "    y_test = test_df[\"is_SI\"].values.astype(np.float32)\n",
    "    \n",
    "    n_subjects, input_channels, seq_len = X_train.shape\n",
    "    \n",
    "    kernel_size = params[\"kernel_size_0\"]\n",
    "    stride = 1\n",
    "    dilation = 1\n",
    "    padding = ((kernel_size - 1) // 2) * dilation\n",
    "    conv = nn.Conv1d(in_channels=input_channels, out_channels=params[\"n_filters_0\"],\n",
    "                     kernel_size=kernel_size, stride=stride, dilation=dilation, padding=padding)\n",
    "    relu = nn.ReLU()\n",
    "    layers = [conv, relu, nn.Dropout(params[\"dropout_prob\"])]\n",
    "    conv_net = nn.Sequential(*layers)\n",
    "    flattened_dim = params[\"n_filters_0\"] * conv_output_length(seq_len, kernel_size, stride, padding, dilation)\n",
    "    \n",
    "    fc_net = nn.Sequential(nn.Linear(flattened_dim, 1))\n",
    "    \n",
    "    class FitbitClassCNN(nn.Module):\n",
    "        def __init__(self, conv_net, fc_net):\n",
    "            super(FitbitClassCNN, self).__init__()\n",
    "            self.conv = conv_net\n",
    "            self.fc = fc_net\n",
    "        def forward(self, x):\n",
    "            x = self.conv(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            return self.fc(x)\n",
    "    \n",
    "    model = FitbitClassCNN(conv_net, fc_net).to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    num_epochs = params[\"num_epochs\"]\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    test_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.float32),\n",
    "        torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def get_preds(loader, model):\n",
    "        preds, truths = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "                truths.append(y_batch.cpu().numpy())\n",
    "        preds = np.concatenate(preds).flatten()\n",
    "        truths = np.concatenate(truths).flatten()\n",
    "        return truths, preds\n",
    "\n",
    "    model.eval()\n",
    "    y_train_true, y_train_pred = get_preds(train_loader, model)\n",
    "    y_test_true, y_test_pred = get_preds(test_loader, model)\n",
    "    \n",
    "    train_auc = roc_auc_score(y_train_true, y_train_pred)\n",
    "    test_auc = roc_auc_score(y_test_true, y_test_pred)\n",
    "    y_train_bin = (y_train_pred >= 0.5).astype(np.float32)\n",
    "    y_test_bin = (y_test_pred >= 0.5).astype(np.float32)\n",
    "    TP_train = np.sum((y_train_bin == 1) & (y_train_true == 1))\n",
    "    FN_train = np.sum((y_train_bin == 0) & (y_train_true == 1))\n",
    "    TN_train = np.sum((y_train_bin == 0) & (y_train_true == 0))\n",
    "    FP_train = np.sum((y_train_bin == 1) & (y_train_true == 0))\n",
    "    train_acc = np.mean(y_train_bin == y_train_true)\n",
    "    train_sens = TP_train / (TP_train + FN_train) if (TP_train + FN_train) > 0 else np.nan\n",
    "    train_spec = TN_train / (TN_train + FP_train) if (TN_train + FP_train) > 0 else np.nan\n",
    "\n",
    "    TP_test = np.sum((y_test_bin == 1) & (y_test_true == 1))\n",
    "    FN_test = np.sum((y_test_bin == 0) & (y_test_true == 1))\n",
    "    TN_test = np.sum((y_test_bin == 0) & (y_test_true == 0))\n",
    "    FP_test = np.sum((y_test_bin == 1) & (y_test_true == 0))\n",
    "    test_acc = np.mean(y_test_bin == y_test_true)\n",
    "    test_sens = TP_test / (TP_test + FN_test) if (TP_test + FN_test) > 0 else np.nan\n",
    "    test_spec = TN_test / (TN_test + FP_test) if (TN_test + FP_test) > 0 else np.nan\n",
    "    \n",
    "    perf_train_df = pd.DataFrame({\n",
    "        \"model\": [\"fitbit_classification\"],\n",
    "        \"type\": [\"train\"],\n",
    "        \"AUC\": [train_auc],\n",
    "        \"accuracy\": [train_acc],\n",
    "        \"sensitivity\": [train_sens],\n",
    "        \"specificity\": [train_spec]\n",
    "    })\n",
    "    perf_test_df = pd.DataFrame({\n",
    "        \"model\": [\"fitbit_classification\"],\n",
    "        \"type\": [\"test\"],\n",
    "        \"AUC\": [test_auc],\n",
    "        \"accuracy\": [test_acc],\n",
    "        \"sensitivity\": [test_sens],\n",
    "        \"specificity\": [test_spec]\n",
    "    })\n",
    "    performance_df = pd.concat([perf_train_df, perf_test_df], ignore_index=True)\n",
    "    \n",
    "    # Compute Integrated Gradients for SHAP.\n",
    "    set_seed(42)\n",
    "    model_for_attr = model.module if hasattr(model, 'module') else model\n",
    "    ig = IntegratedGradients(model_for_attr)\n",
    "    input_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    baseline = torch.zeros_like(input_tensor)\n",
    "    attributions = ig.attribute(input_tensor, baselines=baseline, n_steps=50)\n",
    "    mean_attr = attributions.mean(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    sd_attr = attributions.abs().std(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    if len(predictor_cols) != len(mean_attr):\n",
    "        min_len = min(len(predictor_cols), len(mean_attr))\n",
    "        predictor_names = predictor_cols[:min_len]\n",
    "        mean_attr = mean_attr[:min_len]\n",
    "        sd_attr = sd_attr[:min_len]\n",
    "    else:\n",
    "        predictor_names = predictor_cols\n",
    "    shap_df = pd.DataFrame({\n",
    "        \"predictor\": predictor_names,\n",
    "        \"mean_abs_integrated_gradients\": mean_attr,\n",
    "        \"sd_abs_integrated_gradients\": sd_attr\n",
    "    })\n",
    "    \n",
    "    #################################################################\n",
    "    # CROSS-VALIDATION for fitbit classification\n",
    "    cv_splits = get_stratified_cv_splits(fitbit_dict['train'], subject_id=\"PatientID\", target_var=\"is_SI\", n_splits=5)\n",
    "    cv_fold_metrics = []\n",
    "    \n",
    "    for fold, (cv_train_raw, cv_val_raw) in enumerate(cv_splits, start=1):\n",
    "        cv_train_df, _ = create_subject_dataset(cv_train_raw, outcome_col=\"is_SI\")\n",
    "        cv_val_df, _ = create_subject_dataset(cv_val_raw, outcome_col=\"is_SI\")\n",
    "        X_cv_train = np.stack(cv_train_df[\"X\"].values, axis=0)\n",
    "        y_cv_train = cv_train_df[\"is_SI\"].values.astype(np.float32)\n",
    "        X_cv_val = np.stack(cv_val_df[\"X\"].values, axis=0)\n",
    "        y_cv_val = cv_val_df[\"is_SI\"].values.astype(np.float32)\n",
    "        \n",
    "        n_cv, input_channels_cv, seq_len_cv = X_cv_train.shape\n",
    "        kernel_size = params[\"kernel_size_0\"]\n",
    "        stride = 1\n",
    "        dilation = 1\n",
    "        padding = ((kernel_size - 1) // 2) * dilation\n",
    "        conv_cv = nn.Conv1d(in_channels=input_channels_cv, out_channels=params[\"n_filters_0\"],\n",
    "                            kernel_size=kernel_size, stride=stride, dilation=dilation, padding=padding)\n",
    "        relu_cv = nn.ReLU()\n",
    "        layers_cv = [conv_cv, relu_cv, nn.Dropout(params[\"dropout_prob\"])]\n",
    "        conv_net_cv = nn.Sequential(*layers_cv)\n",
    "        flattened_dim_cv = params[\"n_filters_0\"] * conv_output_length(seq_len_cv, kernel_size, stride, padding, dilation)\n",
    "        \n",
    "        fc_net_cv = nn.Sequential(nn.Linear(flattened_dim_cv, 1))\n",
    "        \n",
    "        class FitbitClassCNN_CV(nn.Module):\n",
    "            def __init__(self, conv_net, fc_net):\n",
    "                super(FitbitClassCNN_CV, self).__init__()\n",
    "                self.conv = conv_net\n",
    "                self.fc = fc_net\n",
    "            def forward(self, x):\n",
    "                x = self.conv(x)\n",
    "                x = x.view(x.size(0), -1)\n",
    "                return self.fc(x)\n",
    "        \n",
    "        model_cv = FitbitClassCNN_CV(conv_net_cv, fc_net_cv).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model_cv = nn.DataParallel(model_cv)\n",
    "        optimizer_cv = optim.Adam(model_cv.parameters(), lr=params[\"lr\"])\n",
    "        loss_fn_cv = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        train_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_train, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        val_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_val, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_val, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader_cv = torch.utils.data.DataLoader(train_dataset_cv, batch_size=32, shuffle=True)\n",
    "        val_loader_cv = torch.utils.data.DataLoader(val_dataset_cv, batch_size=32, shuffle=False)\n",
    "        \n",
    "        model_cv.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for X_batch, y_batch in train_loader_cv:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                optimizer_cv.zero_grad()\n",
    "                outputs = model_cv(X_batch)\n",
    "                loss = loss_fn_cv(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer_cv.step()\n",
    "        \n",
    "        model_cv.eval()\n",
    "        y_val_true_cv, y_val_pred_cv = get_preds(val_loader_cv, model_cv)\n",
    "        auc_cv = roc_auc_score(y_val_true_cv, y_val_pred_cv)\n",
    "        y_val_bin = (y_val_pred_cv >= 0.5).astype(np.float32)\n",
    "        TP = np.sum((y_val_bin == 1) & (y_val_true_cv == 1))\n",
    "        FN = np.sum((y_val_bin == 0) & (y_val_true_cv == 1))\n",
    "        TN = np.sum((y_val_bin == 0) & (y_val_true_cv == 0))\n",
    "        FP = np.sum((y_val_bin == 1) & (y_val_true_cv == 0))\n",
    "        acc = np.mean(y_val_bin == y_val_true_cv)\n",
    "        sens = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "        spec = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
    "        epsilon = 1e-12\n",
    "        bce_cv = np.mean(- (y_val_true_cv * np.log(y_val_pred_cv + epsilon) +\n",
    "                            (1 - y_val_true_cv) * np.log(1 - y_val_pred_cv + epsilon)))\n",
    "        cv_fold_metrics.append({\n",
    "            \"AUC\": auc_cv,\n",
    "            \"accuracy\": acc,\n",
    "            \"sensitivity\": sens,\n",
    "            \"specificity\": spec,\n",
    "            \"bce\": bce_cv\n",
    "        })\n",
    "    \n",
    "    mean_metrics = {k: np.mean([fold[k] for fold in cv_fold_metrics]) for k in [\"AUC\", \"accuracy\", \"sensitivity\", \"specificity\", \"bce\"]}\n",
    "    sd_metrics = {k: np.std([fold[k] for fold in cv_fold_metrics]) for k in [\"AUC\", \"accuracy\", \"sensitivity\", \"specificity\", \"bce\"]}\n",
    "    cv_val_df = pd.DataFrame({\n",
    "        \"stat\": [\"mean\", \"sd\"],\n",
    "        \"model\": [\"fitbit_classification\", \"fitbit_classification\"],\n",
    "        \"AUC\": [mean_metrics[\"AUC\"], sd_metrics[\"AUC\"]],\n",
    "        \"accuracy\": [mean_metrics[\"accuracy\"], sd_metrics[\"accuracy\"]],\n",
    "        \"sensitivity\": [mean_metrics[\"sensitivity\"], sd_metrics[\"sensitivity\"]],\n",
    "        \"specificity\": [mean_metrics[\"specificity\"], sd_metrics[\"specificity\"]],\n",
    "        \"bce\": [mean_metrics[\"bce\"], sd_metrics[\"bce\"]]\n",
    "    })\n",
    "    \n",
    "    return performance_df, shap_df, cv_val_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cde785-92bc-48cb-8b10-0e2a841ca0f5",
   "metadata": {},
   "source": [
    "### Running the Models and Displaying Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f16cc6b4-0b73-480f-b31c-cf2f7224c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the functions (each returns a tuple: (performance_df, shap_df))\n",
    "perf_fitbit_reg, shap_fitbit_reg, cv_fitbit_reg = run_fitbit_regression_best(fitbit_reg_dict)\n",
    "perf_fitbit_class, shap_fitbit_class, cv_fitbit_class = run_fitbit_classification_best(fitbit_class_dict)\n",
    "perf_comb_reg, shap_comb_reg, cv_comb_reg = run_comb_regression_best(comb_reg_dict)\n",
    "perf_comb_class, shap_comb_class, cv_comb_class = run_comb_classification_best(comb_class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb9b5167-63c8-4084-9e67-fccf56711c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the performance dataframes for regression and classification\n",
    "# best_perf_reg = pd.concat([perf_fitbit_reg, perf_comb_reg])\n",
    "# best_perf_class = pd.concat([perf_fitbit_class, perf_comb_class])\n",
    "\n",
    "\n",
    "# Save the combined performance dataframes as TSV files (without index)\n",
    "# best_perf_reg.to_csv(\"results/best_perf_reg.tsv\", sep=\"\\t\", index=False)\n",
    "# best_perf_class.to_csv(\"results/best_perf_class.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Save the SHAP score dataframes separately as TSV files (without index)\n",
    "shap_fitbit_reg.to_csv(\"results/shap_fitbit_reg.tsv\", sep=\"\\t\", index=False)\n",
    "shap_fitbit_class.to_csv(\"results/shap_fitbit_class.tsv\", sep=\"\\t\", index=False)\n",
    "shap_comb_reg.to_csv(\"results/shap_comb_reg.tsv\", sep=\"\\t\", index=False)\n",
    "shap_comb_class.to_csv(\"results/shap_comb_class.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2935d54b-eed4-464c-a145-43f1ec94588c",
   "metadata": {},
   "source": [
    "### Unweighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d6c98c4-05f7-4993-a629-060dfce6eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non‐Weighted Fitbit Regression\n",
    "def run_fitbit_regression_nw_best(fitbit_reg_dict):\n",
    "    set_seed(42)\n",
    "    # Updated hyperparameters for non‐weighted fitbit regression\n",
    "    params = {\n",
    "        \"batch_size\": 32,                      \n",
    "        \"num_epochs\": 5,                        \n",
    "        \"lr\": 1.0975595054006004e-05,           \n",
    "        \"use_regularization\": True,            \n",
    "        \"n_conv\": 3,                           \n",
    "        \"n_hidden\": 2,                          \n",
    "        \"n_filters_0\": 8,                      \n",
    "        \"kernel_size_0\": 7,                    \n",
    "        \"dropout_prob\": 0.4461520705008476,      \n",
    "        \"use_pool_0\": True,                    \n",
    "        \"stride_0\": 1,                          \n",
    "        \"dilation_0\": 1                        \n",
    "    }\n",
    "    \n",
    "    # Create subject-level datasets without using sample weights.\n",
    "    train_df, predictor_cols = create_subject_dataset(fitbit_reg_dict['train'], outcome_col=\"SI_mean\", use_weights=False)\n",
    "    test_df, _ = create_subject_dataset(fitbit_reg_dict['test'], outcome_col=\"SI_mean\", use_weights=False)\n",
    "    \n",
    "    X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "    y_train = train_df[\"SI_mean\"].values.astype(np.float32)\n",
    "    # All weights are 1.0\n",
    "    sw_train = np.ones_like(y_train, dtype=np.float32)\n",
    "    \n",
    "    X_test = np.stack(test_df[\"X\"].values, axis=0)\n",
    "    y_test = test_df[\"SI_mean\"].values.astype(np.float32)\n",
    "    sw_test = np.ones_like(y_test, dtype=np.float32)\n",
    "    \n",
    "    n_subjects, input_channels, seq_len = X_train.shape\n",
    "    \n",
    "    # Build convolutional network.\n",
    "    kernel_size = params[\"kernel_size_0\"]\n",
    "    stride = params[\"stride_0\"]\n",
    "    dilation = params[\"dilation_0\"]\n",
    "    padding = ((kernel_size - 1) // 2) * dilation\n",
    "    conv = nn.Conv1d(in_channels=input_channels, \n",
    "                     out_channels=params[\"n_filters_0\"],\n",
    "                     kernel_size=kernel_size, \n",
    "                     stride=stride, \n",
    "                     dilation=dilation, \n",
    "                     padding=padding)\n",
    "    relu = nn.ReLU()\n",
    "    layers = [conv, relu, nn.Dropout(params[\"dropout_prob\"])]\n",
    "    if params[\"use_pool_0\"]:\n",
    "        layers.append(nn.MaxPool1d(kernel_size=2))\n",
    "        conv_out_len = pool_output_length(seq_len, 2)\n",
    "    else:\n",
    "        conv_out_len = seq_len\n",
    "    conv_net = nn.Sequential(*layers)\n",
    "    flattened_dim = params[\"n_filters_0\"] * conv_out_len\n",
    "    \n",
    "    # Build fully connected network.\n",
    "    fc_layers = []\n",
    "    in_features = flattened_dim\n",
    "    for _ in range(params[\"n_hidden\"]):\n",
    "        fc_layers.append(nn.Linear(in_features, 64))\n",
    "        fc_layers.append(nn.ReLU())\n",
    "        fc_layers.append(nn.Dropout(params[\"dropout_prob\"]))\n",
    "        in_features = 64\n",
    "    fc_layers.append(nn.Linear(in_features, 1))\n",
    "    fc_net = nn.Sequential(*fc_layers)\n",
    "    \n",
    "    class FitRegCNN(nn.Module):\n",
    "        def __init__(self, conv_net, fc_net):\n",
    "            super(FitRegCNN, self).__init__()\n",
    "            self.conv = conv_net\n",
    "            self.fc = fc_net\n",
    "        def forward(self, x):\n",
    "            x = self.conv(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            return self.fc(x)\n",
    "    \n",
    "    model = FitRegCNN(conv_net, fc_net).to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "    loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).view(-1, 1),\n",
    "        torch.tensor(sw_train, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    test_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.float32),\n",
    "        torch.tensor(y_test, dtype=torch.float32).view(-1, 1),\n",
    "        torch.tensor(sw_test, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    # Train the model.\n",
    "    model.train()\n",
    "    for epoch in range(params[\"num_epochs\"]):\n",
    "        for X_batch, y_batch, w_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            w_batch = w_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = (loss_fn(outputs, y_batch).view(-1) * w_batch.view(-1)).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Helper to get predictions.\n",
    "    def get_preds(loader):\n",
    "        preds, truths = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch, _ in loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                preds.append(outputs.cpu().numpy())\n",
    "                truths.append(y_batch.cpu().numpy())\n",
    "        preds = np.concatenate(preds).flatten()\n",
    "        truths = np.concatenate(truths).flatten()\n",
    "        return truths, preds\n",
    "\n",
    "    model.eval()\n",
    "    y_train_true, y_train_pred = get_preds(train_loader)\n",
    "    y_test_true, y_test_pred = get_preds(test_loader)\n",
    "    \n",
    "    perf_train = compute_regression_perf(y_train_true, y_train_pred)\n",
    "    perf_test  = compute_regression_perf(y_test_true, y_test_pred)\n",
    "    \n",
    "    perf_train_df = pd.DataFrame({\n",
    "        \"model\": [\"fitbit_nw_regression\"],\n",
    "        \"type\": [\"train\"],\n",
    "        \"1\": [perf_train[\"1\"]],\n",
    "        \"2\": [perf_train[\"2\"]],\n",
    "        \"3\": [perf_train[\"3\"]],\n",
    "        \"4\": [perf_train[\"4\"]],\n",
    "        \"5\": [perf_train[\"5\"]],\n",
    "        \"overall\": [perf_train[\"overall\"]]\n",
    "    })\n",
    "    perf_test_df = pd.DataFrame({\n",
    "        \"model\": [\"fitbit_nw_regression\"],\n",
    "        \"type\": [\"test\"],\n",
    "        \"1\": [perf_test[\"1\"]],\n",
    "        \"2\": [perf_test[\"2\"]],\n",
    "        \"3\": [perf_test[\"3\"]],\n",
    "        \"4\": [perf_test[\"4\"]],\n",
    "        \"5\": [perf_test[\"5\"]],\n",
    "        \"overall\": [perf_test[\"overall\"]]\n",
    "    })\n",
    "    performance_df = pd.concat([perf_train_df, perf_test_df], ignore_index=True)\n",
    "    \n",
    "    # Compute Integrated Gradients for SHAP.\n",
    "    set_seed(42)\n",
    "    model_for_attr = model.module if hasattr(model, 'module') else model\n",
    "    ig = IntegratedGradients(model_for_attr)\n",
    "    input_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    baseline = torch.zeros_like(input_tensor)\n",
    "    attributions = ig.attribute(input_tensor, baselines=baseline, n_steps=50)\n",
    "    mean_attr = attributions.mean(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    sd_attr = attributions.abs().std(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    if len(predictor_cols) != len(mean_attr):\n",
    "        min_len = min(len(predictor_cols), len(mean_attr))\n",
    "        predictor_names = predictor_cols[:min_len]\n",
    "        mean_attr = mean_attr[:min_len]\n",
    "        sd_attr = sd_attr[:min_len]\n",
    "    else:\n",
    "        predictor_names = predictor_cols\n",
    "    shap_df = pd.DataFrame({\n",
    "        \"predictor\": predictor_names,\n",
    "        \"mean_abs_integrated_gradients\": mean_attr,\n",
    "        \"sd_abs_integrated_gradients\": sd_attr\n",
    "    })\n",
    "    \n",
    "    #################################################################\n",
    "    # CROSS-VALIDATION for non-weight Fitbit regression\n",
    "    cv_splits = get_stratified_cv_splits(fitbit_reg_dict['train'], subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5)\n",
    "    cv_fold_metrics = []\n",
    "    \n",
    "    for fold, (cv_train_raw, cv_val_raw) in enumerate(cv_splits, start=1):\n",
    "        cv_train_df, _ = create_subject_dataset(cv_train_raw, outcome_col=\"SI_mean\", use_weights=False)\n",
    "        cv_val_df, _ = create_subject_dataset(cv_val_raw, outcome_col=\"SI_mean\", use_weights=False)\n",
    "        X_cv_train = np.stack(cv_train_df[\"X\"].values, axis=0)\n",
    "        y_cv_train = cv_train_df[\"SI_mean\"].values.astype(np.float32)\n",
    "        sw_cv_train = np.ones_like(y_cv_train, dtype=np.float32)\n",
    "        X_cv_val = np.stack(cv_val_df[\"X\"].values, axis=0)\n",
    "        y_cv_val = cv_val_df[\"SI_mean\"].values.astype(np.float32)\n",
    "        sw_cv_val = np.ones_like(y_cv_val, dtype=np.float32)\n",
    "        \n",
    "        n_cv, input_channels_cv, seq_len_cv = X_cv_train.shape\n",
    "        \n",
    "        # Build a new model instance (replicating architecture and hyperparameters)\n",
    "        kernel_size = params[\"kernel_size_0\"]\n",
    "        stride = params[\"stride_0\"]\n",
    "        dilation = params[\"dilation_0\"]\n",
    "        padding = ((kernel_size - 1) // 2) * dilation\n",
    "        conv_cv = nn.Conv1d(in_channels=input_channels_cv, \n",
    "                            out_channels=params[\"n_filters_0\"],\n",
    "                            kernel_size=kernel_size, \n",
    "                            stride=stride, \n",
    "                            dilation=dilation, \n",
    "                            padding=padding)\n",
    "        relu_cv = nn.ReLU()\n",
    "        layers_cv = [conv_cv, relu_cv, nn.Dropout(params[\"dropout_prob\"])]\n",
    "        if params[\"use_pool_0\"]:\n",
    "            layers_cv.append(nn.MaxPool1d(kernel_size=2))\n",
    "            conv_out_len_cv = pool_output_length(seq_len_cv, 2)\n",
    "        else:\n",
    "            conv_out_len_cv = seq_len_cv\n",
    "        conv_net_cv = nn.Sequential(*layers_cv)\n",
    "        flattened_dim_cv = params[\"n_filters_0\"] * conv_out_len_cv\n",
    "        \n",
    "        fc_layers_cv = []\n",
    "        in_features_cv = flattened_dim_cv\n",
    "        for _ in range(params[\"n_hidden\"]):\n",
    "            fc_layers_cv.append(nn.Linear(in_features_cv, 64))\n",
    "            fc_layers_cv.append(nn.ReLU())\n",
    "            fc_layers_cv.append(nn.Dropout(params[\"dropout_prob\"]))\n",
    "            in_features_cv = 64\n",
    "        fc_layers_cv.append(nn.Linear(in_features_cv, 1))\n",
    "        fc_net_cv = nn.Sequential(*fc_layers_cv)\n",
    "        \n",
    "        class FitRegCNN_CV(nn.Module):\n",
    "            def __init__(self, conv_net, fc_net):\n",
    "                super(FitRegCNN_CV, self).__init__()\n",
    "                self.conv = conv_net\n",
    "                self.fc = fc_net\n",
    "            def forward(self, x):\n",
    "                x = self.conv(x)\n",
    "                x = x.view(x.size(0), -1)\n",
    "                return self.fc(x)\n",
    "        \n",
    "        model_cv = FitRegCNN_CV(conv_net_cv, fc_net_cv).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model_cv = nn.DataParallel(model_cv)\n",
    "        optimizer_cv = optim.Adam(model_cv.parameters(), lr=params[\"lr\"])\n",
    "        loss_fn_cv = nn.MSELoss(reduction=\"none\")\n",
    "        \n",
    "        train_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_train, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_train, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(sw_cv_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        val_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_val, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_val, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(sw_cv_val, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader_cv = torch.utils.data.DataLoader(train_dataset_cv, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "        val_loader_cv = torch.utils.data.DataLoader(val_dataset_cv, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "        \n",
    "        model_cv.train()\n",
    "        for epoch in range(params[\"num_epochs\"]):\n",
    "            for X_batch, y_batch, w_batch in train_loader_cv:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                w_batch = w_batch.to(device)\n",
    "                optimizer_cv.zero_grad()\n",
    "                outputs = model_cv(X_batch)\n",
    "                loss = (loss_fn_cv(outputs, y_batch).view(-1) * w_batch.view(-1)).mean()\n",
    "                loss.backward()\n",
    "                optimizer_cv.step()\n",
    "        \n",
    "        # Evaluation on CV validation set\n",
    "        model_cv.eval()\n",
    "        def get_preds_cv(loader, model_obj):\n",
    "            preds, truths = [], []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch, _ in loader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    outputs = model_obj(X_batch)\n",
    "                    preds.append(outputs.cpu().numpy())\n",
    "                    truths.append(y_batch.cpu().numpy())\n",
    "            preds = np.concatenate(preds).flatten()\n",
    "            truths = np.concatenate(truths).flatten()\n",
    "            return truths, preds\n",
    "        y_val_true_cv, y_val_pred_cv = get_preds_cv(val_loader_cv, model_cv)\n",
    "        fold_perf = compute_regression_perf(y_val_true_cv, y_val_pred_cv)\n",
    "        # Compute overall MSE for the CV fold.\n",
    "        fold_mse = np.mean((y_val_true_cv - y_val_pred_cv) ** 2)\n",
    "        fold_perf[\"mse\"] = fold_mse\n",
    "        cv_fold_metrics.append(fold_perf)\n",
    "    \n",
    "    # Aggregate fold metrics.\n",
    "    keys = [\"1\", \"2\", \"3\", \"4\", \"5\", \"overall\", \"mse\"]\n",
    "    mean_metrics = {k: np.mean([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    sd_metrics   = {k: np.std([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    cv_val_df = pd.DataFrame({\n",
    "        \"stat\": [\"mean\", \"sd\"],\n",
    "        \"model\": [\"fitbit_nw_regression\", \"fitbit_nw_regression\"],\n",
    "        \"1\": [mean_metrics[\"1\"], sd_metrics[\"1\"]],\n",
    "        \"2\": [mean_metrics[\"2\"], sd_metrics[\"2\"]],\n",
    "        \"3\": [mean_metrics[\"3\"], sd_metrics[\"3\"]],\n",
    "        \"4\": [mean_metrics[\"4\"], sd_metrics[\"4\"]],\n",
    "        \"5\": [mean_metrics[\"5\"], sd_metrics[\"5\"]],\n",
    "        \"overall\": [mean_metrics[\"overall\"], sd_metrics[\"overall\"]],\n",
    "        \"mse\": [mean_metrics[\"mse\"], sd_metrics[\"mse\"]]\n",
    "    })\n",
    "    \n",
    "    return performance_df, shap_df, cv_val_df\n",
    "\n",
    "################################################################\n",
    "# Non‐Weighted Fitbit Classification\n",
    "def run_fitbit_classification_nw_best(fitbit_dict):\n",
    "    set_seed(42)\n",
    "    # Updated hyperparameters for non‐weighted fitbit classification.\n",
    "    # Only num_epochs and lr are updated.\n",
    "    params = {\n",
    "        \"lr\": 0.00025685423297033865,        \n",
    "        \"dropout_prob\": 0.45160954145010296,    \n",
    "        \"num_epochs\": 9,                       \n",
    "        \"n_filters_0\": 24,                      \n",
    "        \"kernel_size_0\": 3                      \n",
    "    }\n",
    "    # Create subject-level datasets without using sample weights.\n",
    "    train_df, predictor_cols = create_subject_dataset(fitbit_dict['train'], outcome_col=\"is_SI\", use_weights=False)\n",
    "    test_df, _ = create_subject_dataset(fitbit_dict['test'], outcome_col=\"is_SI\", use_weights=False)\n",
    "    \n",
    "    X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "    y_train = train_df[\"is_SI\"].values.astype(np.float32)\n",
    "    X_test = np.stack(test_df[\"X\"].values, axis=0)\n",
    "    y_test = test_df[\"is_SI\"].values.astype(np.float32)\n",
    "    \n",
    "    n_subjects, input_channels, seq_len = X_train.shape\n",
    "    \n",
    "    kernel_size = params[\"kernel_size_0\"]\n",
    "    stride = 1\n",
    "    dilation = 1\n",
    "    padding = ((kernel_size - 1) // 2) * dilation\n",
    "    conv = nn.Conv1d(in_channels=input_channels, out_channels=params[\"n_filters_0\"],\n",
    "                     kernel_size=kernel_size, stride=stride, dilation=dilation, padding=padding)\n",
    "    relu = nn.ReLU()\n",
    "    layers = [conv, relu, nn.Dropout(params[\"dropout_prob\"])]\n",
    "    conv_net = nn.Sequential(*layers)\n",
    "    flattened_dim = params[\"n_filters_0\"] * conv_output_length(seq_len, kernel_size, stride, padding, dilation)\n",
    "    \n",
    "    fc_net = nn.Sequential(nn.Linear(flattened_dim, 1))\n",
    "    \n",
    "    class FitbitClassCNN(nn.Module):\n",
    "        def __init__(self, conv_net, fc_net):\n",
    "            super(FitbitClassCNN, self).__init__()\n",
    "            self.conv = conv_net\n",
    "            self.fc = fc_net\n",
    "        def forward(self, x):\n",
    "            x = self.conv(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            return self.fc(x)\n",
    "    \n",
    "    model = FitbitClassCNN(conv_net, fc_net).to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    num_epochs = params[\"num_epochs\"]\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    test_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.float32),\n",
    "        torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def get_preds(loader):\n",
    "        preds, truths = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "                truths.append(y_batch.cpu().numpy())\n",
    "        preds = np.concatenate(preds).flatten()\n",
    "        truths = np.concatenate(truths).flatten()\n",
    "        return truths, preds\n",
    "\n",
    "    model.eval()\n",
    "    y_train_true, y_train_pred = get_preds(train_loader)\n",
    "    y_test_true, y_test_pred = get_preds(test_loader)\n",
    "    \n",
    "    train_auc = roc_auc_score(y_train_true, y_train_pred)\n",
    "    test_auc = roc_auc_score(y_test_true, y_test_pred)\n",
    "    y_train_bin = (y_train_pred >= 0.5).astype(np.float32)\n",
    "    y_test_bin = (y_test_pred >= 0.5).astype(np.float32)\n",
    "    TP_train = np.sum((y_train_bin == 1) & (y_train_true == 1))\n",
    "    FN_train = np.sum((y_train_bin == 0) & (y_train_true == 1))\n",
    "    TN_train = np.sum((y_train_bin == 0) & (y_train_true == 0))\n",
    "    FP_train = np.sum((y_train_bin == 1) & (y_train_true == 0))\n",
    "    train_acc = np.mean(y_train_bin == y_train_true)\n",
    "    train_sens = TP_train / (TP_train + FN_train) if (TP_train + FN_train) > 0 else np.nan\n",
    "    train_spec = TN_train / (TN_train + FP_train) if (TN_train + FP_train) > 0 else np.nan\n",
    "\n",
    "    TP_test = np.sum((y_test_bin == 1) & (y_test_true == 1))\n",
    "    FN_test = np.sum((y_test_bin == 0) & (y_test_true == 1))\n",
    "    TN_test = np.sum((y_test_bin == 0) & (y_test_true == 0))\n",
    "    FP_test = np.sum((y_test_bin == 1) & (y_test_true == 0))\n",
    "    test_acc = np.mean(y_test_bin == y_test_true)\n",
    "    test_sens = TP_test / (TP_test + FN_test) if (TP_test + FN_test) > 0 else np.nan\n",
    "    test_spec = TN_test / (TN_test + FP_test) if (TN_test + FP_test) > 0 else np.nan\n",
    "    \n",
    "    perf_train_df = pd.DataFrame({\n",
    "        \"model\": [\"fitbit_nw_classification\"],\n",
    "        \"type\": [\"train\"],\n",
    "        \"accuracy\": [train_acc],\n",
    "        \"sensitivity\": [train_sens],\n",
    "        \"specificity\": [train_spec]\n",
    "    })\n",
    "    perf_test_df = pd.DataFrame({\n",
    "        \"model\": [\"fitbit_nw_classification\"],\n",
    "        \"type\": [\"test\"],\n",
    "        \"accuracy\": [test_acc],\n",
    "        \"sensitivity\": [test_sens],\n",
    "        \"specificity\": [test_spec]\n",
    "    })\n",
    "    performance_df = pd.concat([perf_train_df, perf_test_df], ignore_index=True)\n",
    "    \n",
    "    # Compute Integrated Gradients for SHAP.\n",
    "    set_seed(42)\n",
    "    model_for_attr = model.module if hasattr(model, 'module') else model\n",
    "    ig = IntegratedGradients(model_for_attr)\n",
    "    input_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    baseline = torch.zeros_like(input_tensor)\n",
    "    attributions = ig.attribute(input_tensor, baselines=baseline, n_steps=50)\n",
    "    mean_attr = attributions.mean(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    sd_attr = attributions.abs().std(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    if len(predictor_cols) != len(mean_attr):\n",
    "        min_len = min(len(predictor_cols), len(mean_attr))\n",
    "        predictor_names = predictor_cols[:min_len]\n",
    "        mean_attr = mean_attr[:min_len]\n",
    "        sd_attr = sd_attr[:min_len]\n",
    "    else:\n",
    "        predictor_names = predictor_cols\n",
    "    shap_df = pd.DataFrame({\n",
    "        \"predictor\": predictor_names,\n",
    "        \"mean_abs_integrated_gradients\": mean_attr,\n",
    "        \"sd_abs_integrated_gradients\": sd_attr\n",
    "    })\n",
    "    \n",
    "    #################################################################\n",
    "    # CROSS-VALIDATION for non-weight fitbit classification\n",
    "    cv_splits = get_stratified_cv_splits(fitbit_dict['train'], subject_id=\"PatientID\", target_var=\"is_SI\", n_splits=5)\n",
    "    cv_fold_metrics = []\n",
    "    \n",
    "    for fold, (cv_train_raw, cv_val_raw) in enumerate(cv_splits, start=1):\n",
    "        cv_train_df, _ = create_subject_dataset(cv_train_raw, outcome_col=\"is_SI\", use_weights=False)\n",
    "        cv_val_df, _ = create_subject_dataset(cv_val_raw, outcome_col=\"is_SI\", use_weights=False)\n",
    "        X_cv_train = np.stack(cv_train_df[\"X\"].values, axis=0)\n",
    "        y_cv_train = cv_train_df[\"is_SI\"].values.astype(np.float32)\n",
    "        X_cv_val = np.stack(cv_val_df[\"X\"].values, axis=0)\n",
    "        y_cv_val = cv_val_df[\"is_SI\"].values.astype(np.float32)\n",
    "        \n",
    "        n_cv, input_channels_cv, seq_len_cv = X_cv_train.shape\n",
    "        kernel_size = params[\"kernel_size_0\"]\n",
    "        stride = 1\n",
    "        dilation = 1\n",
    "        padding = ((kernel_size - 1) // 2) * dilation\n",
    "        conv_cv = nn.Conv1d(in_channels=input_channels_cv,\n",
    "                            out_channels=params[\"n_filters_0\"],\n",
    "                            kernel_size=kernel_size, stride=stride, dilation=dilation, padding=padding)\n",
    "        relu_cv = nn.ReLU()\n",
    "        layers_cv = [conv_cv, relu_cv, nn.Dropout(params[\"dropout_prob\"])]\n",
    "        conv_net_cv = nn.Sequential(*layers_cv)\n",
    "        flattened_dim_cv = params[\"n_filters_0\"] * conv_output_length(seq_len_cv, kernel_size, stride, padding, dilation)\n",
    "        \n",
    "        fc_net_cv = nn.Sequential(nn.Linear(flattened_dim_cv, 1))\n",
    "        \n",
    "        class FitbitClassCNN_CV(nn.Module):\n",
    "            def __init__(self, conv_net, fc_net):\n",
    "                super(FitbitClassCNN_CV, self).__init__()\n",
    "                self.conv = conv_net\n",
    "                self.fc = fc_net\n",
    "            def forward(self, x):\n",
    "                x = self.conv(x)\n",
    "                x = x.view(x.size(0), -1)\n",
    "                return self.fc(x)\n",
    "        \n",
    "        model_cv = FitbitClassCNN_CV(conv_net_cv, fc_net_cv).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model_cv = nn.DataParallel(model_cv)\n",
    "        optimizer_cv = optim.Adam(model_cv.parameters(), lr=params[\"lr\"])\n",
    "        loss_fn_cv = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        train_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_train, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        val_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_val, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_val, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader_cv = torch.utils.data.DataLoader(train_dataset_cv, batch_size=32, shuffle=True)\n",
    "        val_loader_cv = torch.utils.data.DataLoader(val_dataset_cv, batch_size=32, shuffle=False)\n",
    "        \n",
    "        model_cv.train()\n",
    "        for epoch in range(params[\"num_epochs\"]):\n",
    "            for X_batch, y_batch in train_loader_cv:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                optimizer_cv.zero_grad()\n",
    "                outputs = model_cv(X_batch)\n",
    "                loss = loss_fn_cv(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer_cv.step()\n",
    "        \n",
    "        model_cv.eval()\n",
    "        def get_preds_cv(loader, model_obj):\n",
    "            preds, truths = [], []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in loader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    outputs = model_obj(X_batch)\n",
    "                    preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "                    truths.append(y_batch.cpu().numpy())\n",
    "            preds = np.concatenate(preds).flatten()\n",
    "            truths = np.concatenate(truths).flatten()\n",
    "            return truths, preds\n",
    "        y_val_true_cv, y_val_pred_cv = get_preds_cv(val_loader_cv, model_cv)\n",
    "        auc_cv = roc_auc_score(y_val_true_cv, y_val_pred_cv)\n",
    "        y_val_bin = (y_val_pred_cv >= 0.5).astype(np.float32)\n",
    "        TP = np.sum((y_val_bin == 1) & (y_val_true_cv == 1))\n",
    "        FN = np.sum((y_val_bin == 0) & (y_val_true_cv == 1))\n",
    "        TN = np.sum((y_val_bin == 0) & (y_val_true_cv == 0))\n",
    "        FP = np.sum((y_val_bin == 1) & (y_val_true_cv == 0))\n",
    "        acc = np.mean(y_val_bin == y_val_true_cv)\n",
    "        sens = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "        spec = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
    "        # Compute binary cross entropy for the CV fold.\n",
    "        eps = 1e-8\n",
    "        bce_cv = np.mean(- (y_val_true_cv * np.log(np.clip(y_val_pred_cv, eps, 1 - eps)) + \n",
    "                            (1 - y_val_true_cv) * np.log(np.clip(1 - y_val_pred_cv, eps, 1 - eps))))\n",
    "        cv_fold_metrics.append({\"AUC\": auc_cv, \"accuracy\": acc, \"sensitivity\": sens, \"specificity\": spec, \"bce\": bce_cv})\n",
    "    \n",
    "    mean_metrics = {k: np.mean([fold[k] for fold in cv_fold_metrics]) for k in [\"AUC\", \"accuracy\", \"sensitivity\", \"specificity\", \"bce\"]}\n",
    "    sd_metrics = {k: np.std([fold[k] for fold in cv_fold_metrics]) for k in [\"AUC\", \"accuracy\", \"sensitivity\", \"specificity\", \"bce\"]}\n",
    "    cv_val_df = pd.DataFrame({\n",
    "        \"stat\": [\"mean\", \"sd\"],\n",
    "        \"model\": [\"fitbit_nw_classification\", \"fitbit_nw_classification\"],\n",
    "        \"AUC\": [mean_metrics[\"AUC\"], sd_metrics[\"AUC\"]],\n",
    "        \"accuracy\": [mean_metrics[\"accuracy\"], sd_metrics[\"accuracy\"]],\n",
    "        \"sensitivity\": [mean_metrics[\"sensitivity\"], sd_metrics[\"sensitivity\"]],\n",
    "        \"specificity\": [mean_metrics[\"specificity\"], sd_metrics[\"specificity\"]],\n",
    "        \"bce\": [mean_metrics[\"bce\"], sd_metrics[\"bce\"]]\n",
    "    })\n",
    "    \n",
    "    return performance_df, shap_df, cv_val_df\n",
    "\n",
    "################################################################\n",
    "# Non‐Weighted Comb Regression\n",
    "def run_comb_regression_nw_best(comb_dict):\n",
    "    set_seed(42)\n",
    "    # Updated hyperparameters for non‐weighted comb regression.\n",
    "    params = {\n",
    "        \"lr\": 0.00013555143635027785,         \n",
    "        \"dropout_prob\": 0.13884004660199356,     \n",
    "        \"num_epochs\": 10,                       \n",
    "        \"n_filters_0\": 16,                      \n",
    "        \"kernel_size_0\": 3,                     \n",
    "        \"n_conv\": 3,                           \n",
    "        \"n_cov\": 1,                            \n",
    "        \"use_regularization\": False,          \n",
    "        \"dilation_0\": 2,                      \n",
    "        \"use_pool_0\": True                    \n",
    "    }\n",
    "    # Create subject-level datasets without using sample weights.\n",
    "    train_df, predictor_cols = create_subject_dataset(comb_dict['train'], outcome_col=\"SI_mean\", use_weights=False)\n",
    "    test_df, _ = create_subject_dataset(comb_dict['test'], outcome_col=\"SI_mean\", use_weights=False)\n",
    "    \n",
    "    X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "    y_train = train_df[\"SI_mean\"].values.astype(np.float32)\n",
    "    sw_train = np.ones_like(y_train, dtype=np.float32)\n",
    "    \n",
    "    X_test = np.stack(test_df[\"X\"].values, axis=0)\n",
    "    y_test = test_df[\"SI_mean\"].values.astype(np.float32)\n",
    "    sw_test = np.ones_like(y_test, dtype=np.float32)\n",
    "    \n",
    "    n_subjects, input_channels, seq_len = X_train.shape\n",
    "\n",
    "    # Build convolutional network.\n",
    "    kernel_size = params[\"kernel_size_0\"]\n",
    "    stride = 1\n",
    "    dilation = params[\"dilation_0\"]\n",
    "    padding = ((kernel_size - 1) // 2) * dilation\n",
    "    conv = nn.Conv1d(in_channels=input_channels,\n",
    "                     out_channels=params[\"n_filters_0\"],\n",
    "                     kernel_size=kernel_size,\n",
    "                     stride=stride,\n",
    "                     dilation=dilation,\n",
    "                     padding=padding)\n",
    "    relu = nn.ReLU()\n",
    "    layers = [conv, relu, nn.Dropout(params[\"dropout_prob\"])]\n",
    "    if params[\"use_pool_0\"]:\n",
    "        layers.append(nn.MaxPool1d(kernel_size=2))\n",
    "        conv_out_len = pool_output_length(seq_len, 2)\n",
    "    else:\n",
    "        conv_out_len = seq_len\n",
    "    conv_net = nn.Sequential(*layers)\n",
    "    flattened_dim = params[\"n_filters_0\"] * conv_out_len\n",
    "    \n",
    "    # For comb regression, a direct mapping is used.\n",
    "    fc_net = nn.Linear(flattened_dim, 1)\n",
    "    \n",
    "    class CombRegCNN(nn.Module):\n",
    "        def __init__(self, conv_net, fc_net):\n",
    "            super(CombRegCNN, self).__init__()\n",
    "            self.conv = conv_net\n",
    "            self.fc = fc_net\n",
    "        def forward(self, x):\n",
    "            x = self.conv(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            return self.fc(x)\n",
    "    \n",
    "    model = CombRegCNN(conv_net, fc_net).to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "    loss_fn = nn.MSELoss(reduction='none')\n",
    "    num_epochs = params[\"num_epochs\"]\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).view(-1, 1),\n",
    "        torch.tensor(sw_train, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    test_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.float32),\n",
    "        torch.tensor(y_test, dtype=torch.float32).view(-1, 1),\n",
    "        torch.tensor(sw_test, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for X_batch, y_batch, w_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            w_batch = w_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = (loss_fn(outputs, y_batch).view(-1) * w_batch.view(-1)).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def get_preds(loader):\n",
    "        preds, truths = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch, _ in loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                preds.append(outputs.cpu().numpy())\n",
    "                truths.append(y_batch.cpu().numpy())\n",
    "        preds = np.concatenate(preds).flatten()\n",
    "        truths = np.concatenate(truths).flatten()\n",
    "        return truths, preds\n",
    "\n",
    "    model.eval()\n",
    "    y_train_true, y_train_pred = get_preds(train_loader)\n",
    "    y_test_true, y_test_pred = get_preds(test_loader)\n",
    "    \n",
    "    perf_train = compute_regression_perf(y_train_true, y_train_pred)\n",
    "    perf_test  = compute_regression_perf(y_test_true, y_test_pred)\n",
    "    \n",
    "    perf_train_df = pd.DataFrame({\n",
    "        \"model\": [\"comb_nw_regression\"],\n",
    "        \"type\": [\"train\"],\n",
    "        \"1\": [perf_train[\"1\"]],\n",
    "        \"2\": [perf_train[\"2\"]],\n",
    "        \"3\": [perf_train[\"3\"]],\n",
    "        \"4\": [perf_train[\"4\"]],\n",
    "        \"5\": [perf_train[\"5\"]],\n",
    "        \"overall\": [perf_train[\"overall\"]]\n",
    "    })\n",
    "    perf_test_df = pd.DataFrame({\n",
    "        \"model\": [\"comb_nw_regression\"],\n",
    "        \"type\": [\"test\"],\n",
    "        \"1\": [perf_test[\"1\"]],\n",
    "        \"2\": [perf_test[\"2\"]],\n",
    "        \"3\": [perf_test[\"3\"]],\n",
    "        \"4\": [perf_test[\"4\"]],\n",
    "        \"5\": [perf_test[\"5\"]],\n",
    "        \"overall\": [perf_test[\"overall\"]]\n",
    "    })\n",
    "    performance_df = pd.concat([perf_train_df, perf_test_df], ignore_index=True)\n",
    "    \n",
    "    # Compute Integrated Gradients for SHAP.\n",
    "    set_seed(42)\n",
    "    model_for_attr = model.module if hasattr(model, 'module') else model\n",
    "    ig = IntegratedGradients(model_for_attr)\n",
    "    input_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    baseline = torch.zeros_like(input_tensor)\n",
    "    attributions = ig.attribute(input_tensor, baselines=baseline, n_steps=50)\n",
    "    mean_attr = attributions.mean(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    sd_attr = attributions.abs().std(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    if len(predictor_cols) != len(mean_attr):\n",
    "        min_len = min(len(predictor_cols), len(mean_attr))\n",
    "        predictor_names = predictor_cols[:min_len]\n",
    "        mean_attr = mean_attr[:min_len]\n",
    "        sd_attr = sd_attr[:min_len]\n",
    "    else:\n",
    "        predictor_names = predictor_cols\n",
    "    shap_df = pd.DataFrame({\n",
    "        \"predictor\": predictor_names,\n",
    "        \"mean_abs_integrated_gradients\": mean_attr,\n",
    "        \"sd_abs_integrated_gradients\": sd_attr\n",
    "    })\n",
    "    \n",
    "    #################################################################\n",
    "    # CROSS-VALIDATION for non-weight comb regression\n",
    "    cv_splits = get_stratified_cv_splits(comb_dict['train'], subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5)\n",
    "    cv_fold_metrics = []\n",
    "    \n",
    "    for fold, (cv_train_raw, cv_val_raw) in enumerate(cv_splits, start=1):\n",
    "        cv_train_df, _ = create_subject_dataset(cv_train_raw, outcome_col=\"SI_mean\", use_weights=False)\n",
    "        cv_val_df, _ = create_subject_dataset(cv_val_raw, outcome_col=\"SI_mean\", use_weights=False)\n",
    "        X_cv_train = np.stack(cv_train_df[\"X\"].values, axis=0)\n",
    "        y_cv_train = cv_train_df[\"SI_mean\"].values.astype(np.float32)\n",
    "        sw_cv_train = np.ones_like(y_cv_train, dtype=np.float32)\n",
    "        X_cv_val = np.stack(cv_val_df[\"X\"].values, axis=0)\n",
    "        y_cv_val = cv_val_df[\"SI_mean\"].values.astype(np.float32)\n",
    "        sw_cv_val = np.ones_like(y_cv_val, dtype=np.float32)\n",
    "        \n",
    "        n_cv, input_channels_cv, seq_len_cv = X_cv_train.shape\n",
    "        kernel_size = params[\"kernel_size_0\"]\n",
    "        stride = 1\n",
    "        dilation = params[\"dilation_0\"]\n",
    "        padding = ((kernel_size - 1) // 2) * dilation\n",
    "        conv_cv = nn.Conv1d(in_channels=input_channels_cv,\n",
    "                            out_channels=params[\"n_filters_0\"],\n",
    "                            kernel_size=kernel_size,\n",
    "                            stride=stride,\n",
    "                            dilation=dilation,\n",
    "                            padding=padding)\n",
    "        relu_cv = nn.ReLU()\n",
    "        layers_cv = [conv_cv, relu_cv, nn.Dropout(params[\"dropout_prob\"])]\n",
    "        if params[\"use_pool_0\"]:\n",
    "            layers_cv.append(nn.MaxPool1d(kernel_size=2))\n",
    "            conv_out_len_cv = pool_output_length(seq_len_cv, 2)\n",
    "        else:\n",
    "            conv_out_len_cv = seq_len_cv\n",
    "        conv_net_cv = nn.Sequential(*layers_cv)\n",
    "        flattened_dim_cv = params[\"n_filters_0\"] * conv_out_len_cv\n",
    "        \n",
    "        fc_net_cv = nn.Linear(flattened_dim_cv, 1)\n",
    "        \n",
    "        class CombRegCNN_CV(nn.Module):\n",
    "            def __init__(self, conv_net, fc_net):\n",
    "                super(CombRegCNN_CV, self).__init__()\n",
    "                self.conv = conv_net\n",
    "                self.fc = fc_net\n",
    "            def forward(self, x):\n",
    "                x = self.conv(x)\n",
    "                x = x.view(x.size(0), -1)\n",
    "                return self.fc(x)\n",
    "        \n",
    "        model_cv = CombRegCNN_CV(conv_net_cv, fc_net_cv).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model_cv = nn.DataParallel(model_cv)\n",
    "        optimizer_cv = optim.Adam(model_cv.parameters(), lr=params[\"lr\"])\n",
    "        loss_fn_cv = nn.MSELoss(reduction='none')\n",
    "        \n",
    "        train_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_train, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_train, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(sw_cv_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        val_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_val, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_val, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(sw_cv_val, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader_cv = torch.utils.data.DataLoader(train_dataset_cv, batch_size=32, shuffle=True)\n",
    "        val_loader_cv = torch.utils.data.DataLoader(val_dataset_cv, batch_size=32, shuffle=False)\n",
    "        \n",
    "        model_cv.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for X_batch, y_batch, w_batch in train_loader_cv:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                w_batch = w_batch.to(device)\n",
    "                optimizer_cv.zero_grad()\n",
    "                outputs = model_cv(X_batch)\n",
    "                loss = (loss_fn_cv(outputs, y_batch).view(-1) * w_batch.view(-1)).mean()\n",
    "                loss.backward()\n",
    "                optimizer_cv.step()\n",
    "        \n",
    "        model_cv.eval()\n",
    "        def get_preds_cv(loader, model_obj):\n",
    "            preds, truths = [], []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch, _ in loader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    outputs = model_obj(X_batch)\n",
    "                    preds.append(outputs.cpu().numpy())\n",
    "                    truths.append(y_batch.cpu().numpy())\n",
    "            preds = np.concatenate(preds).flatten()\n",
    "            truths = np.concatenate(truths).flatten()\n",
    "            return truths, preds\n",
    "        y_val_true_cv, y_val_pred_cv = get_preds_cv(val_loader_cv, model_cv)\n",
    "        fold_perf = compute_regression_perf(y_val_true_cv, y_val_pred_cv)\n",
    "        fold_mse = np.mean((y_val_true_cv - y_val_pred_cv) ** 2)\n",
    "        fold_perf[\"mse\"] = fold_mse\n",
    "        cv_fold_metrics.append(fold_perf)\n",
    "    \n",
    "    keys = [\"1\", \"2\", \"3\", \"4\", \"5\", \"overall\", \"mse\"]\n",
    "    mean_metrics = {k: np.mean([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    sd_metrics   = {k: np.std([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    cv_val_df = pd.DataFrame({\n",
    "        \"stat\": [\"mean\", \"sd\"],\n",
    "        \"model\": [\"comb_nw_regression\", \"comb_nw_regression\"],\n",
    "        \"1\": [mean_metrics[\"1\"], sd_metrics[\"1\"]],\n",
    "        \"2\": [mean_metrics[\"2\"], sd_metrics[\"2\"]],\n",
    "        \"3\": [mean_metrics[\"3\"], sd_metrics[\"3\"]],\n",
    "        \"4\": [mean_metrics[\"4\"], sd_metrics[\"4\"]],\n",
    "        \"5\": [mean_metrics[\"5\"], sd_metrics[\"5\"]],\n",
    "        \"overall\": [mean_metrics[\"overall\"], sd_metrics[\"overall\"]],\n",
    "        \"mse\": [mean_metrics[\"mse\"], sd_metrics[\"mse\"]]\n",
    "    })\n",
    "    \n",
    "    return performance_df, shap_df, cv_val_df\n",
    "\n",
    "################################################################\n",
    "# Non‐Weighted Comb Classification\n",
    "def run_comb_classification_nw_best(comb_dict):\n",
    "    set_seed(42)\n",
    "    # Updated hyperparameters for non‐weighted comb classification.\n",
    "    # Only batch_size and n_filters_0 are updated.\n",
    "    params = {\n",
    "        \"lr\": 0.00015704157833575886,        \n",
    "        \"dropout_prob\": 0.45497081070937473,    \n",
    "        \"num_epochs\": 8,                       \n",
    "        \"batch_size\": 48,                      \n",
    "        \"n_filters_0\": 8                       \n",
    "    }\n",
    "    # Create subject-level datasets without using sample weights.\n",
    "    train_df, predictor_cols = create_subject_dataset(comb_dict['train'], outcome_col=\"is_SI\", use_weights=False)\n",
    "    test_df, _ = create_subject_dataset(comb_dict['test'], outcome_col=\"is_SI\", use_weights=False)\n",
    "    \n",
    "    X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "    y_train = train_df[\"is_SI\"].values.astype(np.float32)\n",
    "    X_test = np.stack(test_df[\"X\"].values, axis=0)\n",
    "    y_test = test_df[\"is_SI\"].values.astype(np.float32)\n",
    "    \n",
    "    n_subjects, input_channels, seq_len = X_train.shape\n",
    "    \n",
    "    kernel_size = 3\n",
    "    stride = 1\n",
    "    dilation = 1\n",
    "    padding = ((kernel_size - 1) // 2) * dilation\n",
    "    conv = nn.Conv1d(in_channels=input_channels, out_channels=params[\"n_filters_0\"],\n",
    "                     kernel_size=kernel_size, stride=stride, dilation=dilation, padding=padding)\n",
    "    relu = nn.ReLU()\n",
    "    layers = [conv, relu, nn.Dropout(params[\"dropout_prob\"])]\n",
    "    conv_net = nn.Sequential(*layers)\n",
    "    flattened_dim = params[\"n_filters_0\"] * conv_output_length(seq_len, kernel_size, stride, padding, dilation)\n",
    "    \n",
    "    fc_net = nn.Sequential(nn.Linear(flattened_dim, 1))\n",
    "    \n",
    "    class CombClassCNN(nn.Module):\n",
    "        def __init__(self, conv_net, fc_net):\n",
    "            super(CombClassCNN, self).__init__()\n",
    "            self.conv = conv_net\n",
    "            self.fc = fc_net\n",
    "        def forward(self, x):\n",
    "            x = self.conv(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            return self.fc(x)\n",
    "    \n",
    "    model = CombClassCNN(conv_net, fc_net).to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    num_epochs = params[\"num_epochs\"]\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    test_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.float32),\n",
    "        torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def get_preds(loader):\n",
    "        preds, truths = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "                truths.append(y_batch.cpu().numpy())\n",
    "        preds = np.concatenate(preds).flatten()\n",
    "        truths = np.concatenate(truths).flatten()\n",
    "        return truths, preds\n",
    "\n",
    "    model.eval()\n",
    "    y_train_true, y_train_pred = get_preds(train_loader)\n",
    "    y_test_true, y_test_pred = get_preds(test_loader)\n",
    "    \n",
    "    TP_train = np.sum((y_train_pred >= 0.5) & (y_train_true == 1))\n",
    "    FN_train = np.sum((y_train_pred < 0.5) & (y_train_true == 1))\n",
    "    TN_train = np.sum((y_train_pred < 0.5) & (y_train_true == 0))\n",
    "    FP_train = np.sum((y_train_pred >= 0.5) & (y_train_true == 0))\n",
    "    train_acc = np.mean((y_train_pred >= 0.5) == y_train_true)\n",
    "    train_sens = TP_train / (TP_train + FN_train) if (TP_train + FN_train) > 0 else np.nan\n",
    "    train_spec = TN_train / (TN_train + FP_train) if (TN_train + FP_train) > 0 else np.nan\n",
    "    \n",
    "    TP_test = np.sum((y_test_pred >= 0.5) & (y_test_true == 1))\n",
    "    FN_test = np.sum((y_test_pred < 0.5) & (y_test_true == 1))\n",
    "    TN_test = np.sum((y_test_pred < 0.5) & (y_test_true == 0))\n",
    "    FP_test = np.sum((y_test_pred >= 0.5) & (y_test_true == 0))\n",
    "    test_acc = np.mean((y_test_pred >= 0.5) == y_test_true)\n",
    "    test_sens = TP_test / (TP_test + FN_test) if (TP_test + FN_test) > 0 else np.nan\n",
    "    test_spec = TN_test / (TN_test + FP_test) if (TN_test + FP_test) > 0 else np.nan\n",
    "    \n",
    "    perf_train_df = pd.DataFrame({\n",
    "        \"model\": [\"comb_nw_classification\"],\n",
    "        \"type\": [\"train\"],\n",
    "        \"accuracy\": [train_acc],\n",
    "        \"sensitivity\": [train_sens],\n",
    "        \"specificity\": [train_spec]\n",
    "    })\n",
    "    perf_test_df = pd.DataFrame({\n",
    "        \"model\": [\"comb_nw_classification\"],\n",
    "        \"type\": [\"test\"],\n",
    "        \"accuracy\": [test_acc],\n",
    "        \"sensitivity\": [test_sens],\n",
    "        \"specificity\": [test_spec]\n",
    "    })\n",
    "    performance_df = pd.concat([perf_train_df, perf_test_df], ignore_index=True)\n",
    "    \n",
    "    # Compute Integrated Gradients for SHAP.\n",
    "    set_seed(42)\n",
    "    model_for_attr = model.module if hasattr(model, 'module') else model\n",
    "    ig = IntegratedGradients(model_for_attr)\n",
    "    input_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    baseline = torch.zeros_like(input_tensor)\n",
    "    attributions = ig.attribute(input_tensor, baselines=baseline, n_steps=50)\n",
    "    mean_attr = attributions.mean(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    sd_attr = attributions.abs().std(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    if len(predictor_cols) != len(mean_attr):\n",
    "        min_len = min(len(predictor_cols), len(mean_attr))\n",
    "        predictor_names = predictor_cols[:min_len]\n",
    "        mean_attr = mean_attr[:min_len]\n",
    "        sd_attr = sd_attr[:min_len]\n",
    "    else:\n",
    "        predictor_names = predictor_cols\n",
    "    shap_df = pd.DataFrame({\n",
    "        \"predictor\": predictor_names,\n",
    "        \"mean_abs_integrated_gradients\": mean_attr,\n",
    "        \"sd_abs_integrated_gradients\": sd_attr\n",
    "    })\n",
    "    \n",
    "    #################################################################\n",
    "    # CROSS-VALIDATION for non-weight comb classification\n",
    "    cv_splits = get_stratified_cv_splits(comb_dict['train'], subject_id=\"PatientID\", target_var=\"is_SI\", n_splits=5)\n",
    "    cv_fold_metrics = []\n",
    "    \n",
    "    for fold, (cv_train_raw, cv_val_raw) in enumerate(cv_splits, start=1):\n",
    "        cv_train_df, _ = create_subject_dataset(cv_train_raw, outcome_col=\"is_SI\", use_weights=False)\n",
    "        cv_val_df, _ = create_subject_dataset(cv_val_raw, outcome_col=\"is_SI\", use_weights=False)\n",
    "        X_cv_train = np.stack(cv_train_df[\"X\"].values, axis=0)\n",
    "        y_cv_train = cv_train_df[\"is_SI\"].values.astype(np.float32)\n",
    "        X_cv_val = np.stack(cv_val_df[\"X\"].values, axis=0)\n",
    "        y_cv_val = cv_val_df[\"is_SI\"].values.astype(np.float32)\n",
    "        \n",
    "        n_cv, input_channels_cv, seq_len_cv = X_cv_train.shape\n",
    "        kernel_size = 3\n",
    "        stride = 1\n",
    "        dilation = 1\n",
    "        padding = ((kernel_size - 1) // 2) * dilation\n",
    "        conv_cv = nn.Conv1d(in_channels=input_channels_cv,\n",
    "                            out_channels=params[\"n_filters_0\"],\n",
    "                            kernel_size=kernel_size, stride=stride, dilation=dilation, padding=padding)\n",
    "        relu_cv = nn.ReLU()\n",
    "        layers_cv = [conv_cv, relu_cv, nn.Dropout(params[\"dropout_prob\"])]\n",
    "        conv_net_cv = nn.Sequential(*layers_cv)\n",
    "        flattened_dim_cv = params[\"n_filters_0\"] * conv_output_length(seq_len_cv, kernel_size, stride, padding, dilation)\n",
    "        \n",
    "        fc_net_cv = nn.Sequential(nn.Linear(flattened_dim_cv, 1))\n",
    "        \n",
    "        class CombClassCNN_CV(nn.Module):\n",
    "            def __init__(self, conv_net, fc_net):\n",
    "                super(CombClassCNN_CV, self).__init__()\n",
    "                self.conv = conv_net\n",
    "                self.fc = fc_net\n",
    "            def forward(self, x):\n",
    "                x = self.conv(x)\n",
    "                x = x.view(x.size(0), -1)\n",
    "                return self.fc(x)\n",
    "        \n",
    "        model_cv = CombClassCNN_CV(conv_net_cv, fc_net_cv).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model_cv = nn.DataParallel(model_cv)\n",
    "        optimizer_cv = optim.Adam(model_cv.parameters(), lr=params[\"lr\"])\n",
    "        loss_fn_cv = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        train_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_train, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        val_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_val, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_val, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader_cv = torch.utils.data.DataLoader(train_dataset_cv, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "        val_loader_cv = torch.utils.data.DataLoader(val_dataset_cv, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "        \n",
    "        model_cv.train()\n",
    "        for epoch in range(params[\"num_epochs\"]):\n",
    "            for X_batch, y_batch in train_loader_cv:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                optimizer_cv.zero_grad()\n",
    "                outputs = model_cv(X_batch)\n",
    "                loss = loss_fn_cv(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer_cv.step()\n",
    "        \n",
    "        model_cv.eval()\n",
    "        def get_preds_cv(loader, model_obj):\n",
    "            preds, truths = [], []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in loader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    outputs = model_obj(X_batch)\n",
    "                    preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "                    truths.append(y_batch.cpu().numpy())\n",
    "            preds = np.concatenate(preds).flatten()\n",
    "            truths = np.concatenate(truths).flatten()\n",
    "            return truths, preds\n",
    "        y_val_true_cv, y_val_pred_cv = get_preds_cv(val_loader_cv, model_cv)\n",
    "        auc_cv = roc_auc_score(y_val_true_cv, y_val_pred_cv)\n",
    "        y_val_bin = (y_val_pred_cv >= 0.5).astype(np.float32)\n",
    "        TP = np.sum((y_val_bin == 1) & (y_val_true_cv == 1))\n",
    "        FN = np.sum((y_val_bin == 0) & (y_val_true_cv == 1))\n",
    "        TN = np.sum((y_val_bin == 0) & (y_val_true_cv == 0))\n",
    "        FP = np.sum((y_val_bin == 1) & (y_val_true_cv == 0))\n",
    "        acc = np.mean(y_val_bin == y_val_true_cv)\n",
    "        sens = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "        spec = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
    "        eps = 1e-8\n",
    "        bce_cv = np.mean(- (y_val_true_cv * np.log(np.clip(y_val_pred_cv, eps, 1 - eps)) + \n",
    "                            (1 - y_val_true_cv) * np.log(np.clip(1 - y_val_pred_cv, eps, 1 - eps))))\n",
    "        cv_fold_metrics.append({\"AUC\": auc_cv, \"accuracy\": acc, \"sensitivity\": sens, \"specificity\": spec, \"bce\": bce_cv})\n",
    "    \n",
    "    mean_metrics = {k: np.mean([fold[k] for fold in cv_fold_metrics]) for k in [\"AUC\", \"accuracy\", \"sensitivity\", \"specificity\", \"bce\"]}\n",
    "    sd_metrics = {k: np.std([fold[k] for fold in cv_fold_metrics]) for k in [\"AUC\", \"accuracy\", \"sensitivity\", \"specificity\", \"bce\"]}\n",
    "    cv_val_df = pd.DataFrame({\n",
    "        \"stat\": [\"mean\", \"sd\"],\n",
    "        \"model\": [\"comb_nw_classification\", \"comb_nw_classification\"],\n",
    "        \"AUC\": [mean_metrics[\"AUC\"], sd_metrics[\"AUC\"]],\n",
    "        \"accuracy\": [mean_metrics[\"accuracy\"], sd_metrics[\"accuracy\"]],\n",
    "        \"sensitivity\": [mean_metrics[\"sensitivity\"], sd_metrics[\"sensitivity\"]],\n",
    "        \"specificity\": [mean_metrics[\"specificity\"], sd_metrics[\"specificity\"]],\n",
    "        \"bce\": [mean_metrics[\"bce\"], sd_metrics[\"bce\"]]\n",
    "    })\n",
    "    \n",
    "    return performance_df, shap_df, cv_val_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36e23a55-591a-4b44-98ec-71a706fbf531",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_fitbit_reg_nw, shap_fitbit_reg_nw, cv_fitbit_reg_nw = run_fitbit_regression_nw_best(fitbit_reg_dict)\n",
    "perf_fitbit_class_nw, shap_fitbit_class_nw, cv_fitbit_class_nw = run_fitbit_classification_nw_best(fitbit_class_dict)\n",
    "perf_comb_reg_nw, shap_comb_reg_nw, cv_comb_reg_nw = run_comb_regression_nw_best(comb_reg_dict)\n",
    "perf_comb_class_nw, shap_comb_class_nw, cv_comb_class_nw = run_comb_classification_nw_best(comb_class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1ffc728-bbfa-4433-aa30-1dd5fdb8e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the SHAP score dataframes separately as TSV files (without index)\n",
    "shap_fitbit_reg_nw.to_csv(\"results/shap_fitbit_reg_nw.tsv\", sep=\"\\t\", index=False)\n",
    "shap_fitbit_class_nw.to_csv(\"results/shap_fitbit_class_nw.tsv\", sep=\"\\t\", index=False)\n",
    "shap_comb_reg_nw.to_csv(\"results/shap_comb_reg_nw.tsv\", sep=\"\\t\", index=False)\n",
    "shap_comb_class_nw.to_csv(\"results/shap_comb_class_nw.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7823d8-af2b-4c93-ba25-43631a254e92",
   "metadata": {},
   "source": [
    "### Save train/test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a65cd242-93ee-482f-a5e9-34d81764f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine the performance dataframes for regression and classification\n",
    "best_perf_reg = pd.concat([perf_fitbit_reg, perf_comb_reg, perf_fitbit_reg_nw, perf_comb_reg_nw])\n",
    "best_perf_class = pd.concat([perf_fitbit_class, perf_comb_class, perf_fitbit_class_nw, perf_comb_class_nw])\n",
    "\n",
    "\n",
    "# Save the combined performance dataframes as TSV files (without index)\n",
    "best_perf_reg.to_csv(\"results/best_perf_reg.tsv\", sep=\"\\t\", index=False)\n",
    "best_perf_class.to_csv(\"results/best_perf_class.tsv\", sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b710be5-7696-42e8-b828-92a7b3d0d3d4",
   "metadata": {},
   "source": [
    "### Save validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b4be9a3-641c-4746-93f4-779fdd548a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the val performance dataframes for regression and classification\n",
    "best_cv_perf_reg = pd.concat([cv_fitbit_reg, cv_comb_reg, cv_fitbit_reg_nw, cv_comb_reg_nw])\n",
    "best_cv_perf_class = pd.concat([cv_fitbit_class, cv_comb_class, cv_fitbit_class_nw, cv_comb_class_nw])\n",
    "\n",
    "# Save the combined val performance dataframes as TSV files (without index)\n",
    "best_cv_perf_reg.to_csv(\"results/best_val_perf_reg.tsv\", sep=\"\\t\", index=False)\n",
    "best_cv_perf_class.to_csv(\"results/best_val_perf_class.tsv\", sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c00e7-e8c3-440a-bb74-0b24610da7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL_py_env)",
   "language": "python",
   "name": "dl_py_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
