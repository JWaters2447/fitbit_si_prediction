{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e50badf-0136-4534-8845-1fc5135b0db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda with 5 GPU(s)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set deterministic behavior for CUDA (set before torch imports)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import optuna\n",
    "\n",
    "# Deterministic seed\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    return seed\n",
    "\n",
    "# Global device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device} with {torch.cuda.device_count()} GPU(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a76020-97db-4390-8213-aa888732b13b",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c5a5bc8-1d52-49cb-ac4e-21b8df6a3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the data directory as needed\n",
    "DL_DIR = \"../../data/deep_learning\"\n",
    "# Load the regression split dictionary.\n",
    "with open(f'{DL_DIR}/comb_reg_dict.pkl', 'rb') as f:\n",
    "    comb_reg_dict = pickle.load(f)\n",
    "\n",
    "with open(f'{DL_DIR}/fitbit_reg_dict.pkl', 'rb') as f:\n",
    "    fitbit_reg_dict = pickle.load(f)\n",
    "\n",
    "# Load the classification split dictionary.\n",
    "with open(f'{DL_DIR}/comb_class_dict.pkl', 'rb') as f:\n",
    "    comb_class_dict = pickle.load(f)\n",
    "\n",
    "with open(f'{DL_DIR}/fitbit_class_dict.pkl', 'rb') as f:\n",
    "    fitbit_class_dict = pickle.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4462f179-6020-4b71-b634-be8f4db06bd0",
   "metadata": {},
   "source": [
    "### Utility: compute output length after conv and pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a061a33f-40d8-43b8-870c-2f63f6114c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_output_length(L_in, kernel_size, stride, padding, dilation):\n",
    "    return (L_in + 2 * padding - dilation * (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "def pool_output_length(L_in, pool_kernel):\n",
    "    # Assume stride==kernel size for pooling\n",
    "    return L_in // pool_kernel\n",
    "\n",
    "# Build subject-level dataset (each row is one subject)\n",
    "def create_subject_dataset(df, outcome_col=\"SI_mean\", use_weights=True):\n",
    "    \"\"\"\n",
    "    Aggregates records for each subject into a subject-level sample.\n",
    "    If use_weights is False, then sample_weight is set to 1.0 for all subjects.\n",
    "    \"\"\"\n",
    "    exclude_cols = [\"PatientID\", \"timepoints\", \"si_kde_weight\", \"SI_mean\", \"is_SI\", \"SI_level\"]\n",
    "    predictor_cols = [col for col in df.columns if col not in (exclude_cols + [outcome_col])]\n",
    "    \n",
    "    subject_data = []\n",
    "    for pid, group in df.groupby(\"PatientID\"):\n",
    "        group_sorted = group.sort_values(\"timepoints\")\n",
    "        X = group_sorted[predictor_cols].values.T  # shape: (n_features, 39)\n",
    "        y = group_sorted[outcome_col].iloc[0]\n",
    "        if use_weights and \"si_kde_weight\" in group_sorted.columns:\n",
    "            weight = group_sorted[\"si_kde_weight\"].iloc[0]\n",
    "        else:\n",
    "            weight = 1.0\n",
    "        record = {\"PatientID\": pid, \"X\": X, outcome_col: y, \"sample_weight\": weight}\n",
    "        if outcome_col == \"is_SI\" and \"SI_mean\" in group_sorted.columns:\n",
    "            record[\"SI_mean\"] = group_sorted[\"SI_mean\"].iloc[0]\n",
    "        subject_data.append(record)\n",
    "    subj_df = pd.DataFrame(subject_data)\n",
    "    subj_df[f\"{outcome_col}_bin\"] = np.round(subj_df[outcome_col]).astype(int)\n",
    "    return subj_df, predictor_cols\n",
    "\n",
    "\n",
    "def save_results_pickle(result_dict, model_name, use_sample_weights=True):\n",
    "    \"\"\"\n",
    "    Saves the result pickle to a folder \"search\". If use_sample_weights is False,\n",
    "    appends '_nw' to the filename.\n",
    "    \"\"\"\n",
    "    save_folder = \"search\"\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "    if use_sample_weights:\n",
    "        filename = os.path.join(save_folder, f\"{model_name}_deep_search_results.pkl\")\n",
    "    else:\n",
    "        filename = os.path.join(save_folder, f\"{model_name}_nw_deep_search_results.pkl\")\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(result_dict, f)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "    \n",
    "def get_stratified_cv_splits(df, subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5):\n",
    "    \"\"\"\n",
    "    Performs stratified K-fold cross validation at the subject level.\n",
    "    \n",
    "    Parameters:\n",
    "      df : pandas.DataFrame\n",
    "          The original dataframe containing repeated measures.\n",
    "      subject_id : str\n",
    "          The column name for the subject ID (e.g., \"PatientID\").\n",
    "      target_var : str\n",
    "          The target variable; for regression use \"SI_mean\" and for classification use \"is_SI\".\n",
    "      n_splits : int\n",
    "          Number of folds for cross validation.\n",
    "    \n",
    "    Returns:\n",
    "      splits : list of tuples\n",
    "          A list where each element is a tuple (train_df, test_df) corresponding\n",
    "          to one fold. Each dataframe contains all rows (i.e. repeated measures) for the patients in that fold.\n",
    "    \n",
    "    Behavior:\n",
    "      - Isolates unique patient IDs and their target variable by dropping duplicates.\n",
    "      - If target_var is \"SI_mean\", creates a new column \"SI_mean_levels\" (rounded SI_mean).\n",
    "      - Uses the resulting column as the stratification column.\n",
    "      - Performs stratified K-fold CV and then subsets the original dataframe based on the patient IDs.\n",
    "    \"\"\"\n",
    "    # Create a subject-level dataframe (unique patient IDs with their target variable)\n",
    "    subject_df = df[[subject_id, target_var]].drop_duplicates(subset=[subject_id]).copy()\n",
    "    \n",
    "    # For regression: create a new column with the rounded SI_mean values.\n",
    "    if target_var == \"SI_mean\":\n",
    "        subject_df[\"SI_mean_levels\"] = subject_df[target_var].round().astype(int)\n",
    "        strat_col = \"SI_mean_levels\"\n",
    "    else:\n",
    "        strat_col = target_var  # For classification, use the target directly.\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    splits = []\n",
    "    \n",
    "    # Get the subject IDs and stratification labels\n",
    "    subjects = subject_df[subject_id].values\n",
    "    strat_labels = subject_df[strat_col].values\n",
    "    \n",
    "    # For each fold, retrieve patient IDs and then subset the original dataframe.\n",
    "    for train_idx, test_idx in skf.split(subjects, strat_labels):\n",
    "        train_patient_ids = subject_df.iloc[train_idx][subject_id].values\n",
    "        test_patient_ids  = subject_df.iloc[test_idx][subject_id].values\n",
    "        train_split = df[df[subject_id].isin(train_patient_ids)]\n",
    "        test_split  = df[df[subject_id].isin(test_patient_ids)]\n",
    "        splits.append((train_split, test_split))\n",
    "    \n",
    "    return splits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f0a97a-9c61-4052-8c49-4371f6ffcea8",
   "metadata": {},
   "source": [
    "###  Deep Search Objective Functions for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9194e5-2c31-4665-ab59-4eb34d47611a",
   "metadata": {},
   "source": [
    "Comb Deep Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1026e960-a60c-4b0f-854b-4d5cea560a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Updated objective_regression_comb using LSTM\n",
    "def objective_regression_comb_nw(trial, data_dict):\n",
    "    \"\"\"\n",
    "    Objective function for comb_reg using LSTM on non-weighted data.\n",
    "    Searched hyperparameters: lr, dropout.\n",
    "    Fixed defaults: num_epochs=10, hidden_size=64, batch_size=32.\n",
    "    \"\"\"\n",
    "    set_seed(42)\n",
    "    splits = get_stratified_cv_splits(\n",
    "        data_dict['train'], subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5\n",
    "    )\n",
    "    overall_rmse_list = []\n",
    "    bin_rmse_dict = {str(b): [] for b in range(1, 6)}\n",
    "    \n",
    "    # Hyperparameter search\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    dropout_val = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    # Fixed parameters (assumed optimal)\n",
    "    num_epochs = 10\n",
    "    hidden_size = 64\n",
    "    batch_size = 32\n",
    "\n",
    "    for train_split, val_split in splits:\n",
    "        # Force non-weighted by setting use_weights=False\n",
    "        train_df, _ = create_subject_dataset(train_split, outcome_col=\"SI_mean\", use_weights=False)\n",
    "        val_df, _   = create_subject_dataset(val_split, outcome_col=\"SI_mean\", use_weights=False)\n",
    "        \n",
    "        X_train = np.stack(train_df[\"X\"].values, axis=0)  # shape: (n_subjects, n_features, seq_len)\n",
    "        y_train = train_df[\"SI_mean\"].values\n",
    "        w_train = train_df[\"sample_weight\"].values  # all 1.0\n",
    "        \n",
    "        X_val = np.stack(val_df[\"X\"].values, axis=0)\n",
    "        y_val = val_df[\"SI_mean\"].values\n",
    "        \n",
    "        # Transpose to (n_subjects, seq_len, n_features)\n",
    "        X_train = np.transpose(X_train, (0, 2, 1))\n",
    "        X_val   = np.transpose(X_val, (0, 2, 1))\n",
    "        n_subjects, seq_len, input_size = X_train.shape\n",
    "\n",
    "        # Define LSTM model for regression\n",
    "        class CombLSTM(nn.Module):\n",
    "            def __init__(self, input_size, hidden_size, dropout):\n",
    "                super(CombLSTM, self).__init__()\n",
    "                self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "                self.dropout = nn.Dropout(dropout)\n",
    "                self.fc = nn.Linear(hidden_size, 1)\n",
    "            def forward(self, x):\n",
    "                lstm_out, (h_n, _) = self.lstm(x)\n",
    "                h_last = h_n[-1]\n",
    "                out = self.dropout(h_last)\n",
    "                out = self.fc(out)\n",
    "                return out\n",
    "\n",
    "        model = CombLSTM(input_size, hidden_size, dropout_val).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "        \n",
    "        train_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_train, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(w_train, dtype=torch.float32)\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for X_batch, y_batch, weight_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                weight_batch = weight_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = (loss_fn(outputs, y_batch).view(-1) * weight_batch).mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "            preds = model(X_val_tensor).cpu().numpy()\n",
    "        fold_rmse = np.sqrt(np.mean((preds - y_val.reshape(-1, 1)) ** 2))\n",
    "        overall_rmse_list.append(fold_rmse)\n",
    "        \n",
    "        # Compute per-bin RMSE.\n",
    "        for b in range(1, 6):\n",
    "            idx = np.where(val_df[\"SI_mean_bin\"].values == b)[0]\n",
    "            if len(idx) > 0:\n",
    "                bin_rmse = np.sqrt(np.mean((preds[idx] - y_val[idx].reshape(-1, 1)) ** 2))\n",
    "                bin_rmse_dict[str(b)].append(bin_rmse)\n",
    "                \n",
    "    overall_mean_rmse = np.mean(overall_rmse_list)\n",
    "    overall_std_rmse = np.std(overall_rmse_list)\n",
    "    bin_mean_rmse = {str(b): np.mean(bin_rmse_dict[str(b)]) if len(bin_rmse_dict[str(b)]) > 0 else float('nan')\n",
    "                     for b in range(1, 6)}\n",
    "    bin_std_rmse = {str(b): np.std(bin_rmse_dict[str(b)]) if len(bin_rmse_dict[str(b)]) > 0 else float('nan')\n",
    "                    for b in range(1, 6)}\n",
    "    \n",
    "    trial.set_user_attr(\"overall_mean_rmse\", overall_mean_rmse)\n",
    "    trial.set_user_attr(\"overall_std_rmse\", overall_std_rmse)\n",
    "    trial.set_user_attr(\"bin_mean_rmse\", bin_mean_rmse)\n",
    "    trial.set_user_attr(\"bin_std_rmse\", bin_std_rmse)\n",
    "    \n",
    "    return overall_mean_rmse\n",
    "\n",
    "def objective_regression_comb(trial, data_dict):\n",
    "    \"\"\"\n",
    "    Objective function for comb_reg using LSTM.\n",
    "    Tunable hyperparameters:\n",
    "      - hidden_size\n",
    "      - lr\n",
    "      - num_epochs\n",
    "      - dropout\n",
    "    \"\"\"\n",
    "    set_seed(42)\n",
    "    splits = get_stratified_cv_splits(data_dict['train'], subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5)\n",
    "    overall_rmse_list = []\n",
    "    bin_rmse_dict = {str(b): [] for b in range(1, 6)}\n",
    "    \n",
    "    # Tunable hyperparameters\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    dropout_val = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 5, 10)\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 16, 128, step=16)\n",
    "    batch_size = 32  # fixed\n",
    "\n",
    "    for train_split, val_split in splits:\n",
    "        # Build subject-level datasets for training and validation\n",
    "        train_df, _ = create_subject_dataset(train_split, outcome_col=\"SI_mean\")\n",
    "        val_df, _ = create_subject_dataset(val_split, outcome_col=\"SI_mean\")\n",
    "        \n",
    "        X_train = np.stack(train_df[\"X\"].values, axis=0)  # (n_subjects, n_features, seq_len)\n",
    "        y_train = train_df[\"SI_mean\"].values\n",
    "        w_train = train_df[\"sample_weight\"].values\n",
    "        \n",
    "        X_val = np.stack(val_df[\"X\"].values, axis=0)\n",
    "        y_val = val_df[\"SI_mean\"].values\n",
    "        val_bins = val_df[\"SI_mean_bin\"].values\n",
    "        \n",
    "        # Transpose to (n_subjects, seq_len, n_features) for LSTM\n",
    "        X_train = np.transpose(X_train, (0, 2, 1))\n",
    "        X_val = np.transpose(X_val, (0, 2, 1))\n",
    "        n_subjects, seq_len, input_size = X_train.shape\n",
    "\n",
    "        # Define LSTM model for regression\n",
    "        class CombLSTM(nn.Module):\n",
    "            def __init__(self, input_size, hidden_size, dropout):\n",
    "                super(CombLSTM, self).__init__()\n",
    "                self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "                self.dropout = nn.Dropout(dropout)\n",
    "                self.fc = nn.Linear(hidden_size, 1)\n",
    "            def forward(self, x):\n",
    "                lstm_out, (h_n, _) = self.lstm(x)\n",
    "                h_last = h_n[-1]  # take the last hidden state\n",
    "                out = self.dropout(h_last)\n",
    "                out = self.fc(out)\n",
    "                return out\n",
    "\n",
    "        model = CombLSTM(input_size, hidden_size, dropout_val).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "        \n",
    "        train_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_train, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(w_train, dtype=torch.float32)\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for X_batch, y_batch, weight_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                weight_batch = weight_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = (loss_fn(outputs, y_batch).view(-1) * weight_batch).mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "            preds = model(X_val_tensor).cpu().numpy()\n",
    "        fold_rmse = np.sqrt(np.mean((preds - y_val.reshape(-1, 1)) ** 2))\n",
    "        overall_rmse_list.append(fold_rmse)\n",
    "        \n",
    "        # Compute per-bin RMSE.\n",
    "        for b in range(1, 6):\n",
    "            idx = np.where(val_bins == b)[0]\n",
    "            if len(idx) > 0:\n",
    "                bin_rmse = np.sqrt(np.mean((preds[idx] - y_val[idx].reshape(-1, 1)) ** 2))\n",
    "                bin_rmse_dict[str(b)].append(bin_rmse)\n",
    "                \n",
    "    overall_mean_rmse = np.mean(overall_rmse_list)\n",
    "    overall_std_rmse = np.std(overall_rmse_list)\n",
    "    bin_mean_rmse = {str(b): np.mean(bin_rmse_dict[str(b)]) if len(bin_rmse_dict[str(b)]) > 0 else float('nan')\n",
    "                     for b in range(1, 6)}\n",
    "    bin_std_rmse = {str(b): np.std(bin_rmse_dict[str(b)]) if len(bin_rmse_dict[str(b)]) > 0 else float('nan')\n",
    "                    for b in range(1, 6)}\n",
    "    \n",
    "    trial.set_user_attr(\"overall_mean_rmse\", overall_mean_rmse)\n",
    "    trial.set_user_attr(\"overall_std_rmse\", overall_std_rmse)\n",
    "    trial.set_user_attr(\"bin_mean_rmse\", bin_mean_rmse)\n",
    "    trial.set_user_attr(\"bin_std_rmse\", bin_std_rmse)\n",
    "    \n",
    "    return overall_mean_rmse\n",
    "\n",
    "\n",
    "\n",
    "def run_deep_search_comb_reg(data_dict, use_sample_weights, model_name, n_trials=75):\n",
    "    \"\"\"\n",
    "    Run deep search for comb_reg.\n",
    "    Chooses the weighted or non-weighted objective based on use_sample_weights.\n",
    "    \"\"\"\n",
    "    data_dict[\"use_sample_weights\"] = use_sample_weights\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "    def print_progress(study, trial):\n",
    "        print(f\"{model_name} trial {len(study.trials)}/{n_trials}\\r\", end=\"\", flush=True)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    # Select the appropriate objective function:\n",
    "    optimize_fn = objective_regression_comb if use_sample_weights else objective_regression_comb_nw\n",
    "\n",
    "    study.optimize(lambda trial: optimize_fn(trial, data_dict),\n",
    "                   n_trials=n_trials, callbacks=[print_progress])\n",
    "\n",
    "    mean_rows = []\n",
    "    std_rows = []\n",
    "    for t in study.trials:\n",
    "        row_mean = {\n",
    "            \"model\": model_name,\n",
    "            \"type\": \"weighted\" if use_sample_weights else \"not weighted\",\n",
    "            \"1\": t.user_attrs[\"bin_mean_rmse\"].get(\"1\", float('nan')),\n",
    "            \"2\": t.user_attrs[\"bin_mean_rmse\"].get(\"2\", float('nan')),\n",
    "            \"3\": t.user_attrs[\"bin_mean_rmse\"].get(\"3\", float('nan')),\n",
    "            \"4\": t.user_attrs[\"bin_mean_rmse\"].get(\"4\", float('nan')),\n",
    "            \"5\": t.user_attrs[\"bin_mean_rmse\"].get(\"5\", float('nan')),\n",
    "            \"overall\": t.user_attrs[\"overall_mean_rmse\"],\n",
    "            \"config\": t.params\n",
    "        }\n",
    "        row_std = {\n",
    "            \"model\": model_name,\n",
    "            \"type\": \"weighted\" if use_sample_weights else \"not weighted\",\n",
    "            \"1\": t.user_attrs[\"bin_std_rmse\"].get(\"1\", float('nan')),\n",
    "            \"2\": t.user_attrs[\"bin_std_rmse\"].get(\"2\", float('nan')),\n",
    "            \"3\": t.user_attrs[\"bin_std_rmse\"].get(\"3\", float('nan')),\n",
    "            \"4\": t.user_attrs[\"bin_std_rmse\"].get(\"4\", float('nan')),\n",
    "            \"5\": t.user_attrs[\"bin_std_rmse\"].get(\"5\", float('nan')),\n",
    "            \"overall\": t.user_attrs[\"overall_std_rmse\"],\n",
    "            \"config\": t.params\n",
    "        }\n",
    "        mean_rows.append(row_mean)\n",
    "        std_rows.append(row_std)\n",
    "\n",
    "    columns = [\"model\", \"type\", \"1\", \"2\", \"3\", \"4\", \"5\", \"overall\", \"config\"]\n",
    "    mean_df = pd.DataFrame(mean_rows, columns=columns)\n",
    "    std_df = pd.DataFrame(std_rows, columns=columns)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    optimal_configuration = {\n",
    "        \"value\": best_trial.value,\n",
    "        \"params\": best_trial.params,\n",
    "        \"user_attrs\": best_trial.user_attrs\n",
    "    }\n",
    "\n",
    "    result_dict = {\n",
    "        \"mean_rmse\": mean_df,\n",
    "        \"std_rmse\": std_df,\n",
    "        \"optimal_configuration\": optimal_configuration\n",
    "    }\n",
    "    save_results_pickle(result_dict, model_name, use_sample_weights)\n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993d08fc-a042-4d58-89ec-adb4563833d8",
   "metadata": {},
   "source": [
    "Fitbit Deep Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9647b83-09b2-41e7-8027-bd5a93d2484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Updated objective_regression_fitbit using LSTM\n",
    "def objective_regression_fitbit_nw(trial, data_dict):\n",
    "    \"\"\"\n",
    "    Objective function for fitbit_reg using LSTM on non-weighted data.\n",
    "    Searched hyperparameters: lr, n_fc_layers.\n",
    "    Fixed parameters: dropout=0.3, use_regularization=False, bidirectional=False,\n",
    "                      num_epochs=10, batch_size=32, hidden_size=32.\n",
    "    \"\"\"\n",
    "    set_seed(42)\n",
    "    splits = get_stratified_cv_splits(\n",
    "        data_dict['train'], subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5\n",
    "    )\n",
    "    overall_rmse_list = []\n",
    "    bin_rmse_dict = {str(b): [] for b in range(1, 6)}\n",
    "\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    n_fc_layers = trial.suggest_int(\"n_fc_layers\", 1, 3)\n",
    "    \n",
    "    # Fixed parameters\n",
    "    dropout_val = 0.3\n",
    "    use_regularization = False\n",
    "    bidirectional = False\n",
    "    num_epochs = 10\n",
    "    batch_size = 32\n",
    "    hidden_size = 32\n",
    "\n",
    "    for train_split, val_split in splits:\n",
    "        # Force non-weighted\n",
    "        train_df, _ = create_subject_dataset(train_split, outcome_col=\"SI_mean\", use_weights=False)\n",
    "        val_df, _   = create_subject_dataset(val_split, outcome_col=\"SI_mean\", use_weights=False)\n",
    "        \n",
    "        X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "        y_train = train_df[\"SI_mean\"].values\n",
    "        w_train = train_df[\"sample_weight\"].values  # should be all 1.0\n",
    "        \n",
    "        X_val = np.stack(val_df[\"X\"].values, axis=0)\n",
    "        y_val = val_df[\"SI_mean\"].values\n",
    "        \n",
    "        X_train = np.transpose(X_train, (0, 2, 1))\n",
    "        X_val   = np.transpose(X_val, (0, 2, 1))\n",
    "        n_subjects, seq_len, input_size = X_train.shape\n",
    "\n",
    "        class FitbitRegLSTM(nn.Module):\n",
    "            def __init__(self, input_size, hidden_size, dropout, bidirectional, n_fc_layers):\n",
    "                super(FitbitRegLSTM, self).__init__()\n",
    "                self.bidirectional = bidirectional\n",
    "                self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                                    batch_first=True, bidirectional=bidirectional)\n",
    "                fc_in_dim = hidden_size * (2 if bidirectional else 1)\n",
    "                fc_layers = []\n",
    "                for i in range(n_fc_layers - 1):\n",
    "                    fc_layers.append(nn.Linear(fc_in_dim, fc_in_dim))\n",
    "                    fc_layers.append(nn.ReLU())\n",
    "                fc_layers.append(nn.Linear(fc_in_dim, 1))\n",
    "                self.fc = nn.Sequential(*fc_layers)\n",
    "                self.dropout = nn.Dropout(dropout)\n",
    "            def forward(self, x):\n",
    "                lstm_out, (h_n, _) = self.lstm(x)\n",
    "                if self.bidirectional:\n",
    "                    h_last = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "                else:\n",
    "                    h_last = h_n[-1]\n",
    "                out = self.dropout(h_last)\n",
    "                out = self.fc(out)\n",
    "                return out\n",
    "\n",
    "        model = FitbitRegLSTM(input_size, hidden_size, dropout_val, bidirectional, n_fc_layers).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4 if use_regularization else 0.0)\n",
    "        loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "        \n",
    "        train_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_train, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(w_train, dtype=torch.float32)\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for X_batch, y_batch, weight_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                weight_batch = weight_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = (loss_fn(outputs, y_batch).view(-1) * weight_batch).mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "            preds = model(X_val_tensor).cpu().numpy()\n",
    "        fold_rmse = np.sqrt(np.mean((preds - y_val.reshape(-1, 1)) ** 2))\n",
    "        overall_rmse_list.append(fold_rmse)\n",
    "        \n",
    "        for b in range(1, 6):\n",
    "            idx = np.where(val_df[\"SI_mean_bin\"].values == b)[0]\n",
    "            if len(idx) > 0:\n",
    "                bin_rmse = np.sqrt(np.mean((preds[idx] - y_val[idx].reshape(-1, 1)) ** 2))\n",
    "                bin_rmse_dict[str(b)].append(bin_rmse)\n",
    "    \n",
    "    overall_mean_rmse = np.mean(overall_rmse_list)\n",
    "    overall_std_rmse = np.std(overall_rmse_list)\n",
    "    bin_mean_rmse = {str(b): np.mean(bin_rmse_dict[str(b)]) if len(bin_rmse_dict[str(b)]) > 0 else float('nan')\n",
    "                     for b in range(1, 6)}\n",
    "    bin_std_rmse = {str(b): np.std(bin_rmse_dict[str(b)]) if len(bin_rmse_dict[str(b)]) > 0 else float('nan')\n",
    "                    for b in range(1, 6)}\n",
    "    \n",
    "    trial.set_user_attr(\"overall_mean_rmse\", overall_mean_rmse)\n",
    "    trial.set_user_attr(\"overall_std_rmse\", overall_std_rmse)\n",
    "    trial.set_user_attr(\"bin_mean_rmse\", bin_mean_rmse)\n",
    "    trial.set_user_attr(\"bin_std_rmse\", bin_std_rmse)\n",
    "    \n",
    "    return overall_mean_rmse\n",
    "\n",
    "def objective_regression_fitbit(trial, data_dict):\n",
    "    \"\"\"\n",
    "    Objective function for fitbit_reg using LSTM.\n",
    "    Tunable hyperparameters:\n",
    "      - dropout\n",
    "      - lr\n",
    "      - use_regularization\n",
    "      - bidirectional\n",
    "      - num_epochs\n",
    "      - n_fc_layers\n",
    "      - batch_size\n",
    "    \"\"\"\n",
    "    set_seed(42)\n",
    "    splits = get_stratified_cv_splits(data_dict['train'], subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5)\n",
    "    overall_rmse_list = []\n",
    "    bin_rmse_dict = {str(b): [] for b in range(1, 6)}\n",
    "\n",
    "    # Tunable hyperparameters\n",
    "    dropout_val = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    use_regularization = trial.suggest_categorical(\"use_regularization\", [True, False])\n",
    "    bidirectional = trial.suggest_categorical(\"bidirectional\", [True, False])\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 5, 10)\n",
    "    n_fc_layers = trial.suggest_int(\"n_fc_layers\", 1, 3)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 16, 64, step=16)\n",
    "    \n",
    "    # Set a default hidden_size (could be set to a fixed value)\n",
    "    hidden_size = 32\n",
    "\n",
    "    for train_split, val_split in splits:\n",
    "        train_df, _ = create_subject_dataset(train_split, outcome_col=\"SI_mean\")\n",
    "        val_df, _ = create_subject_dataset(val_split, outcome_col=\"SI_mean\")\n",
    "        \n",
    "        X_train = np.stack(train_df[\"X\"].values, axis=0)  # (n_subjects, n_features, seq_len)\n",
    "        y_train = train_df[\"SI_mean\"].values\n",
    "        w_train = train_df[\"sample_weight\"].values\n",
    "        \n",
    "        X_val = np.stack(val_df[\"X\"].values, axis=0)\n",
    "        y_val = val_df[\"SI_mean\"].values\n",
    "        val_bins = val_df[\"SI_mean_bin\"].values\n",
    "        \n",
    "        # Transpose to (n_subjects, seq_len, n_features)\n",
    "        X_train = np.transpose(X_train, (0, 2, 1))\n",
    "        X_val = np.transpose(X_val, (0, 2, 1))\n",
    "        n_subjects, seq_len, input_size = X_train.shape\n",
    "\n",
    "        # Define LSTM model for fitbit regression\n",
    "        class FitbitRegLSTM(nn.Module):\n",
    "            def __init__(self, input_size, hidden_size, dropout, bidirectional, n_fc_layers):\n",
    "                super(FitbitRegLSTM, self).__init__()\n",
    "                self.bidirectional = bidirectional\n",
    "                self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                                    batch_first=True, bidirectional=bidirectional)\n",
    "                fc_in_dim = hidden_size * (2 if bidirectional else 1)\n",
    "                fc_layers = []\n",
    "                # Build a stack of fully-connected layers\n",
    "                for i in range(n_fc_layers - 1):\n",
    "                    fc_layers.append(nn.Linear(fc_in_dim, fc_in_dim))\n",
    "                    fc_layers.append(nn.ReLU())\n",
    "                fc_layers.append(nn.Linear(fc_in_dim, 1))\n",
    "                self.fc = nn.Sequential(*fc_layers)\n",
    "                self.dropout = nn.Dropout(dropout)\n",
    "            def forward(self, x):\n",
    "                lstm_out, (h_n, _) = self.lstm(x)\n",
    "                if self.bidirectional:\n",
    "                    h_last = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "                else:\n",
    "                    h_last = h_n[-1]\n",
    "                out = self.dropout(h_last)\n",
    "                out = self.fc(out)\n",
    "                return out\n",
    "\n",
    "        model = FitbitRegLSTM(input_size, hidden_size, dropout_val, bidirectional, n_fc_layers).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        \n",
    "        weight_decay = 1e-4 if use_regularization else 0.0\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "        \n",
    "        train_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_train, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(w_train, dtype=torch.float32)\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for X_batch, y_batch, weight_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                weight_batch = weight_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = (loss_fn(outputs, y_batch).view(-1) * weight_batch).mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "            preds = model(X_val_tensor).cpu().numpy()\n",
    "        fold_rmse = np.sqrt(np.mean((preds - y_val.reshape(-1, 1)) ** 2))\n",
    "        overall_rmse_list.append(fold_rmse)\n",
    "        \n",
    "        for b in range(1, 6):\n",
    "            idx = np.where(val_bins == b)[0]\n",
    "            if len(idx) > 0:\n",
    "                bin_rmse = np.sqrt(np.mean((preds[idx] - y_val[idx].reshape(-1, 1)) ** 2))\n",
    "                bin_rmse_dict[str(b)].append(bin_rmse)\n",
    "    \n",
    "    overall_mean_rmse = np.mean(overall_rmse_list)\n",
    "    overall_std_rmse = np.std(overall_rmse_list)\n",
    "    bin_mean_rmse = {str(b): np.mean(bin_rmse_dict[str(b)]) if len(bin_rmse_dict[str(b)]) > 0 else float('nan')\n",
    "                     for b in range(1, 6)}\n",
    "    bin_std_rmse = {str(b): np.std(bin_rmse_dict[str(b)]) if len(bin_rmse_dict[str(b)]) > 0 else float('nan')\n",
    "                    for b in range(1, 6)}\n",
    "    \n",
    "    trial.set_user_attr(\"overall_mean_rmse\", overall_mean_rmse)\n",
    "    trial.set_user_attr(\"overall_std_rmse\", overall_std_rmse)\n",
    "    trial.set_user_attr(\"bin_mean_rmse\", bin_mean_rmse)\n",
    "    trial.set_user_attr(\"bin_std_rmse\", bin_std_rmse)\n",
    "    \n",
    "    return overall_mean_rmse\n",
    "\n",
    "def run_deep_search_fitbit_reg(data_dict, use_sample_weights, model_name, n_trials=75):\n",
    "    \"\"\"\n",
    "    Run deep search for fitbit_reg.\n",
    "    Chooses weighted vs. non-weighted objective based on use_sample_weights.\n",
    "    \"\"\"\n",
    "    data_dict[\"use_sample_weights\"] = use_sample_weights\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "    def print_progress(study, trial):\n",
    "        print(f\"{model_name} trial {len(study.trials)}/{n_trials}\\r\", end=\"\", flush=True)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    optimize_fn = objective_regression_fitbit if use_sample_weights else objective_regression_fitbit_nw\n",
    "\n",
    "    study.optimize(lambda trial: optimize_fn(trial, data_dict),\n",
    "                   n_trials=n_trials, callbacks=[print_progress])\n",
    "\n",
    "    mean_rows = []\n",
    "    std_rows = []\n",
    "    for t in study.trials:\n",
    "        row_mean = {\n",
    "            \"model\": model_name,\n",
    "            \"type\": \"weighted\" if use_sample_weights else \"not weighted\",\n",
    "            \"1\": t.user_attrs[\"bin_mean_rmse\"].get(\"1\", float('nan')),\n",
    "            \"2\": t.user_attrs[\"bin_mean_rmse\"].get(\"2\", float('nan')),\n",
    "            \"3\": t.user_attrs[\"bin_mean_rmse\"].get(\"3\", float('nan')),\n",
    "            \"4\": t.user_attrs[\"bin_mean_rmse\"].get(\"4\", float('nan')),\n",
    "            \"5\": t.user_attrs[\"bin_mean_rmse\"].get(\"5\", float('nan')),\n",
    "            \"overall\": t.user_attrs[\"overall_mean_rmse\"],\n",
    "            \"config\": t.params\n",
    "        }\n",
    "        row_std = {\n",
    "            \"model\": model_name,\n",
    "            \"type\": \"weighted\" if use_sample_weights else \"not weighted\",\n",
    "            \"1\": t.user_attrs[\"bin_std_rmse\"].get(\"1\", float('nan')),\n",
    "            \"2\": t.user_attrs[\"bin_std_rmse\"].get(\"2\", float('nan')),\n",
    "            \"3\": t.user_attrs[\"bin_std_rmse\"].get(\"3\", float('nan')),\n",
    "            \"4\": t.user_attrs[\"bin_std_rmse\"].get(\"4\", float('nan')),\n",
    "            \"5\": t.user_attrs[\"bin_std_rmse\"].get(\"5\", float('nan')),\n",
    "            \"overall\": t.user_attrs[\"overall_std_rmse\"],\n",
    "            \"config\": t.params\n",
    "        }\n",
    "        mean_rows.append(row_mean)\n",
    "        std_rows.append(row_std)\n",
    "\n",
    "    columns = [\"model\", \"type\", \"1\", \"2\", \"3\", \"4\", \"5\", \"overall\", \"config\"]\n",
    "    mean_df = pd.DataFrame(mean_rows, columns=columns)\n",
    "    std_df = pd.DataFrame(std_rows, columns=columns)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    optimal_configuration = {\n",
    "        \"value\": best_trial.value,\n",
    "        \"params\": best_trial.params,\n",
    "        \"user_attrs\": best_trial.user_attrs\n",
    "    }\n",
    "\n",
    "    result_dict = {\n",
    "        \"mean_rmse\": mean_df,\n",
    "        \"std_rmse\": std_df,\n",
    "        \"optimal_configuration\": optimal_configuration\n",
    "    }\n",
    "    save_results_pickle(result_dict, model_name, use_sample_weights)\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9b9bbe-47d8-4441-a57f-462b123416af",
   "metadata": {},
   "source": [
    "### Run deep search for regression with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6217c2cd-e8e7-499e-8191-3a1fcfc04cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to search/comb_reg_deep_deep_search_results.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>type</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>overall</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.974058</td>\n",
       "      <td>1.867155</td>\n",
       "      <td>2.886802</td>\n",
       "      <td>3.848536</td>\n",
       "      <td>4.840912</td>\n",
       "      <td>1.344001</td>\n",
       "      <td>{'lr': 1.086808570211826e-05, 'dropout': 0.307...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>1.587175</td>\n",
       "      <td>0.996586</td>\n",
       "      <td>0.706037</td>\n",
       "      <td>1.324154</td>\n",
       "      <td>2.083702</td>\n",
       "      <td>1.501365</td>\n",
       "      <td>{'lr': 0.008965726470407422, 'dropout': 0.4961...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.755983</td>\n",
       "      <td>0.913275</td>\n",
       "      <td>1.819670</td>\n",
       "      <td>2.243301</td>\n",
       "      <td>3.014674</td>\n",
       "      <td>0.893202</td>\n",
       "      <td>{'lr': 6.950816077040568e-05, 'dropout': 0.117...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>1.838039</td>\n",
       "      <td>1.134017</td>\n",
       "      <td>0.573269</td>\n",
       "      <td>0.959681</td>\n",
       "      <td>1.679585</td>\n",
       "      <td>1.722771</td>\n",
       "      <td>{'lr': 0.0007935277671176808, 'dropout': 0.335...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>1.837981</td>\n",
       "      <td>1.205668</td>\n",
       "      <td>0.708730</td>\n",
       "      <td>1.109147</td>\n",
       "      <td>1.843238</td>\n",
       "      <td>1.737177</td>\n",
       "      <td>{'lr': 0.00596797524338459, 'dropout': 0.13007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>1.672951</td>\n",
       "      <td>1.034426</td>\n",
       "      <td>0.691218</td>\n",
       "      <td>1.195380</td>\n",
       "      <td>1.943159</td>\n",
       "      <td>1.575959</td>\n",
       "      <td>{'lr': 0.0015556261807786443, 'dropout': 0.267...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.521678</td>\n",
       "      <td>1.114152</td>\n",
       "      <td>2.182451</td>\n",
       "      <td>2.789847</td>\n",
       "      <td>3.636688</td>\n",
       "      <td>0.839580</td>\n",
       "      <td>{'lr': 6.291552319554909e-05, 'dropout': 0.328...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.519888</td>\n",
       "      <td>1.220057</td>\n",
       "      <td>2.304465</td>\n",
       "      <td>2.952493</td>\n",
       "      <td>3.807083</td>\n",
       "      <td>0.877323</td>\n",
       "      <td>{'lr': 5.87281898959135e-05, 'dropout': 0.3183...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.981837</td>\n",
       "      <td>0.797988</td>\n",
       "      <td>1.522853</td>\n",
       "      <td>1.876069</td>\n",
       "      <td>2.645501</td>\n",
       "      <td>1.016205</td>\n",
       "      <td>{'lr': 8.117895348975394e-05, 'dropout': 0.297...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.669155</td>\n",
       "      <td>1.516332</td>\n",
       "      <td>2.580216</td>\n",
       "      <td>3.353668</td>\n",
       "      <td>4.161526</td>\n",
       "      <td>1.054628</td>\n",
       "      <td>{'lr': 3.359794033515547e-05, 'dropout': 0.347...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            model      type         1         2         3         4         5  \\\n",
       "0   comb_reg_deep  weighted  0.974058  1.867155  2.886802  3.848536  4.840912   \n",
       "1   comb_reg_deep  weighted  1.587175  0.996586  0.706037  1.324154  2.083702   \n",
       "2   comb_reg_deep  weighted  0.755983  0.913275  1.819670  2.243301  3.014674   \n",
       "3   comb_reg_deep  weighted  1.838039  1.134017  0.573269  0.959681  1.679585   \n",
       "4   comb_reg_deep  weighted  1.837981  1.205668  0.708730  1.109147  1.843238   \n",
       "..            ...       ...       ...       ...       ...       ...       ...   \n",
       "70  comb_reg_deep  weighted  1.672951  1.034426  0.691218  1.195380  1.943159   \n",
       "71  comb_reg_deep  weighted  0.521678  1.114152  2.182451  2.789847  3.636688   \n",
       "72  comb_reg_deep  weighted  0.519888  1.220057  2.304465  2.952493  3.807083   \n",
       "73  comb_reg_deep  weighted  0.981837  0.797988  1.522853  1.876069  2.645501   \n",
       "74  comb_reg_deep  weighted  0.669155  1.516332  2.580216  3.353668  4.161526   \n",
       "\n",
       "     overall                                             config  \n",
       "0   1.344001  {'lr': 1.086808570211826e-05, 'dropout': 0.307...  \n",
       "1   1.501365  {'lr': 0.008965726470407422, 'dropout': 0.4961...  \n",
       "2   0.893202  {'lr': 6.950816077040568e-05, 'dropout': 0.117...  \n",
       "3   1.722771  {'lr': 0.0007935277671176808, 'dropout': 0.335...  \n",
       "4   1.737177  {'lr': 0.00596797524338459, 'dropout': 0.13007...  \n",
       "..       ...                                                ...  \n",
       "70  1.575959  {'lr': 0.0015556261807786443, 'dropout': 0.267...  \n",
       "71  0.839580  {'lr': 6.291552319554909e-05, 'dropout': 0.328...  \n",
       "72  0.877323  {'lr': 5.87281898959135e-05, 'dropout': 0.3183...  \n",
       "73  1.016205  {'lr': 8.117895348975394e-05, 'dropout': 0.297...  \n",
       "74  1.054628  {'lr': 3.359794033515547e-05, 'dropout': 0.347...  \n",
       "\n",
       "[75 rows x 9 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run deep search for COMB dataset (n_trials determines how many steps/trials are executed)\n",
    "results_comb_reg = run_deep_search_comb_reg(comb_reg_dict, use_sample_weights=True, model_name=\"comb_reg_deep\", n_trials=75)\n",
    "results_comb_reg[\"mean_rmse\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c957cf7-51cf-405f-bf70-d290d2d020c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to search/fitbit_reg_deep_deep_search_results.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>type</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>overall</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>1.886885</td>\n",
       "      <td>1.180709</td>\n",
       "      <td>0.673292</td>\n",
       "      <td>1.038544</td>\n",
       "      <td>1.826427</td>\n",
       "      <td>1.772961</td>\n",
       "      <td>{'lr': 0.009716870066016842, 'dropout': 0.4663...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.950231</td>\n",
       "      <td>1.832606</td>\n",
       "      <td>2.877450</td>\n",
       "      <td>3.807033</td>\n",
       "      <td>4.802780</td>\n",
       "      <td>1.320486</td>\n",
       "      <td>{'lr': 1.1799543502498368e-05, 'dropout': 0.16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.799951</td>\n",
       "      <td>1.677605</td>\n",
       "      <td>2.741591</td>\n",
       "      <td>3.632390</td>\n",
       "      <td>4.671007</td>\n",
       "      <td>1.186541</td>\n",
       "      <td>{'lr': 4.674540872903447e-05, 'dropout': 0.141...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.933308</td>\n",
       "      <td>1.817364</td>\n",
       "      <td>2.871818</td>\n",
       "      <td>3.748563</td>\n",
       "      <td>4.689571</td>\n",
       "      <td>1.303411</td>\n",
       "      <td>{'lr': 1.9436751746165336e-05, 'dropout': 0.31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>1.806186</td>\n",
       "      <td>1.120519</td>\n",
       "      <td>0.648974</td>\n",
       "      <td>0.986284</td>\n",
       "      <td>1.797882</td>\n",
       "      <td>1.696076</td>\n",
       "      <td>{'lr': 0.0008730673512666253, 'dropout': 0.121...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.446178</td>\n",
       "      <td>1.230032</td>\n",
       "      <td>2.281848</td>\n",
       "      <td>3.094991</td>\n",
       "      <td>3.807506</td>\n",
       "      <td>0.851752</td>\n",
       "      <td>{'lr': 9.598696524308393e-05, 'dropout': 0.224...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.457043</td>\n",
       "      <td>0.921864</td>\n",
       "      <td>1.947518</td>\n",
       "      <td>2.670598</td>\n",
       "      <td>3.596875</td>\n",
       "      <td>0.749002</td>\n",
       "      <td>{'lr': 0.00012727545530741416, 'dropout': 0.25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.557769</td>\n",
       "      <td>0.790508</td>\n",
       "      <td>1.737624</td>\n",
       "      <td>2.486406</td>\n",
       "      <td>3.167749</td>\n",
       "      <td>0.757053</td>\n",
       "      <td>{'lr': 0.0001334513928577592, 'dropout': 0.275...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.594035</td>\n",
       "      <td>0.755733</td>\n",
       "      <td>1.711971</td>\n",
       "      <td>2.395836</td>\n",
       "      <td>3.340215</td>\n",
       "      <td>0.769571</td>\n",
       "      <td>{'lr': 0.00014515083939798962, 'dropout': 0.29...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>1.838888</td>\n",
       "      <td>1.122334</td>\n",
       "      <td>0.561770</td>\n",
       "      <td>0.981014</td>\n",
       "      <td>1.697840</td>\n",
       "      <td>1.722377</td>\n",
       "      <td>{'lr': 0.00020120406820448123, 'dropout': 0.23...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            model      type         1         2         3         4         5  \\\n",
       "0   comb_reg_deep  weighted  1.886885  1.180709  0.673292  1.038544  1.826427   \n",
       "1   comb_reg_deep  weighted  0.950231  1.832606  2.877450  3.807033  4.802780   \n",
       "2   comb_reg_deep  weighted  0.799951  1.677605  2.741591  3.632390  4.671007   \n",
       "3   comb_reg_deep  weighted  0.933308  1.817364  2.871818  3.748563  4.689571   \n",
       "4   comb_reg_deep  weighted  1.806186  1.120519  0.648974  0.986284  1.797882   \n",
       "..            ...       ...       ...       ...       ...       ...       ...   \n",
       "70  comb_reg_deep  weighted  0.446178  1.230032  2.281848  3.094991  3.807506   \n",
       "71  comb_reg_deep  weighted  0.457043  0.921864  1.947518  2.670598  3.596875   \n",
       "72  comb_reg_deep  weighted  0.557769  0.790508  1.737624  2.486406  3.167749   \n",
       "73  comb_reg_deep  weighted  0.594035  0.755733  1.711971  2.395836  3.340215   \n",
       "74  comb_reg_deep  weighted  1.838888  1.122334  0.561770  0.981014  1.697840   \n",
       "\n",
       "     overall                                             config  \n",
       "0   1.772961  {'lr': 0.009716870066016842, 'dropout': 0.4663...  \n",
       "1   1.320486  {'lr': 1.1799543502498368e-05, 'dropout': 0.16...  \n",
       "2   1.186541  {'lr': 4.674540872903447e-05, 'dropout': 0.141...  \n",
       "3   1.303411  {'lr': 1.9436751746165336e-05, 'dropout': 0.31...  \n",
       "4   1.696076  {'lr': 0.0008730673512666253, 'dropout': 0.121...  \n",
       "..       ...                                                ...  \n",
       "70  0.851752  {'lr': 9.598696524308393e-05, 'dropout': 0.224...  \n",
       "71  0.749002  {'lr': 0.00012727545530741416, 'dropout': 0.25...  \n",
       "72  0.757053  {'lr': 0.0001334513928577592, 'dropout': 0.275...  \n",
       "73  0.769571  {'lr': 0.00014515083939798962, 'dropout': 0.29...  \n",
       "74  1.722377  {'lr': 0.00020120406820448123, 'dropout': 0.23...  \n",
       "\n",
       "[75 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run deep search for FITBIT dataset\n",
    "results_fitbit_reg = run_deep_search_fitbit_reg(fitbit_reg_dict, use_sample_weights=True, model_name=\"fitbit_reg_deep\", n_trials=75)\n",
    "results_fitbit_reg[\"mean_rmse\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8462120d-798f-49da-9ec1-6354ddaef603",
   "metadata": {},
   "source": [
    "### Run deep search for regression without weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "542557d8-c4b0-477e-89a9-e11eeb37f223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to search/comb_reg_deep_nw_deep_search_results.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>type</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>overall</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.762167</td>\n",
       "      <td>1.664520</td>\n",
       "      <td>2.705903</td>\n",
       "      <td>3.673872</td>\n",
       "      <td>4.643009</td>\n",
       "      <td>1.163452</td>\n",
       "      <td>{'lr': 1.0784117086383168e-05, 'dropout': 0.17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.255529</td>\n",
       "      <td>0.711054</td>\n",
       "      <td>1.682638</td>\n",
       "      <td>2.636605</td>\n",
       "      <td>3.700819</td>\n",
       "      <td>0.602227</td>\n",
       "      <td>{'lr': 0.0003385621410648702, 'dropout': 0.414...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.394708</td>\n",
       "      <td>0.693075</td>\n",
       "      <td>1.647597</td>\n",
       "      <td>2.524740</td>\n",
       "      <td>3.587499</td>\n",
       "      <td>0.648502</td>\n",
       "      <td>{'lr': 0.0043696633242284386, 'dropout': 0.208...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.324434</td>\n",
       "      <td>0.684831</td>\n",
       "      <td>1.657795</td>\n",
       "      <td>2.564347</td>\n",
       "      <td>3.632518</td>\n",
       "      <td>0.617927</td>\n",
       "      <td>{'lr': 0.0007450940369219274, 'dropout': 0.224...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.767520</td>\n",
       "      <td>1.669812</td>\n",
       "      <td>2.711057</td>\n",
       "      <td>3.679885</td>\n",
       "      <td>4.650118</td>\n",
       "      <td>1.168072</td>\n",
       "      <td>{'lr': 1.046526095246742e-05, 'dropout': 0.157...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.274251</td>\n",
       "      <td>0.731098</td>\n",
       "      <td>1.722326</td>\n",
       "      <td>2.650167</td>\n",
       "      <td>3.622489</td>\n",
       "      <td>0.613389</td>\n",
       "      <td>{'lr': 7.447390323461143e-05, 'dropout': 0.229...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.255647</td>\n",
       "      <td>0.702178</td>\n",
       "      <td>1.680308</td>\n",
       "      <td>2.614226</td>\n",
       "      <td>3.665305</td>\n",
       "      <td>0.597130</td>\n",
       "      <td>{'lr': 0.00018275118830783695, 'dropout': 0.21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.255699</td>\n",
       "      <td>0.702785</td>\n",
       "      <td>1.679002</td>\n",
       "      <td>2.614845</td>\n",
       "      <td>3.671535</td>\n",
       "      <td>0.597397</td>\n",
       "      <td>{'lr': 0.00020276656894787232, 'dropout': 0.21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.255981</td>\n",
       "      <td>0.704251</td>\n",
       "      <td>1.678937</td>\n",
       "      <td>2.616246</td>\n",
       "      <td>3.678460</td>\n",
       "      <td>0.598121</td>\n",
       "      <td>{'lr': 0.0002289303718206388, 'dropout': 0.209...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.261519</td>\n",
       "      <td>0.701565</td>\n",
       "      <td>1.684598</td>\n",
       "      <td>2.615193</td>\n",
       "      <td>3.642970</td>\n",
       "      <td>0.598924</td>\n",
       "      <td>{'lr': 0.00013042980648488058, 'dropout': 0.19...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            model          type         1         2         3         4  \\\n",
       "0   comb_reg_deep  not weighted  0.762167  1.664520  2.705903  3.673872   \n",
       "1   comb_reg_deep  not weighted  0.255529  0.711054  1.682638  2.636605   \n",
       "2   comb_reg_deep  not weighted  0.394708  0.693075  1.647597  2.524740   \n",
       "3   comb_reg_deep  not weighted  0.324434  0.684831  1.657795  2.564347   \n",
       "4   comb_reg_deep  not weighted  0.767520  1.669812  2.711057  3.679885   \n",
       "..            ...           ...       ...       ...       ...       ...   \n",
       "70  comb_reg_deep  not weighted  0.274251  0.731098  1.722326  2.650167   \n",
       "71  comb_reg_deep  not weighted  0.255647  0.702178  1.680308  2.614226   \n",
       "72  comb_reg_deep  not weighted  0.255699  0.702785  1.679002  2.614845   \n",
       "73  comb_reg_deep  not weighted  0.255981  0.704251  1.678937  2.616246   \n",
       "74  comb_reg_deep  not weighted  0.261519  0.701565  1.684598  2.615193   \n",
       "\n",
       "           5   overall                                             config  \n",
       "0   4.643009  1.163452  {'lr': 1.0784117086383168e-05, 'dropout': 0.17...  \n",
       "1   3.700819  0.602227  {'lr': 0.0003385621410648702, 'dropout': 0.414...  \n",
       "2   3.587499  0.648502  {'lr': 0.0043696633242284386, 'dropout': 0.208...  \n",
       "3   3.632518  0.617927  {'lr': 0.0007450940369219274, 'dropout': 0.224...  \n",
       "4   4.650118  1.168072  {'lr': 1.046526095246742e-05, 'dropout': 0.157...  \n",
       "..       ...       ...                                                ...  \n",
       "70  3.622489  0.613389  {'lr': 7.447390323461143e-05, 'dropout': 0.229...  \n",
       "71  3.665305  0.597130  {'lr': 0.00018275118830783695, 'dropout': 0.21...  \n",
       "72  3.671535  0.597397  {'lr': 0.00020276656894787232, 'dropout': 0.21...  \n",
       "73  3.678460  0.598121  {'lr': 0.0002289303718206388, 'dropout': 0.209...  \n",
       "74  3.642970  0.598924  {'lr': 0.00013042980648488058, 'dropout': 0.19...  \n",
       "\n",
       "[75 rows x 9 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run deep search for COMB dataset (n_trials determines how many steps/trials are executed)\n",
    "results_comb_reg = run_deep_search_comb_reg(comb_reg_dict, use_sample_weights=False, model_name=\"comb_reg_deep\", n_trials=75)\n",
    "results_comb_reg[\"mean_rmse\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7b199df-3b00-43d6-ad74-beffb4b0733d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to search/fitbit_reg_deep_nw_deep_search_results.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>type</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>overall</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.762167</td>\n",
       "      <td>1.664520</td>\n",
       "      <td>2.705903</td>\n",
       "      <td>3.673872</td>\n",
       "      <td>4.643009</td>\n",
       "      <td>1.163452</td>\n",
       "      <td>{'lr': 1.0784117086383168e-05, 'dropout': 0.17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.255529</td>\n",
       "      <td>0.711054</td>\n",
       "      <td>1.682638</td>\n",
       "      <td>2.636605</td>\n",
       "      <td>3.700819</td>\n",
       "      <td>0.602227</td>\n",
       "      <td>{'lr': 0.0003385621410648702, 'dropout': 0.414...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.394708</td>\n",
       "      <td>0.693075</td>\n",
       "      <td>1.647597</td>\n",
       "      <td>2.524740</td>\n",
       "      <td>3.587499</td>\n",
       "      <td>0.648502</td>\n",
       "      <td>{'lr': 0.0043696633242284386, 'dropout': 0.208...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.324434</td>\n",
       "      <td>0.684831</td>\n",
       "      <td>1.657795</td>\n",
       "      <td>2.564347</td>\n",
       "      <td>3.632518</td>\n",
       "      <td>0.617927</td>\n",
       "      <td>{'lr': 0.0007450940369219274, 'dropout': 0.224...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.767520</td>\n",
       "      <td>1.669812</td>\n",
       "      <td>2.711057</td>\n",
       "      <td>3.679885</td>\n",
       "      <td>4.650118</td>\n",
       "      <td>1.168072</td>\n",
       "      <td>{'lr': 1.046526095246742e-05, 'dropout': 0.157...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.274251</td>\n",
       "      <td>0.731098</td>\n",
       "      <td>1.722326</td>\n",
       "      <td>2.650167</td>\n",
       "      <td>3.622489</td>\n",
       "      <td>0.613389</td>\n",
       "      <td>{'lr': 7.447390323461143e-05, 'dropout': 0.229...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.255647</td>\n",
       "      <td>0.702178</td>\n",
       "      <td>1.680308</td>\n",
       "      <td>2.614226</td>\n",
       "      <td>3.665305</td>\n",
       "      <td>0.597130</td>\n",
       "      <td>{'lr': 0.00018275118830783695, 'dropout': 0.21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.255699</td>\n",
       "      <td>0.702785</td>\n",
       "      <td>1.679002</td>\n",
       "      <td>2.614845</td>\n",
       "      <td>3.671535</td>\n",
       "      <td>0.597397</td>\n",
       "      <td>{'lr': 0.00020276656894787232, 'dropout': 0.21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.255981</td>\n",
       "      <td>0.704251</td>\n",
       "      <td>1.678937</td>\n",
       "      <td>2.616246</td>\n",
       "      <td>3.678460</td>\n",
       "      <td>0.598121</td>\n",
       "      <td>{'lr': 0.0002289303718206388, 'dropout': 0.209...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>comb_reg_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.261519</td>\n",
       "      <td>0.701565</td>\n",
       "      <td>1.684598</td>\n",
       "      <td>2.615193</td>\n",
       "      <td>3.642970</td>\n",
       "      <td>0.598924</td>\n",
       "      <td>{'lr': 0.00013042980648488058, 'dropout': 0.19...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            model          type         1         2         3         4  \\\n",
       "0   comb_reg_deep  not weighted  0.762167  1.664520  2.705903  3.673872   \n",
       "1   comb_reg_deep  not weighted  0.255529  0.711054  1.682638  2.636605   \n",
       "2   comb_reg_deep  not weighted  0.394708  0.693075  1.647597  2.524740   \n",
       "3   comb_reg_deep  not weighted  0.324434  0.684831  1.657795  2.564347   \n",
       "4   comb_reg_deep  not weighted  0.767520  1.669812  2.711057  3.679885   \n",
       "..            ...           ...       ...       ...       ...       ...   \n",
       "70  comb_reg_deep  not weighted  0.274251  0.731098  1.722326  2.650167   \n",
       "71  comb_reg_deep  not weighted  0.255647  0.702178  1.680308  2.614226   \n",
       "72  comb_reg_deep  not weighted  0.255699  0.702785  1.679002  2.614845   \n",
       "73  comb_reg_deep  not weighted  0.255981  0.704251  1.678937  2.616246   \n",
       "74  comb_reg_deep  not weighted  0.261519  0.701565  1.684598  2.615193   \n",
       "\n",
       "           5   overall                                             config  \n",
       "0   4.643009  1.163452  {'lr': 1.0784117086383168e-05, 'dropout': 0.17...  \n",
       "1   3.700819  0.602227  {'lr': 0.0003385621410648702, 'dropout': 0.414...  \n",
       "2   3.587499  0.648502  {'lr': 0.0043696633242284386, 'dropout': 0.208...  \n",
       "3   3.632518  0.617927  {'lr': 0.0007450940369219274, 'dropout': 0.224...  \n",
       "4   4.650118  1.168072  {'lr': 1.046526095246742e-05, 'dropout': 0.157...  \n",
       "..       ...       ...                                                ...  \n",
       "70  3.622489  0.613389  {'lr': 7.447390323461143e-05, 'dropout': 0.229...  \n",
       "71  3.665305  0.597130  {'lr': 0.00018275118830783695, 'dropout': 0.21...  \n",
       "72  3.671535  0.597397  {'lr': 0.00020276656894787232, 'dropout': 0.21...  \n",
       "73  3.678460  0.598121  {'lr': 0.0002289303718206388, 'dropout': 0.209...  \n",
       "74  3.642970  0.598924  {'lr': 0.00013042980648488058, 'dropout': 0.19...  \n",
       "\n",
       "[75 rows x 9 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run deep search for FITBIT dataset\n",
    "results_fitbit_reg = run_deep_search_fitbit_reg(fitbit_reg_dict, use_sample_weights=False, model_name=\"fitbit_reg_deep\", n_trials=75)\n",
    "results_fitbit_reg[\"mean_rmse\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e6def1-1794-4260-8b85-8b6f8a36fea9",
   "metadata": {},
   "source": [
    "###  Deep Search Objective Functions for classifcation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8389af99-d0f7-4501-837d-69afa45c334a",
   "metadata": {},
   "source": [
    "### Classification Deep Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b2345-220e-4ab9-b45e-b73b13141bb0",
   "metadata": {},
   "source": [
    "Comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a187162-c2f7-4cf8-970f-6c3ea50ef294",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Updated objective_classification_comb using LSTM\n",
    "def objective_classification_comb_nw(trial, data_dict):\n",
    "    \"\"\"\n",
    "    Objective function for comb_class using LSTM on non-weighted data.\n",
    "    Searched hyperparameters: dropout, lr, hidden_size, n_layers, bidirection.\n",
    "    Fixed parameters: batch_size=32, epochs=10.\n",
    "    \"\"\"\n",
    "    set_seed(42)\n",
    "    splits = get_stratified_cv_splits(\n",
    "        data_dict['train'], subject_id=\"PatientID\", target_var=\"is_SI\", n_splits=5\n",
    "    )\n",
    "    \n",
    "    overall_acc_list = []\n",
    "    overall_sens_list = []\n",
    "    overall_spec_list = []\n",
    "    \n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    dropout_val = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 16, 128, step=16)\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    bidirectional = trial.suggest_categorical(\"bidirection\", [True, False])\n",
    "    batch_size = 32\n",
    "\n",
    "    for train_split, val_split in splits:\n",
    "        # Force non-weighted for classification as well\n",
    "        train_df, _ = create_subject_dataset(train_split, outcome_col=\"is_SI\", use_weights=False)\n",
    "        val_df, _   = create_subject_dataset(val_split, outcome_col=\"is_SI\", use_weights=False)\n",
    "        \n",
    "        X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "        y_train = train_df[\"is_SI\"].values.astype(np.float32)\n",
    "        \n",
    "        X_val = np.stack(val_df[\"X\"].values, axis=0)\n",
    "        y_val = val_df[\"is_SI\"].values.astype(np.float32)\n",
    "        \n",
    "        X_train = np.transpose(X_train, (0, 2, 1))\n",
    "        X_val   = np.transpose(X_val, (0, 2, 1))\n",
    "        n_subjects, seq_len, input_size = X_train.shape\n",
    "\n",
    "        class CombClassLSTM(nn.Module):\n",
    "            def __init__(self, input_size, hidden_size, dropout, n_layers, bidirectional):\n",
    "                super(CombClassLSTM, self).__init__()\n",
    "                self.bidirectional = bidirectional\n",
    "                self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=n_layers,\n",
    "                                    batch_first=True, bidirectional=bidirectional)\n",
    "                fc_in_dim = hidden_size * (2 if bidirectional else 1)\n",
    "                self.dropout = nn.Dropout(dropout)\n",
    "                self.fc = nn.Linear(fc_in_dim, 1)\n",
    "            def forward(self, x):\n",
    "                lstm_out, (h_n, _) = self.lstm(x)\n",
    "                if self.bidirectional:\n",
    "                    h_last = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "                else:\n",
    "                    h_last = h_n[-1]\n",
    "                out = self.dropout(h_last)\n",
    "                out = self.fc(out)\n",
    "                return out\n",
    "        \n",
    "        model = CombClassLSTM(input_size, hidden_size, dropout_val, n_layers, bidirectional).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        train_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(10):  # fixed epochs\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = loss_fn(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "            logits = model(X_val_tensor)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().reshape(-1)\n",
    "            preds = (probs >= 0.5).astype(np.float32)\n",
    "        fold_acc = np.mean(preds == y_val)\n",
    "        TP = np.sum((preds == 1) & (y_val == 1))\n",
    "        FN = np.sum((preds == 0) & (y_val == 1))\n",
    "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "        TN = np.sum((preds == 0) & (y_val == 0))\n",
    "        FP = np.sum((preds == 1) & (y_val == 0))\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
    "        \n",
    "        overall_acc_list.append(fold_acc)\n",
    "        overall_sens_list.append(sensitivity)\n",
    "        overall_spec_list.append(specificity)\n",
    "    \n",
    "    mean_acc = np.mean(overall_acc_list)\n",
    "    std_acc = np.std(overall_acc_list)\n",
    "    mean_sens = np.mean(overall_sens_list)\n",
    "    std_sens = np.std(overall_sens_list)\n",
    "    mean_spec = np.mean(overall_spec_list)\n",
    "    std_spec = np.std(overall_spec_list)\n",
    "    \n",
    "    trial.set_user_attr(\"overall_acc_mean\", mean_acc)\n",
    "    trial.set_user_attr(\"overall_acc_std\", std_acc)\n",
    "    trial.set_user_attr(\"sensitivity_mean\", mean_sens)\n",
    "    trial.set_user_attr(\"sensitivity_std\", std_sens)\n",
    "    trial.set_user_attr(\"specificity_mean\", mean_spec)\n",
    "    trial.set_user_attr(\"specificity_std\", std_spec)\n",
    "    \n",
    "    return 1 - mean_acc\n",
    "\n",
    "def objective_classification_comb(trial, data_dict):\n",
    "    \"\"\"\n",
    "    Objective function for comb_class using LSTM.\n",
    "    Tunable hyperparameters:\n",
    "      - dropout\n",
    "      - lr\n",
    "      - num_epochs\n",
    "      - hidden_size\n",
    "      - bidirectional\n",
    "    \"\"\"\n",
    "    set_seed(42)\n",
    "    splits = get_stratified_cv_splits(data_dict['train'], subject_id=\"PatientID\", target_var=\"is_SI\", n_splits=5)\n",
    "    \n",
    "    overall_acc_list = []\n",
    "    overall_sens_list = []\n",
    "    overall_spec_list = []\n",
    "    \n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    dropout_val = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 5, 10)\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 16, 128, step=16)\n",
    "    bidirectional = trial.suggest_categorical(\"bidirectional\", [True, False])\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 16, 64, step=16)\n",
    "    \n",
    "    for train_split, val_split in splits:\n",
    "        train_df, _ = create_subject_dataset(train_split, outcome_col=\"is_SI\")\n",
    "        val_df, _ = create_subject_dataset(val_split, outcome_col=\"is_SI\")\n",
    "        \n",
    "        X_train = np.stack(train_df[\"X\"].values, axis=0)  # (n_subjects, n_features, seq_len)\n",
    "        y_train = train_df[\"is_SI\"].values.astype(np.float32)\n",
    "        \n",
    "        X_val = np.stack(val_df[\"X\"].values, axis=0)\n",
    "        y_val = val_df[\"is_SI\"].values.astype(np.float32)\n",
    "        \n",
    "        # Transpose for LSTM: (n_subjects, seq_len, n_features)\n",
    "        X_train = np.transpose(X_train, (0, 2, 1))\n",
    "        X_val = np.transpose(X_val, (0, 2, 1))\n",
    "        n_subjects, seq_len, input_size = X_train.shape\n",
    "\n",
    "        # Define LSTM model for classification\n",
    "        class CombClassLSTM(nn.Module):\n",
    "            def __init__(self, input_size, hidden_size, dropout, bidirectional):\n",
    "                super(CombClassLSTM, self).__init__()\n",
    "                self.bidirectional = bidirectional\n",
    "                self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True, bidirectional=bidirectional)\n",
    "                fc_in_dim = hidden_size * (2 if bidirectional else 1)\n",
    "                self.dropout = nn.Dropout(dropout)\n",
    "                self.fc = nn.Linear(fc_in_dim, 1)\n",
    "            def forward(self, x):\n",
    "                lstm_out, (h_n, _) = self.lstm(x)\n",
    "                if self.bidirectional:\n",
    "                    h_last = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "                else:\n",
    "                    h_last = h_n[-1]\n",
    "                out = self.dropout(h_last)\n",
    "                out = self.fc(out)\n",
    "                return out\n",
    "        \n",
    "        model = CombClassLSTM(input_size, hidden_size, dropout_val, bidirectional).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        train_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = loss_fn(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "            logits = model(X_val_tensor)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().reshape(-1)\n",
    "            preds = (probs >= 0.5).astype(np.float32)\n",
    "        fold_acc = np.mean(preds == y_val)\n",
    "        TP = np.sum((preds == 1) & (y_val == 1))\n",
    "        FN = np.sum((preds == 0) & (y_val == 1))\n",
    "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "        TN = np.sum((preds == 0) & (y_val == 0))\n",
    "        FP = np.sum((preds == 1) & (y_val == 0))\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
    "        \n",
    "        overall_acc_list.append(fold_acc)\n",
    "        overall_sens_list.append(sensitivity)\n",
    "        overall_spec_list.append(specificity)\n",
    "    \n",
    "    mean_acc = np.mean(overall_acc_list)\n",
    "    std_acc = np.std(overall_acc_list)\n",
    "    mean_sens = np.mean(overall_sens_list)\n",
    "    std_sens = np.std(overall_sens_list)\n",
    "    mean_spec = np.mean(overall_spec_list)\n",
    "    std_spec = np.std(overall_spec_list)\n",
    "    \n",
    "    trial.set_user_attr(\"overall_acc_mean\", mean_acc)\n",
    "    trial.set_user_attr(\"overall_acc_std\", std_acc)\n",
    "    trial.set_user_attr(\"sensitivity_mean\", mean_sens)\n",
    "    trial.set_user_attr(\"sensitivity_std\", std_sens)\n",
    "    trial.set_user_attr(\"specificity_mean\", mean_spec)\n",
    "    trial.set_user_attr(\"specificity_std\", std_spec)\n",
    "    \n",
    "    return 1 - mean_acc  # minimization objective\n",
    "\n",
    "\n",
    "def run_deep_search_comb_class(data_dict, use_sample_weights, model_name, n_trials=75):\n",
    "    \"\"\"\n",
    "    Run deep search for comb_class.\n",
    "    Selects the proper objective function based on use_sample_weights.\n",
    "    \"\"\"\n",
    "    data_dict[\"use_sample_weights\"] = use_sample_weights\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "    def print_progress(study, trial):\n",
    "        print(f\"{model_name} trial {len(study.trials)}/{n_trials}\\r\", end=\"\", flush=True)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    optimize_fn = objective_classification_comb if use_sample_weights else objective_classification_comb_nw\n",
    "\n",
    "    study.optimize(lambda trial: optimize_fn(trial, data_dict),\n",
    "                   n_trials=n_trials, callbacks=[print_progress])\n",
    "\n",
    "    mean_rows = []\n",
    "    std_rows = []\n",
    "    for t in study.trials:\n",
    "        row_mean = {\n",
    "            \"model\": model_name,\n",
    "            \"type\": \"weighted\" if use_sample_weights else \"not weighted\",\n",
    "            \"accuracy\": t.user_attrs[\"overall_acc_mean\"],\n",
    "            \"sensitivity\": t.user_attrs[\"sensitivity_mean\"],\n",
    "            \"specificity\": t.user_attrs[\"specificity_mean\"],\n",
    "            \"config\": t.params\n",
    "        }\n",
    "        row_std = {\n",
    "            \"model\": model_name,\n",
    "            \"type\": \"weighted\" if use_sample_weights else \"not weighted\",\n",
    "            \"accuracy\": t.user_attrs[\"overall_acc_std\"],\n",
    "            \"sensitivity\": t.user_attrs[\"sensitivity_std\"],\n",
    "            \"specificity\": t.user_attrs[\"specificity_std\"],\n",
    "            \"config\": t.params\n",
    "        }\n",
    "        mean_rows.append(row_mean)\n",
    "        std_rows.append(row_std)\n",
    "\n",
    "    columns = [\"model\", \"type\", \"accuracy\", \"sensitivity\", \"specificity\", \"config\"]\n",
    "    mean_df = pd.DataFrame(mean_rows, columns=columns)\n",
    "    std_df = pd.DataFrame(std_rows, columns=columns)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    optimal_configuration = {\n",
    "        \"value\": best_trial.value,\n",
    "        \"params\": best_trial.params,\n",
    "        \"user_attrs\": best_trial.user_attrs\n",
    "    }\n",
    "\n",
    "    result_dict = {\n",
    "        \"mean_metrics\": mean_df,\n",
    "        \"std_metrics\": std_df,\n",
    "        \"optimal_configuration\": optimal_configuration\n",
    "    }\n",
    "    save_results_pickle(result_dict, model_name, use_sample_weights)\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3481d059-4d6b-4ecc-8c05-92c7904c146b",
   "metadata": {},
   "source": [
    "comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc16c008-bcd4-4e5d-a142-2c9bca72acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_classification_fitbit_nw(trial, data_dict):\n",
    "    \"\"\"\n",
    "    Objective function for fitbit_class using LSTM on non-weighted data.\n",
    "    Searched hyperparameters: lr, dropout, batch_size.\n",
    "    Fixed parameters: hidden_size=32, num_spochs=7.\n",
    "    \"\"\"\n",
    "    set_seed(42)\n",
    "    splits = get_stratified_cv_splits(\n",
    "        data_dict['train'], subject_id=\"PatientID\", target_var=\"is_SI\", n_splits=5\n",
    "    )\n",
    "    \n",
    "    overall_acc_list = []\n",
    "    overall_sens_list = []\n",
    "    overall_spec_list = []\n",
    "    \n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    dropout_val = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 16, 64, step=16)\n",
    "    \n",
    "    # Fixed parameters\n",
    "    hidden_size = 32\n",
    "    num_spochs = 7  # fixed number of epochs\n",
    "    \n",
    "    for train_split, val_split in splits:\n",
    "        # Force non-weighted\n",
    "        train_df, _ = create_subject_dataset(train_split, outcome_col=\"is_SI\", use_weights=False)\n",
    "        val_df, _   = create_subject_dataset(val_split, outcome_col=\"is_SI\", use_weights=False)\n",
    "        \n",
    "        X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "        y_train = train_df[\"is_SI\"].values.astype(np.float32)\n",
    "        \n",
    "        X_val = np.stack(val_df[\"X\"].values, axis=0)\n",
    "        y_val = val_df[\"is_SI\"].values.astype(np.float32)\n",
    "        \n",
    "        X_train = np.transpose(X_train, (0, 2, 1))\n",
    "        X_val   = np.transpose(X_val, (0, 2, 1))\n",
    "        n_subjects, seq_len, input_size = X_train.shape\n",
    "\n",
    "        class FitbitClassLSTM(nn.Module):\n",
    "            def __init__(self, input_size, hidden_size, dropout):\n",
    "                super(FitbitClassLSTM, self).__init__()\n",
    "                self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "                self.dropout = nn.Dropout(dropout)\n",
    "                self.fc = nn.Linear(hidden_size, 1)\n",
    "            def forward(self, x):\n",
    "                lstm_out, (h_n, _) = self.lstm(x)\n",
    "                h_last = h_n[-1]\n",
    "                out = self.dropout(h_last)\n",
    "                out = self.fc(out)\n",
    "                return out\n",
    "\n",
    "        model = FitbitClassLSTM(input_size, hidden_size, dropout_val).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        train_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(num_spochs):\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = loss_fn(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "            logits = model(X_val_tensor)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().reshape(-1)\n",
    "            preds = (probs >= 0.5).astype(np.float32)\n",
    "        fold_acc = np.mean(preds == y_val)\n",
    "        TP = np.sum((preds == 1) & (y_val == 1))\n",
    "        FN = np.sum((preds == 0) & (y_val == 1))\n",
    "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "        TN = np.sum((preds == 0) & (y_val == 0))\n",
    "        FP = np.sum((preds == 1) & (y_val == 0))\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
    "        \n",
    "        overall_acc_list.append(fold_acc)\n",
    "        overall_sens_list.append(sensitivity)\n",
    "        overall_spec_list.append(specificity)\n",
    "    \n",
    "    mean_acc = np.mean(overall_acc_list)\n",
    "    std_acc = np.std(overall_acc_list)\n",
    "    mean_sens = np.mean(overall_sens_list)\n",
    "    std_sens = np.std(overall_sens_list)\n",
    "    mean_spec = np.mean(overall_spec_list)\n",
    "    std_spec = np.std(overall_spec_list)\n",
    "    \n",
    "    trial.set_user_attr(\"overall_acc_mean\", mean_acc)\n",
    "    trial.set_user_attr(\"overall_acc_std\", std_acc)\n",
    "    trial.set_user_attr(\"sensitivity_mean\", mean_sens)\n",
    "    trial.set_user_attr(\"sensitivity_std\", std_sens)\n",
    "    trial.set_user_attr(\"specificity_mean\", mean_spec)\n",
    "    trial.set_user_attr(\"specificity_std\", std_spec)\n",
    "    \n",
    "    return 1 - mean_acc\n",
    "\n",
    "def objective_classification_fitbit(trial, data_dict):\n",
    "    \"\"\"\n",
    "    Objective function for fitbit_class using LSTM.\n",
    "    Tunable hyperparameters:\n",
    "      - lr\n",
    "      - hidden_size\n",
    "      - dropout\n",
    "      - num_spochs\n",
    "    \"\"\"\n",
    "    set_seed(42)\n",
    "    splits = get_stratified_cv_splits(data_dict['train'], subject_id=\"PatientID\", target_var=\"is_SI\", n_splits=5)\n",
    "    \n",
    "    overall_acc_list = []\n",
    "    overall_sens_list = []\n",
    "    overall_spec_list = []\n",
    "    \n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 16, 128, step=16)\n",
    "    dropout_val = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    num_spochs = trial.suggest_int(\"num_spochs\", 5, 10)\n",
    "    batch_size = 32  # fixed\n",
    "    \n",
    "    for train_split, val_split in splits:\n",
    "        train_df, _ = create_subject_dataset(train_split, outcome_col=\"is_SI\")\n",
    "        val_df, _ = create_subject_dataset(val_split, outcome_col=\"is_SI\")\n",
    "        \n",
    "        X_train = np.stack(train_df[\"X\"].values, axis=0)  # (n_subjects, n_features, seq_len)\n",
    "        y_train = train_df[\"is_SI\"].values.astype(np.float32)\n",
    "        \n",
    "        X_val = np.stack(val_df[\"X\"].values, axis=0)\n",
    "        y_val = val_df[\"is_SI\"].values.astype(np.float32)\n",
    "        \n",
    "        # Transpose to (n_subjects, seq_len, n_features)\n",
    "        X_train = np.transpose(X_train, (0, 2, 1))\n",
    "        X_val = np.transpose(X_val, (0, 2, 1))\n",
    "        n_subjects, seq_len, input_size = X_train.shape\n",
    "        \n",
    "        # Define LSTM model for fitbit classification\n",
    "        class FitbitClassLSTM(nn.Module):\n",
    "            def __init__(self, input_size, hidden_size, dropout):\n",
    "                super(FitbitClassLSTM, self).__init__()\n",
    "                self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "                self.dropout = nn.Dropout(dropout)\n",
    "                self.fc = nn.Linear(hidden_size, 1)\n",
    "            def forward(self, x):\n",
    "                lstm_out, (h_n, _) = self.lstm(x)\n",
    "                h_last = h_n[-1]\n",
    "                out = self.dropout(h_last)\n",
    "                out = self.fc(out)\n",
    "                return out\n",
    "\n",
    "        model = FitbitClassLSTM(input_size, hidden_size, dropout_val).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        train_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(num_spochs):  # using num_spochs instead of fixed epochs\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = loss_fn(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "            logits = model(X_val_tensor)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().reshape(-1)\n",
    "            preds = (probs >= 0.5).astype(np.float32)\n",
    "        fold_acc = np.mean(preds == y_val)\n",
    "        TP = np.sum((preds == 1) & (y_val == 1))\n",
    "        FN = np.sum((preds == 0) & (y_val == 1))\n",
    "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "        TN = np.sum((preds == 0) & (y_val == 0))\n",
    "        FP = np.sum((preds == 1) & (y_val == 0))\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
    "        \n",
    "        overall_acc_list.append(fold_acc)\n",
    "        overall_sens_list.append(sensitivity)\n",
    "        overall_spec_list.append(specificity)\n",
    "    \n",
    "    mean_acc = np.mean(overall_acc_list)\n",
    "    std_acc = np.std(overall_acc_list)\n",
    "    mean_sens = np.mean(overall_sens_list)\n",
    "    std_sens = np.std(overall_sens_list)\n",
    "    mean_spec = np.mean(overall_spec_list)\n",
    "    std_spec = np.std(overall_spec_list)\n",
    "    \n",
    "    trial.set_user_attr(\"overall_acc_mean\", mean_acc)\n",
    "    trial.set_user_attr(\"overall_acc_std\", std_acc)\n",
    "    trial.set_user_attr(\"sensitivity_mean\", mean_sens)\n",
    "    trial.set_user_attr(\"sensitivity_std\", std_sens)\n",
    "    trial.set_user_attr(\"specificity_mean\", mean_spec)\n",
    "    trial.set_user_attr(\"specificity_std\", std_spec)\n",
    "    \n",
    "    return 1 - mean_acc\n",
    "\n",
    "def run_deep_search_fitbit_class(data_dict, use_sample_weights, model_name, n_trials=75):\n",
    "    \"\"\"\n",
    "    Run deep search for fitbit_class.\n",
    "    Selects the proper objective function based on the use_sample_weights flag.\n",
    "    \"\"\"\n",
    "    data_dict[\"use_sample_weights\"] = use_sample_weights\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "    def print_progress(study, trial):\n",
    "        print(f\"{model_name} trial {len(study.trials)}/{n_trials}\\r\", end=\"\", flush=True)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    optimize_fn = objective_classification_fitbit if use_sample_weights else objective_classification_fitbit_nw\n",
    "\n",
    "    study.optimize(lambda trial: optimize_fn(trial, data_dict),\n",
    "                   n_trials=n_trials, callbacks=[print_progress])\n",
    "\n",
    "    mean_rows = []\n",
    "    std_rows = []\n",
    "    for t in study.trials:\n",
    "        row_mean = {\n",
    "            \"model\": model_name,\n",
    "            \"type\": \"weighted\" if use_sample_weights else \"not weighted\",\n",
    "            \"accuracy\": t.user_attrs[\"overall_acc_mean\"],\n",
    "            \"sensitivity\": t.user_attrs[\"sensitivity_mean\"],\n",
    "            \"specificity\": t.user_attrs[\"specificity_mean\"],\n",
    "            \"config\": t.params\n",
    "        }\n",
    "        row_std = {\n",
    "            \"model\": model_name,\n",
    "            \"type\": \"weighted\" if use_sample_weights else \"not weighted\",\n",
    "            \"accuracy\": t.user_attrs[\"overall_acc_std\"],\n",
    "            \"sensitivity\": t.user_attrs[\"sensitivity_std\"],\n",
    "            \"specificity\": t.user_attrs[\"specificity_std\"],\n",
    "            \"config\": t.params\n",
    "        }\n",
    "        mean_rows.append(row_mean)\n",
    "        std_rows.append(row_std)\n",
    "\n",
    "    columns = [\"model\", \"type\", \"accuracy\", \"sensitivity\", \"specificity\", \"config\"]\n",
    "    mean_df = pd.DataFrame(mean_rows, columns=columns)\n",
    "    std_df = pd.DataFrame(std_rows, columns=columns)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    optimal_configuration = {\n",
    "        \"value\": best_trial.value,\n",
    "        \"params\": best_trial.params,\n",
    "        \"user_attrs\": best_trial.user_attrs\n",
    "    }\n",
    "\n",
    "    result_dict = {\n",
    "        \"mean_metrics\": mean_df,\n",
    "        \"std_metrics\": std_df,\n",
    "        \"optimal_configuration\": optimal_configuration\n",
    "    }\n",
    "    save_results_pickle(result_dict, model_name, use_sample_weights)\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b516c6-5192-40bc-9f75-4944736e7ee7",
   "metadata": {},
   "source": [
    "### Run Deep Search for Classification with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad2a3e2e-a6c3-4dc0-9894-64516b0a5ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to search/comb_class_deep_deep_search_results.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.763398</td>\n",
       "      <td>0.009412</td>\n",
       "      <td>0.999267</td>\n",
       "      <td>{'lr': 9.27062449324707e-05, 'dropout': 0.4552...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.718146</td>\n",
       "      <td>0.091655</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>{'lr': 9.611670464299026e-05, 'dropout': 0.134...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.742201</td>\n",
       "      <td>0.156908</td>\n",
       "      <td>0.925275</td>\n",
       "      <td>{'lr': 0.007287458567971986, 'dropout': 0.3569...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.761720</td>\n",
       "      <td>0.004679</td>\n",
       "      <td>0.998535</td>\n",
       "      <td>{'lr': 4.903496748640037e-05, 'dropout': 0.343...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.758371</td>\n",
       "      <td>0.014118</td>\n",
       "      <td>0.991209</td>\n",
       "      <td>{'lr': 2.4021044387147108e-05, 'dropout': 0.29...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.728784</td>\n",
       "      <td>0.182544</td>\n",
       "      <td>0.899634</td>\n",
       "      <td>{'lr': 0.0007938829759058924, 'dropout': 0.260...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.762282</td>\n",
       "      <td>0.044487</td>\n",
       "      <td>0.986813</td>\n",
       "      <td>{'lr': 0.00016201017991013098, 'dropout': 0.29...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.764512</td>\n",
       "      <td>0.039781</td>\n",
       "      <td>0.991209</td>\n",
       "      <td>{'lr': 0.00019936384138497678, 'dropout': 0.31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.763955</td>\n",
       "      <td>0.051518</td>\n",
       "      <td>0.986813</td>\n",
       "      <td>{'lr': 0.00022303473310716408, 'dropout': 0.17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.763399</td>\n",
       "      <td>0.018769</td>\n",
       "      <td>0.996337</td>\n",
       "      <td>{'lr': 0.0002863353135814341, 'dropout': 0.312...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              model      type  accuracy  sensitivity  specificity  \\\n",
       "0   comb_class_deep  weighted  0.763398     0.009412     0.999267   \n",
       "1   comb_class_deep  weighted  0.718146     0.091655     0.914286   \n",
       "2   comb_class_deep  weighted  0.742201     0.156908     0.925275   \n",
       "3   comb_class_deep  weighted  0.761720     0.004679     0.998535   \n",
       "4   comb_class_deep  weighted  0.758371     0.014118     0.991209   \n",
       "..              ...       ...       ...          ...          ...   \n",
       "70  comb_class_deep  weighted  0.728784     0.182544     0.899634   \n",
       "71  comb_class_deep  weighted  0.762282     0.044487     0.986813   \n",
       "72  comb_class_deep  weighted  0.764512     0.039781     0.991209   \n",
       "73  comb_class_deep  weighted  0.763955     0.051518     0.986813   \n",
       "74  comb_class_deep  weighted  0.763399     0.018769     0.996337   \n",
       "\n",
       "                                               config  \n",
       "0   {'lr': 9.27062449324707e-05, 'dropout': 0.4552...  \n",
       "1   {'lr': 9.611670464299026e-05, 'dropout': 0.134...  \n",
       "2   {'lr': 0.007287458567971986, 'dropout': 0.3569...  \n",
       "3   {'lr': 4.903496748640037e-05, 'dropout': 0.343...  \n",
       "4   {'lr': 2.4021044387147108e-05, 'dropout': 0.29...  \n",
       "..                                                ...  \n",
       "70  {'lr': 0.0007938829759058924, 'dropout': 0.260...  \n",
       "71  {'lr': 0.00016201017991013098, 'dropout': 0.29...  \n",
       "72  {'lr': 0.00019936384138497678, 'dropout': 0.31...  \n",
       "73  {'lr': 0.00022303473310716408, 'dropout': 0.17...  \n",
       "74  {'lr': 0.0002863353135814341, 'dropout': 0.312...  \n",
       "\n",
       "[75 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_comb_class = run_deep_search_comb_class(comb_class_dict, use_sample_weights=True, model_name=\"comb_class_deep\", n_trials=75)\n",
    "results_comb_class[\"mean_metrics\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dca28ccf-1577-48fb-a387-20d3c3982c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to search/fitbit_class_deep_deep_search_results.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.490226</td>\n",
       "      <td>0.576635</td>\n",
       "      <td>0.462271</td>\n",
       "      <td>{'lr': 1.1820409925225312e-05, 'hidden_size': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.761163</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>0.997802</td>\n",
       "      <td>{'lr': 0.0001547622268491406, 'hidden_size': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.737748</td>\n",
       "      <td>0.088755</td>\n",
       "      <td>0.940659</td>\n",
       "      <td>{'lr': 3.210643749578772e-05, 'hidden_size': 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.538554</td>\n",
       "      <td>0.383776</td>\n",
       "      <td>0.586813</td>\n",
       "      <td>{'lr': 3.7523058456721776e-05, 'hidden_size': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.762277</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'lr': 0.00011395742363510966, 'hidden_size': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.763955</td>\n",
       "      <td>0.018769</td>\n",
       "      <td>0.997070</td>\n",
       "      <td>{'lr': 0.0001663799488249265, 'hidden_size': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.763955</td>\n",
       "      <td>0.018769</td>\n",
       "      <td>0.997070</td>\n",
       "      <td>{'lr': 0.00017296898246593756, 'hidden_size': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.763396</td>\n",
       "      <td>0.018769</td>\n",
       "      <td>0.996337</td>\n",
       "      <td>{'lr': 0.00017847120939321913, 'hidden_size': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.760604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.998535</td>\n",
       "      <td>{'lr': 0.00018117713776052256, 'hidden_size': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>weighted</td>\n",
       "      <td>0.761165</td>\n",
       "      <td>0.018687</td>\n",
       "      <td>0.993407</td>\n",
       "      <td>{'lr': 0.00027424980105462916, 'hidden_size': ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                model      type  accuracy  sensitivity  specificity  \\\n",
       "0   fitbit_class_deep  weighted  0.490226     0.576635     0.462271   \n",
       "1   fitbit_class_deep  weighted  0.761163     0.004651     0.997802   \n",
       "2   fitbit_class_deep  weighted  0.737748     0.088755     0.940659   \n",
       "3   fitbit_class_deep  weighted  0.538554     0.383776     0.586813   \n",
       "4   fitbit_class_deep  weighted  0.762277     0.002326     1.000000   \n",
       "..                ...       ...       ...          ...          ...   \n",
       "70  fitbit_class_deep  weighted  0.763955     0.018769     0.997070   \n",
       "71  fitbit_class_deep  weighted  0.763955     0.018769     0.997070   \n",
       "72  fitbit_class_deep  weighted  0.763396     0.018769     0.996337   \n",
       "73  fitbit_class_deep  weighted  0.760604     0.000000     0.998535   \n",
       "74  fitbit_class_deep  weighted  0.761165     0.018687     0.993407   \n",
       "\n",
       "                                               config  \n",
       "0   {'lr': 1.1820409925225312e-05, 'hidden_size': ...  \n",
       "1   {'lr': 0.0001547622268491406, 'hidden_size': 1...  \n",
       "2   {'lr': 3.210643749578772e-05, 'hidden_size': 4...  \n",
       "3   {'lr': 3.7523058456721776e-05, 'hidden_size': ...  \n",
       "4   {'lr': 0.00011395742363510966, 'hidden_size': ...  \n",
       "..                                                ...  \n",
       "70  {'lr': 0.0001663799488249265, 'hidden_size': 1...  \n",
       "71  {'lr': 0.00017296898246593756, 'hidden_size': ...  \n",
       "72  {'lr': 0.00017847120939321913, 'hidden_size': ...  \n",
       "73  {'lr': 0.00018117713776052256, 'hidden_size': ...  \n",
       "74  {'lr': 0.00027424980105462916, 'hidden_size': ...  \n",
       "\n",
       "[75 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_fitbit_class = run_deep_search_fitbit_class(fitbit_class_dict, use_sample_weights=True, model_name=\"fitbit_class_deep\", n_trials=75)\n",
    "results_fitbit_class[\"mean_metrics\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84aa58a-8235-41f0-9b39-04a4c4d161bb",
   "metadata": {},
   "source": [
    "### Run Deep Search for Classification without weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "437465a6-a5b6-4345-938f-51f7c3fd87f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to search/comb_class_deep_nw_deep_search_results.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.764513</td>\n",
       "      <td>0.028153</td>\n",
       "      <td>0.994872</td>\n",
       "      <td>{'lr': 0.00016358797976479862, 'dropout': 0.14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.763396</td>\n",
       "      <td>0.009412</td>\n",
       "      <td>0.999267</td>\n",
       "      <td>{'lr': 5.8754396545751666e-05, 'dropout': 0.11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.761720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'lr': 4.1535039949794044e-05, 'dropout': 0.37...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.726548</td>\n",
       "      <td>0.161587</td>\n",
       "      <td>0.903297</td>\n",
       "      <td>{'lr': 0.0025937976958127404, 'dropout': 0.372...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.761720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'lr': 1.4795861732331276e-05, 'dropout': 0.23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.717634</td>\n",
       "      <td>0.189631</td>\n",
       "      <td>0.882784</td>\n",
       "      <td>{'lr': 0.0007061443580878221, 'dropout': 0.396...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.764510</td>\n",
       "      <td>0.011710</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'lr': 7.830546659454098e-05, 'dropout': 0.359...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.765074</td>\n",
       "      <td>0.046785</td>\n",
       "      <td>0.989744</td>\n",
       "      <td>{'lr': 0.00013777799219093285, 'dropout': 0.34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.765074</td>\n",
       "      <td>0.046785</td>\n",
       "      <td>0.989744</td>\n",
       "      <td>{'lr': 0.00013646870644887075, 'dropout': 0.34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>comb_class_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.760041</td>\n",
       "      <td>0.063201</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>{'lr': 0.00018961280916619558, 'dropout': 0.37...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              model          type  accuracy  sensitivity  specificity  \\\n",
       "0   comb_class_deep  not weighted  0.764513     0.028153     0.994872   \n",
       "1   comb_class_deep  not weighted  0.763396     0.009412     0.999267   \n",
       "2   comb_class_deep  not weighted  0.761720     0.000000     1.000000   \n",
       "3   comb_class_deep  not weighted  0.726548     0.161587     0.903297   \n",
       "4   comb_class_deep  not weighted  0.761720     0.000000     1.000000   \n",
       "..              ...           ...       ...          ...          ...   \n",
       "70  comb_class_deep  not weighted  0.717634     0.189631     0.882784   \n",
       "71  comb_class_deep  not weighted  0.764510     0.011710     1.000000   \n",
       "72  comb_class_deep  not weighted  0.765074     0.046785     0.989744   \n",
       "73  comb_class_deep  not weighted  0.765074     0.046785     0.989744   \n",
       "74  comb_class_deep  not weighted  0.760041     0.063201     0.978022   \n",
       "\n",
       "                                               config  \n",
       "0   {'lr': 0.00016358797976479862, 'dropout': 0.14...  \n",
       "1   {'lr': 5.8754396545751666e-05, 'dropout': 0.11...  \n",
       "2   {'lr': 4.1535039949794044e-05, 'dropout': 0.37...  \n",
       "3   {'lr': 0.0025937976958127404, 'dropout': 0.372...  \n",
       "4   {'lr': 1.4795861732331276e-05, 'dropout': 0.23...  \n",
       "..                                                ...  \n",
       "70  {'lr': 0.0007061443580878221, 'dropout': 0.396...  \n",
       "71  {'lr': 7.830546659454098e-05, 'dropout': 0.359...  \n",
       "72  {'lr': 0.00013777799219093285, 'dropout': 0.34...  \n",
       "73  {'lr': 0.00013646870644887075, 'dropout': 0.34...  \n",
       "74  {'lr': 0.00018961280916619558, 'dropout': 0.37...  \n",
       "\n",
       "[75 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_comb_class = run_deep_search_comb_class(comb_class_dict, use_sample_weights=False, model_name=\"comb_class_deep\", n_trials=75)\n",
    "results_comb_class[\"mean_metrics\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2de255de-12ee-4153-9f90-3cbec7eeec3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to search/fitbit_class_deep_nw_deep_search_results.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.756706</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.992674</td>\n",
       "      <td>{'lr': 7.922039201138631e-05, 'dropout': 0.405...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.761163</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.998535</td>\n",
       "      <td>{'lr': 0.00030929653350372435, 'dropout': 0.25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.756705</td>\n",
       "      <td>0.072640</td>\n",
       "      <td>0.970696</td>\n",
       "      <td>{'lr': 0.005983293892025289, 'dropout': 0.2982...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.638585</td>\n",
       "      <td>0.221122</td>\n",
       "      <td>0.768498</td>\n",
       "      <td>{'lr': 3.978021711839062e-05, 'dropout': 0.350...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.756697</td>\n",
       "      <td>0.023338</td>\n",
       "      <td>0.986081</td>\n",
       "      <td>{'lr': 0.008579417466902212, 'dropout': 0.4561...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.750012</td>\n",
       "      <td>0.063311</td>\n",
       "      <td>0.964835</td>\n",
       "      <td>{'lr': 0.0028354454973709667, 'dropout': 0.171...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.762837</td>\n",
       "      <td>0.025663</td>\n",
       "      <td>0.993407</td>\n",
       "      <td>{'lr': 0.0009601436128208786, 'dropout': 0.184...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.763395</td>\n",
       "      <td>0.025663</td>\n",
       "      <td>0.994139</td>\n",
       "      <td>{'lr': 0.0009763774609188299, 'dropout': 0.208...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.760608</td>\n",
       "      <td>0.014008</td>\n",
       "      <td>0.994139</td>\n",
       "      <td>{'lr': 0.0006862805508258147, 'dropout': 0.217...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>fitbit_class_deep</td>\n",
       "      <td>not weighted</td>\n",
       "      <td>0.491866</td>\n",
       "      <td>0.468974</td>\n",
       "      <td>0.498168</td>\n",
       "      <td>{'lr': 1.6261572218509496e-05, 'dropout': 0.13...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                model          type  accuracy  sensitivity  specificity  \\\n",
       "0   fitbit_class_deep  not weighted  0.756706     0.002326     0.992674   \n",
       "1   fitbit_class_deep  not weighted  0.761163     0.002326     0.998535   \n",
       "2   fitbit_class_deep  not weighted  0.756705     0.072640     0.970696   \n",
       "3   fitbit_class_deep  not weighted  0.638585     0.221122     0.768498   \n",
       "4   fitbit_class_deep  not weighted  0.756697     0.023338     0.986081   \n",
       "..                ...           ...       ...          ...          ...   \n",
       "70  fitbit_class_deep  not weighted  0.750012     0.063311     0.964835   \n",
       "71  fitbit_class_deep  not weighted  0.762837     0.025663     0.993407   \n",
       "72  fitbit_class_deep  not weighted  0.763395     0.025663     0.994139   \n",
       "73  fitbit_class_deep  not weighted  0.760608     0.014008     0.994139   \n",
       "74  fitbit_class_deep  not weighted  0.491866     0.468974     0.498168   \n",
       "\n",
       "                                               config  \n",
       "0   {'lr': 7.922039201138631e-05, 'dropout': 0.405...  \n",
       "1   {'lr': 0.00030929653350372435, 'dropout': 0.25...  \n",
       "2   {'lr': 0.005983293892025289, 'dropout': 0.2982...  \n",
       "3   {'lr': 3.978021711839062e-05, 'dropout': 0.350...  \n",
       "4   {'lr': 0.008579417466902212, 'dropout': 0.4561...  \n",
       "..                                                ...  \n",
       "70  {'lr': 0.0028354454973709667, 'dropout': 0.171...  \n",
       "71  {'lr': 0.0009601436128208786, 'dropout': 0.184...  \n",
       "72  {'lr': 0.0009763774609188299, 'dropout': 0.208...  \n",
       "73  {'lr': 0.0006862805508258147, 'dropout': 0.217...  \n",
       "74  {'lr': 1.6261572218509496e-05, 'dropout': 0.13...  \n",
       "\n",
       "[75 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_fitbit_class = run_deep_search_fitbit_class(fitbit_class_dict, use_sample_weights=False, model_name=\"fitbit_class_deep\", n_trials=75)\n",
    "results_fitbit_class[\"mean_metrics\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be062b-33dd-4a4f-ad2c-f7a7d2934393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL_py_env)",
   "language": "python",
   "name": "dl_py_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
