{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad7aab01-c799-40c3-bbdb-cfb2b14be896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda with 5 GPU(s)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from captum.attr import IntegratedGradients\n",
    "import math\n",
    "\n",
    "# Set deterministic behavior for CUDA (set before torch imports)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # for multi-GPU\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    return seed\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device} with {torch.cuda.device_count()} GPU(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647ec8de-bb3a-4b01-8729-1a8d2729050f",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e1bd439-3db4-4faa-8f12-435ff961441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the data directory as needed\n",
    "DL_DIR = \"../../data/deep_learning\"\n",
    "# Load the regression split dictionary.\n",
    "with open(f'{DL_DIR}/comb_reg_dict.pkl', 'rb') as f:\n",
    "    comb_reg_dict = pickle.load(f)\n",
    "\n",
    "with open(f'{DL_DIR}/fitbit_reg_dict.pkl', 'rb') as f:\n",
    "    fitbit_reg_dict = pickle.load(f)\n",
    "\n",
    "# Load the classification split dictionary.\n",
    "with open(f'{DL_DIR}/comb_class_dict.pkl', 'rb') as f:\n",
    "    comb_class_dict = pickle.load(f)\n",
    "\n",
    "with open(f'{DL_DIR}/fitbit_class_dict.pkl', 'rb') as f:\n",
    "    fitbit_class_dict = pickle.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07425ba7-e84c-44ab-86a9-a63fd7798cb1",
   "metadata": {},
   "source": [
    "### Best results Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d35fbd6-6a09-4056-948a-75512ed97d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights\n",
    "with open(\"search/fitbit_reg_deep_deep_search_results.pkl\", \"rb\") as f:\n",
    "    fitbit_reg_deep_results = pickle.load(f)\n",
    "\n",
    "with open(\"search/fitbit_class_deep_deep_search_results.pkl\", \"rb\") as f:\n",
    "    fitbit_class_deep_results = pickle.load(f)\n",
    "\n",
    "with open(\"search/comb_reg_deep_deep_search_results.pkl\", \"rb\") as f:\n",
    "    comb_reg_deep_results = pickle.load(f)\n",
    "\n",
    "with open(\"search/comb_class_deep_deep_search_results.pkl\", \"rb\") as f:\n",
    "    comb_class_deep_results = pickle.load(f)\n",
    "    \n",
    "    \n",
    "# No weights \n",
    "with open(\"search/fitbit_reg_deep_nw_deep_search_results.pkl\", \"rb\") as f:\n",
    "    fitbit_reg_deep_nw_results = pickle.load(f)\n",
    "\n",
    "with open(\"search/fitbit_class_deep_nw_deep_search_results.pkl\", \"rb\") as f:\n",
    "    fitbit_class_deep_nw_results = pickle.load(f)\n",
    "\n",
    "with open(\"search/comb_reg_deep_nw_deep_search_results.pkl\", \"rb\") as f:\n",
    "    comb_reg_deep_nw_results = pickle.load(f)\n",
    "\n",
    "with open(\"search/comb_class_deep_nw_deep_search_results.pkl\", \"rb\") as f:\n",
    "    comb_class_deep_nw_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba10f91-1acc-447d-a5c6-5ecb8b80ed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitbit_reg_deep_results[\"optimal_configuration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5453065f-da7b-427a-afcc-850270e17908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 0.23549431225782347,\n",
       " 'params': {'lr': 0.00031758480409139323,\n",
       "  'hidden_size': 64,\n",
       "  'dropout': 0.36701580916299,\n",
       "  'num_spochs': 9},\n",
       " 'user_attrs': {'overall_acc_mean': 0.7645056877421765,\n",
       "  'overall_acc_std': 0.006123973196714372,\n",
       "  'sensitivity_mean': 0.025690834473324216,\n",
       "  'sensitivity_std': 0.024846296825552,\n",
       "  'specificity_mean': 0.9956043956043956,\n",
       "  'specificity_std': 0.00146520146520146}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitbit_class_deep_results[\"optimal_configuration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90055ce1-e324-4877-9331-254b624dcf52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 0.7485472133582107,\n",
       " 'params': {'lr': 0.00012004813059897005,\n",
       "  'dropout': 0.2502347689046808,\n",
       "  'num_epochs': 10,\n",
       "  'hidden_size': 32},\n",
       " 'user_attrs': {'overall_mean_rmse': 0.7485472133582107,\n",
       "  'overall_std_rmse': 0.05045642676314084,\n",
       "  'bin_mean_rmse': {'1': 0.443451280136975,\n",
       "   '2': 0.9429971173114511,\n",
       "   '3': 1.9470383007780865,\n",
       "   '4': 2.7182801184909673,\n",
       "   '5': 3.3986684848700057},\n",
       "  'bin_std_rmse': {'1': 0.10593590884094498,\n",
       "   '2': 0.12376209254981475,\n",
       "   '3': 0.09850425797262995,\n",
       "   '4': 0.22652698806066127,\n",
       "   '5': 0.5179540533787148}}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_reg_deep_results[\"optimal_configuration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0dabcf8-74a9-4bf0-88d0-6e523ffdff8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 0.23325189461726392,\n",
       " 'params': {'lr': 0.00022022273601106883,\n",
       "  'dropout': 0.21531870315776552,\n",
       "  'num_epochs': 9,\n",
       "  'hidden_size': 64,\n",
       "  'bidirectional': False,\n",
       "  'batch_size': 32},\n",
       " 'user_attrs': {'overall_acc_mean': 0.7667481053827361,\n",
       "  'overall_acc_std': 0.007594274536308643,\n",
       "  'sensitivity_mean': 0.05381668946648427,\n",
       "  'sensitivity_std': 0.029951850938901757,\n",
       "  'specificity_mean': 0.9897435897435898,\n",
       "  'specificity_std': 0.010717024789983732}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_class_deep_results[\"optimal_configuration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "339abe19-6a4d-475e-b979-62133b08dd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 0.5998888609934407,\n",
       " 'params': {'lr': 0.00037998740411755944, 'n_fc_layers': 2},\n",
       " 'user_attrs': {'overall_mean_rmse': 0.5998888609934407,\n",
       "  'overall_std_rmse': 0.02228616656708193,\n",
       "  'bin_mean_rmse': {'1': 0.223655501024313,\n",
       "   '2': 0.73351722099294,\n",
       "   '3': 1.7294829336259823,\n",
       "   '4': 2.675533703296235,\n",
       "   '5': 3.6317181333612245},\n",
       "  'bin_std_rmse': {'1': 0.016845718857783316,\n",
       "   '2': 0.03813259400128359,\n",
       "   '3': 0.06170120346218208,\n",
       "   '4': 0.16016432137983694,\n",
       "   '5': 0.15675265134217586}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitbit_reg_deep_nw_results[\"optimal_configuration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71da5ce9-c8d4-45f4-8ffe-058247be0afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 0.23660384992452643,\n",
       " 'params': {'lr': 0.001000968792210794,\n",
       "  'dropout': 0.145910416042635,\n",
       "  'batch_size': 64},\n",
       " 'user_attrs': {'overall_acc_mean': 0.7633961500754736,\n",
       "  'overall_acc_std': 0.004619422075463298,\n",
       "  'sensitivity_mean': 0.02801641586867305,\n",
       "  'sensitivity_std': 0.03416429354628078,\n",
       "  'specificity_mean': 0.9934065934065934,\n",
       "  'specificity_std': 0.008157896502315062}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitbit_class_deep_nw_results[\"optimal_configuration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "223f822c-f83c-4f44-a069-83db34a779f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 0.597130227779869,\n",
       " 'params': {'lr': 0.00018275118830783695, 'dropout': 0.21206897079668371},\n",
       " 'user_attrs': {'overall_mean_rmse': 0.597130227779869,\n",
       "  'overall_std_rmse': 0.021255435755677465,\n",
       "  'bin_mean_rmse': {'1': 0.25564708002108094,\n",
       "   '2': 0.7021779488339193,\n",
       "   '3': 1.6803077536608448,\n",
       "   '4': 2.614225983320734,\n",
       "   '5': 3.6653054628431816},\n",
       "  'bin_std_rmse': {'1': 0.019683894381976715,\n",
       "   '2': 0.04929049610404701,\n",
       "   '3': 0.07332097460499346,\n",
       "   '4': 0.12892503562979357,\n",
       "   '5': 0.16052921753090277}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_reg_deep_nw_results[\"optimal_configuration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3429273e-4a71-49f9-ac9c-6d595d40e7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 0.23492631611708503,\n",
       " 'params': {'lr': 0.00013777799219093285,\n",
       "  'dropout': 0.3430940832899809,\n",
       "  'hidden_size': 96,\n",
       "  'n_layers': 2,\n",
       "  'bidirection': False},\n",
       " 'user_attrs': {'overall_acc_mean': 0.765073683882915,\n",
       "  'overall_acc_std': 0.0077639481426336355,\n",
       "  'sensitivity_mean': 0.046785225718194254,\n",
       "  'sensitivity_std': 0.023394612757608856,\n",
       "  'specificity_mean': 0.9897435897435898,\n",
       "  'specificity_std': 0.009091336004388911}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_class_deep_nw_results[\"optimal_configuration\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934e7d80-cbf2-4c21-ab32-559a5bd2c1e2",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec77271e-b7ed-46fb-a9c1-0350d78a2ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_output_length(L_in, kernel_size, stride, padding, dilation):\n",
    "    return (L_in + 2 * padding - dilation * (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "def pool_output_length(L_in, pool_kernel):\n",
    "    return L_in // pool_kernel  # assume stride equals kernel size\n",
    "\n",
    "def create_subject_dataset(df, outcome_col=\"SI_mean\", weighted=True):\n",
    "    \"\"\"\n",
    "    Build a subject-level dataset.\n",
    "    Each subject's predictors are arranged in a matrix of shape (n_features, timepoints).\n",
    "    The outcome is taken from the column specified by outcome_col.\n",
    "    A binning column is created for stratification.\n",
    "    \n",
    "    This updated version excludes the outcome_col from the predictors.\n",
    "    When weighted=False, sample weights are set to 1.0 regardless of a weight column.\n",
    "    \"\"\"\n",
    "    exclude_cols = [\"PatientID\", \"timepoints\", \"si_kde_weight\", \"SI_mean\", \"is_SI\", \"SI_level\"]\n",
    "    predictor_cols = [col for col in df.columns if col not in (exclude_cols + [outcome_col])]\n",
    "    \n",
    "    subject_data = []\n",
    "    for pid, group in df.groupby(\"PatientID\"):\n",
    "        group_sorted = group.sort_values(\"timepoints\")\n",
    "        X = group_sorted[predictor_cols].values.T  # shape: (n_features, timepoints)\n",
    "        y = group_sorted[outcome_col].iloc[0]\n",
    "        # If using weights and the weight column is present, use it; otherwise, set to 1.0.\n",
    "        if weighted and \"si_kde_weight\" in group.columns:\n",
    "            weight = group_sorted[\"si_kde_weight\"].iloc[0]\n",
    "        else:\n",
    "            weight = 1.0\n",
    "        record = {\"PatientID\": pid, \"X\": X, outcome_col: y, \"sample_weight\": weight}\n",
    "        subject_data.append(record)\n",
    "    subj_df = pd.DataFrame(subject_data)\n",
    "    subj_df[f\"{outcome_col}_bin\"] = np.round(subj_df[outcome_col]).astype(int)\n",
    "    return subj_df, predictor_cols\n",
    "\n",
    "def save_results_pickle(result_dict, model_name):\n",
    "    \"\"\"\n",
    "    Saves a pickle file with the results in a folder named \"search\".\n",
    "    The filename will be: <model_name>_results.pkl\n",
    "    \"\"\"\n",
    "    save_folder = \"search\"\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "    filename = os.path.join(save_folder, f\"{model_name}_results.pkl\")\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(result_dict, f)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "\n",
    "def compute_rmse(y_true, y_pred):\n",
    "    return math.sqrt(np.mean((y_pred - y_true)**2))\n",
    "\n",
    "def compute_regression_perf(y_true, y_pred):\n",
    "    perf = {}\n",
    "    perf[\"overall\"] = compute_rmse(y_true, y_pred)\n",
    "    levels = [1, 2, 3, 4, 5]\n",
    "    y_levels = np.round(y_true).astype(int)\n",
    "    for level in levels:\n",
    "        inds = np.where(y_levels == level)[0]\n",
    "        if len(inds) > 0:\n",
    "            perf[str(level)] = compute_rmse(y_true[inds], y_pred[inds])\n",
    "        else:\n",
    "            perf[str(level)] = np.nan\n",
    "    return perf\n",
    "\n",
    "def compute_classification_metrics(y_true, y_pred):\n",
    "    acc = np.mean(y_pred == y_true)\n",
    "    TP = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    FN = np.sum((y_pred == 0) & (y_true == 1))\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "    TN = np.sum((y_pred == 0) & (y_true == 0))\n",
    "    FP = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
    "    return acc, sensitivity, specificity\n",
    "\n",
    "\n",
    "def get_stratified_cv_splits(df, subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5):\n",
    "    \"\"\"\n",
    "    Performs stratified K-fold cross validation at the subject level.\n",
    "    \n",
    "    Parameters:\n",
    "      df : pandas.DataFrame\n",
    "          The original dataframe containing repeated measures.\n",
    "      subject_id : str\n",
    "          The column name for the subject ID (e.g., \"PatientID\").\n",
    "      target_var : str\n",
    "          The target variable; for regression use \"SI_mean\" and for classification use \"is_SI\".\n",
    "      n_splits : int\n",
    "          Number of folds for cross validation.\n",
    "    \n",
    "    Returns:\n",
    "      splits : list of tuples\n",
    "          A list where each element is a tuple (train_df, test_df) corresponding\n",
    "          to one fold. Each dataframe contains all rows (i.e. repeated measures) for the patients in that fold.\n",
    "    \n",
    "    Behavior:\n",
    "      - Isolates unique patient IDs and their target variable by dropping duplicates.\n",
    "      - If target_var is \"SI_mean\", creates a new column \"SI_mean_levels\" (rounded SI_mean).\n",
    "      - Uses the resulting column as the stratification column.\n",
    "      - Performs stratified K-fold CV and then subsets the original dataframe based on the patient IDs.\n",
    "    \"\"\"\n",
    "    # Create a subject-level dataframe (unique patient IDs with their target variable)\n",
    "    subject_df = df[[subject_id, target_var]].drop_duplicates(subset=[subject_id]).copy()\n",
    "    \n",
    "    # For regression: create a new column with the rounded SI_mean values.\n",
    "    if target_var == \"SI_mean\":\n",
    "        subject_df[\"SI_mean_levels\"] = subject_df[target_var].round().astype(int)\n",
    "        strat_col = \"SI_mean_levels\"\n",
    "    else:\n",
    "        strat_col = target_var  # For classification, use the target directly.\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    splits = []\n",
    "    \n",
    "    # Get the subject IDs and stratification labels\n",
    "    subjects = subject_df[subject_id].values\n",
    "    strat_labels = subject_df[strat_col].values\n",
    "    \n",
    "    # For each fold, retrieve patient IDs and then subset the original dataframe.\n",
    "    for train_idx, test_idx in skf.split(subjects, strat_labels):\n",
    "        train_patient_ids = subject_df.iloc[train_idx][subject_id].values\n",
    "        test_patient_ids  = subject_df.iloc[test_idx][subject_id].values\n",
    "        train_split = df[df[subject_id].isin(train_patient_ids)]\n",
    "        test_split  = df[df[subject_id].isin(test_patient_ids)]\n",
    "        splits.append((train_split, test_split))\n",
    "    \n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a922a4aa-9038-424c-8bc5-61126aa45ad9",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1766e61-90f9-4976-a8dc-cec521be9662",
   "metadata": {},
   "source": [
    "### Model Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b0cf3-d131-4dcc-861d-01b4dddbb808",
   "metadata": {},
   "source": [
    "Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f0a2a85-646a-4cef-a2ed-cb7ee79a95ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fitbit_regression_best(fitbit_reg_dict):\n",
    "    set_seed(42)\n",
    "    # Best parameters from deep search:\n",
    "    params = {\n",
    "        'batch_size': 16,\n",
    "        'num_epochs': 5,\n",
    "        'lr': 8.586725588917542e-05,\n",
    "        'use_regularization': False,  # not used here\n",
    "        'n_fc_layers': 3,\n",
    "        'dropout': 0.31991342041522364,\n",
    "        'bidirectional': True,\n",
    "        'hidden_size': 64  # chosen default for regression\n",
    "    }\n",
    "    # Create subject-level datasets.\n",
    "    train_df, predictor_cols = create_subject_dataset(fitbit_reg_dict['train'], outcome_col=\"SI_mean\")\n",
    "    test_df, _ = create_subject_dataset(fitbit_reg_dict['test'], outcome_col=\"SI_mean\")\n",
    "    \n",
    "    X_train = np.stack(train_df[\"X\"].values, axis=0)  # shape: (n_subjects, n_features, seq_len)\n",
    "    y_train = train_df[\"SI_mean\"].values.astype(np.float32)\n",
    "    sw_train = train_df[\"sample_weight\"].values.astype(np.float32)\n",
    "    \n",
    "    X_test = np.stack(test_df[\"X\"].values, axis=0)\n",
    "    y_test = test_df[\"SI_mean\"].values.astype(np.float32)\n",
    "    sw_test = test_df[\"sample_weight\"].values.astype(np.float32)\n",
    "    \n",
    "    n_subjects, input_channels, seq_len = X_train.shape\n",
    "\n",
    "    # Define the LSTM-based model inside the function.\n",
    "    class LocalFitRegLSTM(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, n_fc_layers, dropout, bidirectional):\n",
    "            super(LocalFitRegLSTM, self).__init__()\n",
    "            self.lstm = nn.LSTM(input_size=input_size,\n",
    "                                hidden_size=hidden_size,\n",
    "                                num_layers=1,\n",
    "                                batch_first=True,\n",
    "                                dropout=dropout,\n",
    "                                bidirectional=bidirectional)\n",
    "            fc_input = hidden_size * (2 if bidirectional else 1)\n",
    "            fc_layers = []\n",
    "            for _ in range(n_fc_layers):\n",
    "                fc_layers.append(nn.Linear(fc_input, 64))\n",
    "                fc_layers.append(nn.ReLU())\n",
    "                fc_layers.append(nn.Dropout(dropout))\n",
    "                fc_input = 64\n",
    "            fc_layers.append(nn.Linear(fc_input, 1))\n",
    "            self.fc_net = nn.Sequential(*fc_layers)\n",
    "        def forward(self, x):\n",
    "            # Input x shape: [batch, n_features, seq_len] -> transpose to [batch, seq_len, n_features]\n",
    "            x = x.permute(0, 2, 1)\n",
    "            out, _ = self.lstm(x)\n",
    "            out = out[:, -1, :]  # take last time step\n",
    "            out = self.fc_net(out)\n",
    "            return out\n",
    "\n",
    "    model = LocalFitRegLSTM(input_size=input_channels,\n",
    "                            hidden_size=params[\"hidden_size\"],\n",
    "                            n_fc_layers=params[\"n_fc_layers\"],\n",
    "                            dropout=params[\"dropout\"],\n",
    "                            bidirectional=params[\"bidirectional\"]).to(device)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "    loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).view(-1, 1),\n",
    "        torch.tensor(sw_train, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    test_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.float32),\n",
    "        torch.tensor(y_test, dtype=torch.float32).view(-1, 1),\n",
    "        torch.tensor(sw_test, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(params[\"num_epochs\"]):\n",
    "        for X_batch, y_batch, w_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            w_batch = w_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = (loss_fn(outputs, y_batch).view(-1) * w_batch.view(-1)).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def get_preds(loader):\n",
    "        preds, truths = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch, _ in loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                preds.append(outputs.cpu().numpy())\n",
    "                truths.append(y_batch.cpu().numpy())\n",
    "        preds = np.concatenate(preds).flatten()\n",
    "        truths = np.concatenate(truths).flatten()\n",
    "        return truths, preds\n",
    "\n",
    "    model.eval()\n",
    "    y_train_true, y_train_pred = get_preds(train_loader)\n",
    "    y_test_true, y_test_pred = get_preds(test_loader)\n",
    "    \n",
    "    perf_train = compute_regression_perf(y_train_true, y_train_pred)\n",
    "    perf_test  = compute_regression_perf(y_test_true, y_test_pred)\n",
    "    \n",
    "    perf_train_df = pd.DataFrame({\n",
    "        \"model\": [\"fitbit_regression\"],\n",
    "        \"type\": [\"train\"],\n",
    "        \"1\": [perf_train[\"1\"]],\n",
    "        \"2\": [perf_train[\"2\"]],\n",
    "        \"3\": [perf_train[\"3\"]],\n",
    "        \"4\": [perf_train[\"4\"]],\n",
    "        \"5\": [perf_train[\"5\"]],\n",
    "        \"overall\": [perf_train[\"overall\"]]\n",
    "    })\n",
    "    perf_test_df = pd.DataFrame({\n",
    "        \"model\": [\"fitbit_regression\"],\n",
    "        \"type\": [\"test\"],\n",
    "        \"1\": [perf_test[\"1\"]],\n",
    "        \"2\": [perf_test[\"2\"]],\n",
    "        \"3\": [perf_test[\"3\"]],\n",
    "        \"4\": [perf_test[\"4\"]],\n",
    "        \"5\": [perf_test[\"5\"]],\n",
    "        \"overall\": [perf_test[\"overall\"]]\n",
    "    })\n",
    "    performance_df = pd.concat([perf_train_df, perf_test_df], ignore_index=True)\n",
    "    \n",
    "    # Compute Integrated Gradients for SHAP.\n",
    "    set_seed(42)\n",
    "    model_for_attr = model.module if hasattr(model, 'module') else model\n",
    "    # Set the model to train mode to enable cuDNN LSTM backward.\n",
    "    model_for_attr.train()\n",
    "    ig = IntegratedGradients(model_for_attr)\n",
    "    input_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    baseline = torch.zeros_like(input_tensor)\n",
    "    attributions = ig.attribute(input_tensor, baselines=baseline, n_steps=50, internal_batch_size=8)\n",
    "    mean_attr = attributions.mean(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    sd_attr = attributions.abs().std(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    if len(predictor_cols) != len(mean_attr):\n",
    "        min_len = min(len(predictor_cols), len(mean_attr))\n",
    "        predictor_names = predictor_cols[:min_len]\n",
    "        mean_attr = mean_attr[:min_len]\n",
    "        sd_attr = sd_attr[:min_len]\n",
    "    else:\n",
    "        predictor_names = predictor_cols\n",
    "    shap_df = pd.DataFrame({\n",
    "        \"predictor\": predictor_names,\n",
    "        \"mean_abs_integrated_gradients\": mean_attr,\n",
    "        \"sd_abs_integrated_gradients\": sd_attr\n",
    "    })\n",
    "    \n",
    "    #################################################################\n",
    "    # CROSS-VALIDATION for Fitbit regression using LSTM\n",
    "    cv_splits = get_stratified_cv_splits(fitbit_reg_dict['train'], subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5)\n",
    "    cv_fold_metrics = []  # to store each fold's performance dictionary\n",
    "    \n",
    "    for fold, (cv_train_raw, cv_val_raw) in enumerate(cv_splits, start=1):\n",
    "        # Create subject-level datasets for this CV fold.\n",
    "        cv_train_df, _ = create_subject_dataset(cv_train_raw, outcome_col=\"SI_mean\")\n",
    "        cv_val_df, _ = create_subject_dataset(cv_val_raw, outcome_col=\"SI_mean\")\n",
    "        X_cv_train = np.stack(cv_train_df[\"X\"].values, axis=0)\n",
    "        y_cv_train = cv_train_df[\"SI_mean\"].values.astype(np.float32)\n",
    "        sw_cv_train = cv_train_df[\"sample_weight\"].values.astype(np.float32)\n",
    "        \n",
    "        X_cv_val = np.stack(cv_val_df[\"X\"].values, axis=0)\n",
    "        y_cv_val = cv_val_df[\"SI_mean\"].values.astype(np.float32)\n",
    "        sw_cv_val = cv_val_df[\"sample_weight\"].values.astype(np.float32)\n",
    "        \n",
    "        n_cv, input_channels_cv, seq_len_cv = X_cv_train.shape\n",
    "        \n",
    "        # Build a new model instance using the same architecture.\n",
    "        model_cv = LocalFitRegLSTM(input_size=input_channels_cv,\n",
    "                                   hidden_size=params[\"hidden_size\"],\n",
    "                                   n_fc_layers=params[\"n_fc_layers\"],\n",
    "                                   dropout=params[\"dropout\"],\n",
    "                                   bidirectional=params[\"bidirectional\"]).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model_cv = nn.DataParallel(model_cv)\n",
    "        \n",
    "        optimizer_cv = optim.Adam(model_cv.parameters(), lr=params[\"lr\"])\n",
    "        loss_fn_cv = nn.MSELoss(reduction=\"none\")\n",
    "        \n",
    "        train_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_train, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_train, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(sw_cv_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        val_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_val, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_val, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(sw_cv_val, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader_cv = torch.utils.data.DataLoader(train_dataset_cv, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "        val_loader_cv = torch.utils.data.DataLoader(val_dataset_cv, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "        \n",
    "        model_cv.train()\n",
    "        for epoch in range(params[\"num_epochs\"]):\n",
    "            for X_batch, y_batch, w_batch in train_loader_cv:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                w_batch = w_batch.to(device)\n",
    "                optimizer_cv.zero_grad()\n",
    "                outputs = model_cv(X_batch)\n",
    "                loss = (loss_fn_cv(outputs, y_batch).view(-1) * w_batch.view(-1)).mean()\n",
    "                loss.backward()\n",
    "                optimizer_cv.step()\n",
    "        \n",
    "        def get_preds_cv(loader):\n",
    "            preds, truths = [], []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch, _ in loader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    outputs = model_cv(X_batch)\n",
    "                    preds.append(outputs.cpu().numpy())\n",
    "                    truths.append(y_batch.cpu().numpy())\n",
    "            preds = np.concatenate(preds).flatten()\n",
    "            truths = np.concatenate(truths).flatten()\n",
    "            return truths, preds\n",
    "\n",
    "        model_cv.eval()\n",
    "        y_val_true_cv, y_val_pred_cv = get_preds_cv(val_loader_cv)\n",
    "        fold_perf = compute_regression_perf(y_val_true_cv, y_val_pred_cv)\n",
    "        # Compute overall MSE for this fold\n",
    "        fold_perf[\"mse\"] = np.mean((y_val_true_cv - y_val_pred_cv) ** 2)\n",
    "        cv_fold_metrics.append(fold_perf)\n",
    "    \n",
    "    # Aggregate CV metrics over folds.\n",
    "    keys = [\"1\", \"2\", \"3\", \"4\", \"5\", \"overall\", \"mse\"]\n",
    "    mean_metrics = {k: np.mean([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    sd_metrics   = {k: np.std([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    cv_val_df = pd.DataFrame({\n",
    "        \"stat\": [\"mean\", \"sd\"],\n",
    "        \"model\": [\"fitbit_regression\", \"fitbit_regression\"],\n",
    "        \"1\": [mean_metrics[\"1\"], sd_metrics[\"1\"]],\n",
    "        \"2\": [mean_metrics[\"2\"], sd_metrics[\"2\"]],\n",
    "        \"3\": [mean_metrics[\"3\"], sd_metrics[\"3\"]],\n",
    "        \"4\": [mean_metrics[\"4\"], sd_metrics[\"4\"]],\n",
    "        \"5\": [mean_metrics[\"5\"], sd_metrics[\"5\"]],\n",
    "        \"overall\": [mean_metrics[\"overall\"], sd_metrics[\"overall\"]],\n",
    "        \"mse\": [mean_metrics[\"mse\"], sd_metrics[\"mse\"]]\n",
    "    })\n",
    "    \n",
    "    return performance_df, shap_df, cv_val_df\n",
    "\n",
    "def run_fitbit_classification_best(fitbit_dict):\n",
    "    set_seed(42)\n",
    "    # Best parameters from deep search:\n",
    "    params = {\n",
    "        'lr': 0.00031758480409139323,\n",
    "        'hidden_size': 64,\n",
    "        'dropout': 0.36701580916299,\n",
    "        'num_epochs': 9  # using provided num_epochs as epochs\n",
    "        # Batch size remains 32 as before.\n",
    "    }\n",
    "    train_df, predictor_cols = create_subject_dataset(fitbit_dict['train'], outcome_col=\"is_SI\")\n",
    "    test_df, _ = create_subject_dataset(fitbit_dict['test'], outcome_col=\"is_SI\")\n",
    "    \n",
    "    X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "    y_train = train_df[\"is_SI\"].values.astype(np.float32)\n",
    "    X_test = np.stack(test_df[\"X\"].values, axis=0)\n",
    "    y_test = test_df[\"is_SI\"].values.astype(np.float32)\n",
    "    \n",
    "    n_subjects, input_channels, seq_len = X_train.shape\n",
    "\n",
    "    # Define the local LSTM classifier.\n",
    "    class LocalFitbitClassLSTM(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, dropout):\n",
    "            super(LocalFitbitClassLSTM, self).__init__()\n",
    "            self.lstm = nn.LSTM(input_size=input_size,\n",
    "                                hidden_size=hidden_size,\n",
    "                                num_layers=1,\n",
    "                                batch_first=True,\n",
    "                                dropout=dropout,\n",
    "                                bidirectional=False)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "        def forward(self, x):\n",
    "            x = x.permute(0, 2, 1)\n",
    "            out, _ = self.lstm(x)\n",
    "            out = out[:, -1, :]\n",
    "            out = self.dropout(out)\n",
    "            out = self.fc(out)\n",
    "            return out\n",
    "\n",
    "    model = LocalFitbitClassLSTM(input_size=input_channels,\n",
    "                                 hidden_size=params[\"hidden_size\"],\n",
    "                                 dropout=params[\"dropout\"]).to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    num_epochs = params[\"num_epochs\"]\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    test_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.float32),\n",
    "        torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def get_preds(loader):\n",
    "        preds, truths = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "                truths.append(y_batch.cpu().numpy())\n",
    "        preds = np.concatenate(preds).flatten()\n",
    "        truths = np.concatenate(truths).flatten()\n",
    "        return truths, preds\n",
    "\n",
    "    model.eval()\n",
    "    y_train_true, y_train_pred = get_preds(train_loader)\n",
    "    y_test_true, y_test_pred = get_preds(test_loader)\n",
    "    \n",
    "    # Compute classification metrics (using AUC along with accuracy, sensitivity, specificity)\n",
    "    train_auc = roc_auc_score(y_train_true, y_train_pred)\n",
    "    test_auc = roc_auc_score(y_test_true, y_test_pred)\n",
    "    y_train_bin = (y_train_pred >= 0.5).astype(np.float32)\n",
    "    y_test_bin = (y_test_pred >= 0.5).astype(np.float32)\n",
    "    TP_train = np.sum((y_train_bin == 1) & (y_train_true == 1))\n",
    "    FN_train = np.sum((y_train_bin == 0) & (y_train_true == 1))\n",
    "    TN_train = np.sum((y_train_bin == 0) & (y_train_true == 0))\n",
    "    FP_train = np.sum((y_train_bin == 1) & (y_train_true == 0))\n",
    "    train_acc = np.mean(y_train_bin == y_train_true)\n",
    "    train_sens = TP_train / (TP_train + FN_train) if (TP_train + FN_train) > 0 else np.nan\n",
    "    train_spec = TN_train / (TN_train + FP_train) if (TN_train + FP_train) > 0 else np.nan\n",
    "\n",
    "    TP_test = np.sum((y_test_bin == 1) & (y_test_true == 1))\n",
    "    FN_test = np.sum((y_test_bin == 0) & (y_test_true == 1))\n",
    "    TN_test = np.sum((y_test_bin == 0) & (y_test_true == 0))\n",
    "    FP_test = np.sum((y_test_bin == 1) & (y_test_true == 0))\n",
    "    test_acc = np.mean(y_test_bin == y_test_true)\n",
    "    test_sens = TP_test / (TP_test + FN_test) if (TP_test + FN_test) > 0 else np.nan\n",
    "    test_spec = TN_test / (TN_test + FP_test) if (TN_test + FP_test) > 0 else np.nan\n",
    "    \n",
    "    perf_train_df = pd.DataFrame({\n",
    "        \"model\": [\"fitbit_classification\"],\n",
    "        \"type\": [\"train\"],\n",
    "        \"AUC\": [train_auc],\n",
    "        \"accuracy\": [train_acc],\n",
    "        \"sensitivity\": [train_sens],\n",
    "        \"specificity\": [train_spec]\n",
    "    })\n",
    "    perf_test_df = pd.DataFrame({\n",
    "        \"model\": [\"fitbit_classification\"],\n",
    "        \"type\": [\"test\"],\n",
    "        \"AUC\": [test_auc],\n",
    "        \"accuracy\": [test_acc],\n",
    "        \"sensitivity\": [test_sens],\n",
    "        \"specificity\": [test_spec]\n",
    "    })\n",
    "    performance_df = pd.concat([perf_train_df, perf_test_df], ignore_index=True)\n",
    "    \n",
    "    # Compute Integrated Gradients for SHAP.\n",
    "    set_seed(42)\n",
    "    model_for_attr = model.module if hasattr(model, 'module') else model\n",
    "    model_for_attr.train()\n",
    "    ig = IntegratedGradients(model_for_attr)\n",
    "    input_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    baseline = torch.zeros_like(input_tensor)\n",
    "    attributions = ig.attribute(input_tensor, baselines=baseline, n_steps=50, internal_batch_size=8)\n",
    "    mean_attr = attributions.mean(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    sd_attr = attributions.abs().std(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    if len(predictor_cols) != len(mean_attr):\n",
    "        min_len = min(len(predictor_cols), len(mean_attr))\n",
    "        predictor_names = predictor_cols[:min_len]\n",
    "        mean_attr = mean_attr[:min_len]\n",
    "        sd_attr = sd_attr[:min_len]\n",
    "    else:\n",
    "        predictor_names = predictor_cols\n",
    "    shap_df = pd.DataFrame({\n",
    "        \"predictor\": predictor_names,\n",
    "        \"mean_abs_integrated_gradients\": mean_attr,\n",
    "        \"sd_abs_integrated_gradients\": sd_attr\n",
    "    })\n",
    "    \n",
    "    #################################################################\n",
    "    # CROSS-VALIDATION for Fitbit classification using LSTM\n",
    "    cv_splits = get_stratified_cv_splits(fitbit_dict['train'], subject_id=\"PatientID\", target_var=\"is_SI\", n_splits=5)\n",
    "    cv_fold_metrics = []\n",
    "    \n",
    "    for fold, (cv_train_raw, cv_val_raw) in enumerate(cv_splits, start=1):\n",
    "        cv_train_df, _ = create_subject_dataset(cv_train_raw, outcome_col=\"is_SI\")\n",
    "        cv_val_df, _ = create_subject_dataset(cv_val_raw, outcome_col=\"is_SI\")\n",
    "        X_cv_train = np.stack(cv_train_df[\"X\"].values, axis=0)\n",
    "        y_cv_train = cv_train_df[\"is_SI\"].values.astype(np.float32)\n",
    "        X_cv_val = np.stack(cv_val_df[\"X\"].values, axis=0)\n",
    "        y_cv_val = cv_val_df[\"is_SI\"].values.astype(np.float32)\n",
    "        \n",
    "        n_cv, input_channels_cv, seq_len_cv = X_cv_train.shape\n",
    "        \n",
    "        model_cv = LocalFitbitClassLSTM(input_size=input_channels_cv,\n",
    "                                         hidden_size=params[\"hidden_size\"],\n",
    "                                         dropout=params[\"dropout\"]).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model_cv = nn.DataParallel(model_cv)\n",
    "        \n",
    "        optimizer_cv = optim.Adam(model_cv.parameters(), lr=params[\"lr\"])\n",
    "        loss_fn_cv = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        train_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_train, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        val_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_val, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_val, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader_cv = torch.utils.data.DataLoader(train_dataset_cv, batch_size=32, shuffle=True)\n",
    "        val_loader_cv = torch.utils.data.DataLoader(val_dataset_cv, batch_size=32, shuffle=False)\n",
    "        \n",
    "        model_cv.train()\n",
    "        for epoch in range(params[\"num_epochs\"]):\n",
    "            for X_batch, y_batch in train_loader_cv:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                optimizer_cv.zero_grad()\n",
    "                outputs = model_cv(X_batch)\n",
    "                loss = loss_fn_cv(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer_cv.step()\n",
    "        \n",
    "        def get_preds_cv(loader):\n",
    "            preds, truths = [], []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in loader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    outputs = model_cv(X_batch)\n",
    "                    preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "                    truths.append(y_batch.cpu().numpy())\n",
    "            preds = np.concatenate(preds).flatten()\n",
    "            truths = np.concatenate(truths).flatten()\n",
    "            return truths, preds\n",
    "        \n",
    "        model_cv.eval()\n",
    "        y_val_true_cv, y_val_pred_cv = get_preds_cv(val_loader_cv)\n",
    "        auc_cv = roc_auc_score(y_val_true_cv, y_val_pred_cv)\n",
    "        y_val_bin = (y_val_pred_cv >= 0.5).astype(np.float32)\n",
    "        TP = np.sum((y_val_bin == 1) & (y_val_true_cv == 1))\n",
    "        FN = np.sum((y_val_bin == 0) & (y_val_true_cv == 1))\n",
    "        TN = np.sum((y_val_bin == 0) & (y_val_true_cv == 0))\n",
    "        FP = np.sum((y_val_bin == 1) & (y_val_true_cv == 0))\n",
    "        acc = np.mean(y_val_bin == y_val_true_cv)\n",
    "        sens = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "        spec = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
    "        # Compute binary cross entropy for the fold with a small epsilon for stability.\n",
    "        epsilon = 1e-7\n",
    "        bse = -np.mean(\n",
    "            y_val_true_cv * np.log(y_val_pred_cv + epsilon) +\n",
    "            (1 - y_val_true_cv) * np.log(1 - y_val_pred_cv + epsilon)\n",
    "        )\n",
    "        cv_fold_metrics.append({\"AUC\": auc_cv, \"accuracy\": acc, \"sensitivity\": sens, \"specificity\": spec, \"bse\": bse})\n",
    "    \n",
    "    keys = [\"AUC\", \"accuracy\", \"sensitivity\", \"specificity\", \"bse\"]\n",
    "    mean_metrics = {k: np.mean([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    sd_metrics = {k: np.std([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    cv_val_df = pd.DataFrame({\n",
    "        \"stat\": [\"mean\", \"sd\"],\n",
    "        \"model\": [\"fitbit_classification\", \"fitbit_classification\"],\n",
    "        \"AUC\": [mean_metrics[\"AUC\"], sd_metrics[\"AUC\"]],\n",
    "        \"accuracy\": [mean_metrics[\"accuracy\"], sd_metrics[\"accuracy\"]],\n",
    "        \"sensitivity\": [mean_metrics[\"sensitivity\"], sd_metrics[\"sensitivity\"]],\n",
    "        \"specificity\": [mean_metrics[\"specificity\"], sd_metrics[\"specificity\"]],\n",
    "        \"bse\": [mean_metrics[\"bse\"], sd_metrics[\"bse\"]]\n",
    "    })\n",
    "    \n",
    "    return performance_df, shap_df, cv_val_df\n",
    "\n",
    "def run_comb_regression_best(comb_dict):\n",
    "    set_seed(42)\n",
    "    # Best parameters from deep search:\n",
    "    params = {\n",
    "        'lr': 0.00012004813059897005,\n",
    "        'dropout': 0.2502347689046808,\n",
    "        'num_epochs': 10,\n",
    "        'hidden_size': 32\n",
    "    }\n",
    "    train_df, predictor_cols = create_subject_dataset(comb_dict['train'], outcome_col=\"SI_mean\")\n",
    "    test_df, _ = create_subject_dataset(comb_dict['test'], outcome_col=\"SI_mean\")\n",
    "    \n",
    "    X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "    y_train = train_df[\"SI_mean\"].values.astype(np.float32)\n",
    "    sw_train = train_df[\"sample_weight\"].values.astype(np.float32)\n",
    "    \n",
    "    X_test = np.stack(test_df[\"X\"].values, axis=0)\n",
    "    y_test = test_df[\"SI_mean\"].values.astype(np.float32)\n",
    "    sw_test = test_df[\"sample_weight\"].values.astype(np.float32)\n",
    "    \n",
    "    n_subjects, input_channels, seq_len = X_train.shape\n",
    "\n",
    "    # Define the local LSTM model for comb regression.\n",
    "    class LocalCombRegLSTM(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, dropout):\n",
    "            super(LocalCombRegLSTM, self).__init__()\n",
    "            self.lstm = nn.LSTM(input_size=input_size,\n",
    "                                hidden_size=hidden_size,\n",
    "                                num_layers=1,\n",
    "                                batch_first=True,\n",
    "                                dropout=dropout,\n",
    "                                bidirectional=False)\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "        def forward(self, x):\n",
    "            x = x.permute(0, 2, 1)\n",
    "            out, _ = self.lstm(x)\n",
    "            out = out[:, -1, :]\n",
    "            out = self.fc(out)\n",
    "            return out\n",
    "\n",
    "    model = LocalCombRegLSTM(input_size=input_channels,\n",
    "                             hidden_size=params[\"hidden_size\"],\n",
    "                             dropout=params[\"dropout\"]).to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "    loss_fn = nn.MSELoss(reduction='none')\n",
    "    num_epochs = params[\"num_epochs\"]\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).view(-1, 1),\n",
    "        torch.tensor(sw_train, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    test_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.float32),\n",
    "        torch.tensor(y_test, dtype=torch.float32).view(-1, 1),\n",
    "        torch.tensor(sw_test, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for X_batch, y_batch, w_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            w_batch = w_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = (loss_fn(outputs, y_batch).view(-1) * w_batch.view(-1)).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def get_preds(loader):\n",
    "        preds, truths = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch, _ in loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                preds.append(outputs.cpu().numpy())\n",
    "                truths.append(y_batch.cpu().numpy())\n",
    "        preds = np.concatenate(preds).flatten()\n",
    "        truths = np.concatenate(truths).flatten()\n",
    "        return truths, preds\n",
    "\n",
    "    model.eval()\n",
    "    y_train_true, y_train_pred = get_preds(train_loader)\n",
    "    y_test_true, y_test_pred = get_preds(test_loader)\n",
    "    \n",
    "    perf_train = compute_regression_perf(y_train_true, y_train_pred)\n",
    "    perf_test  = compute_regression_perf(y_test_true, y_test_pred)\n",
    "    \n",
    "    perf_train_df = pd.DataFrame({\n",
    "        \"model\": [\"comb_regression\"],\n",
    "        \"type\": [\"train\"],\n",
    "        \"1\": [perf_train[\"1\"]],\n",
    "        \"2\": [perf_train[\"2\"]],\n",
    "        \"3\": [perf_train[\"3\"]],\n",
    "        \"4\": [perf_train[\"4\"]],\n",
    "        \"5\": [perf_train[\"5\"]],\n",
    "        \"overall\": [perf_train[\"overall\"]]\n",
    "    })\n",
    "    perf_test_df = pd.DataFrame({\n",
    "        \"model\": [\"comb_regression\"],\n",
    "        \"type\": [\"test\"],\n",
    "        \"1\": [perf_test[\"1\"]],\n",
    "        \"2\": [perf_test[\"2\"]],\n",
    "        \"3\": [perf_test[\"3\"]],\n",
    "        \"4\": [perf_test[\"4\"]],\n",
    "        \"5\": [perf_test[\"5\"]],\n",
    "        \"overall\": [perf_test[\"overall\"]]\n",
    "    })\n",
    "    performance_df = pd.concat([perf_train_df, perf_test_df], ignore_index=True)\n",
    "    \n",
    "    # Compute Integrated Gradients for SHAP.\n",
    "    set_seed(42)\n",
    "    model_for_attr = model.module if hasattr(model, 'module') else model\n",
    "    model_for_attr.train()\n",
    "    ig = IntegratedGradients(model_for_attr)\n",
    "    input_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    baseline = torch.zeros_like(input_tensor)\n",
    "    attributions = ig.attribute(input_tensor, baselines=baseline, n_steps=50, internal_batch_size=8)\n",
    "    mean_attr = attributions.mean(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    sd_attr = attributions.abs().std(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    if len(predictor_cols) != len(mean_attr):\n",
    "        min_len = min(len(predictor_cols), len(mean_attr))\n",
    "        predictor_names = predictor_cols[:min_len]\n",
    "        mean_attr = mean_attr[:min_len]\n",
    "        sd_attr = sd_attr[:min_len]\n",
    "    else:\n",
    "        predictor_names = predictor_cols\n",
    "    shap_df = pd.DataFrame({\n",
    "        \"predictor\": predictor_names,\n",
    "        \"mean_abs_integrated_gradients\": mean_attr,\n",
    "        \"sd_abs_integrated_gradients\": sd_attr\n",
    "    })\n",
    "    \n",
    "    #################################################################\n",
    "    # CROSS-VALIDATION for comb regression using LSTM\n",
    "    cv_splits = get_stratified_cv_splits(comb_dict['train'], subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5)\n",
    "    cv_fold_metrics = []\n",
    "    \n",
    "    for fold, (cv_train_raw, cv_val_raw) in enumerate(cv_splits, start=1):\n",
    "        cv_train_df, _ = create_subject_dataset(cv_train_raw, outcome_col=\"SI_mean\")\n",
    "        cv_val_df, _ = create_subject_dataset(cv_val_raw, outcome_col=\"SI_mean\")\n",
    "        X_cv_train = np.stack(cv_train_df[\"X\"].values, axis=0)\n",
    "        y_cv_train = cv_train_df[\"SI_mean\"].values.astype(np.float32)\n",
    "        sw_cv_train = cv_train_df[\"sample_weight\"].values.astype(np.float32)\n",
    "        \n",
    "        X_cv_val = np.stack(cv_val_df[\"X\"].values, axis=0)\n",
    "        y_cv_val = cv_val_df[\"SI_mean\"].values.astype(np.float32)\n",
    "        sw_cv_val = cv_val_df[\"sample_weight\"].values.astype(np.float32)\n",
    "        \n",
    "        n_cv, input_channels_cv, seq_len_cv = X_cv_train.shape\n",
    "        \n",
    "        model_cv = LocalCombRegLSTM(input_size=input_channels_cv,\n",
    "                                    hidden_size=params[\"hidden_size\"],\n",
    "                                    dropout=params[\"dropout\"]).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model_cv = nn.DataParallel(model_cv)\n",
    "        \n",
    "        optimizer_cv = optim.Adam(model_cv.parameters(), lr=params[\"lr\"])\n",
    "        loss_fn_cv = nn.MSELoss(reduction='none')\n",
    "        \n",
    "        train_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_train, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_train, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(sw_cv_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        val_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_val, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_val, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(sw_cv_val, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader_cv = torch.utils.data.DataLoader(train_dataset_cv, batch_size=32, shuffle=True)\n",
    "        val_loader_cv = torch.utils.data.DataLoader(val_dataset_cv, batch_size=32, shuffle=False)\n",
    "        \n",
    "        model_cv.train()\n",
    "        for epoch in range(params[\"num_epochs\"]):\n",
    "            for X_batch, y_batch, w_batch in train_loader_cv:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                w_batch = w_batch.to(device)\n",
    "                optimizer_cv.zero_grad()\n",
    "                outputs = model_cv(X_batch)\n",
    "                loss = (loss_fn_cv(outputs, y_batch).view(-1) * w_batch.view(-1)).mean()\n",
    "                loss.backward()\n",
    "                optimizer_cv.step()\n",
    "        \n",
    "        def get_preds_cv(loader):\n",
    "            preds, truths = [], []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch, _ in loader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    outputs = model_cv(X_batch)\n",
    "                    preds.append(outputs.cpu().numpy())\n",
    "                    truths.append(y_batch.cpu().numpy())\n",
    "            preds = np.concatenate(preds).flatten()\n",
    "            truths = np.concatenate(truths).flatten()\n",
    "            return truths, preds\n",
    "\n",
    "        model_cv.eval()\n",
    "        y_val_true_cv, y_val_pred_cv = get_preds_cv(val_loader_cv)\n",
    "        fold_perf = compute_regression_perf(y_val_true_cv, y_val_pred_cv)\n",
    "        # Compute overall MSE for this fold.\n",
    "        fold_perf[\"mse\"] = np.mean((y_val_true_cv - y_val_pred_cv) ** 2)\n",
    "        cv_fold_metrics.append(fold_perf)\n",
    "    \n",
    "    keys = [\"1\", \"2\", \"3\", \"4\", \"5\", \"overall\", \"mse\"]\n",
    "    mean_metrics = {k: np.mean([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    sd_metrics   = {k: np.std([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    cv_val_df = pd.DataFrame({\n",
    "        \"stat\": [\"mean\", \"sd\"],\n",
    "        \"model\": [\"comb_regression\", \"comb_regression\"],\n",
    "        \"1\": [mean_metrics[\"1\"], sd_metrics[\"1\"]],\n",
    "        \"2\": [mean_metrics[\"2\"], sd_metrics[\"2\"]],\n",
    "        \"3\": [mean_metrics[\"3\"], sd_metrics[\"3\"]],\n",
    "        \"4\": [mean_metrics[\"4\"], sd_metrics[\"4\"]],\n",
    "        \"5\": [mean_metrics[\"5\"], sd_metrics[\"5\"]],\n",
    "        \"overall\": [mean_metrics[\"overall\"], sd_metrics[\"overall\"]],\n",
    "        \"mse\": [mean_metrics[\"mse\"], sd_metrics[\"mse\"]]\n",
    "    })\n",
    "    \n",
    "    return performance_df, shap_df, cv_val_df\n",
    "\n",
    "def run_comb_classification_best(comb_dict):\n",
    "    set_seed(42)\n",
    "    # Best parameters from deep search:\n",
    "    params = {\n",
    "        'lr': 0.00022022273601106883,\n",
    "        'dropout': 0.21531870315776552,\n",
    "        'num_epochs': 9,\n",
    "        'hidden_size': 64,\n",
    "        'bidirectional': False,\n",
    "        'batch_size': 32\n",
    "    }\n",
    "    train_df, predictor_cols = create_subject_dataset(comb_dict['train'], outcome_col=\"is_SI\")\n",
    "    test_df, _ = create_subject_dataset(comb_dict['test'], outcome_col=\"is_SI\")\n",
    "    \n",
    "    X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "    y_train = train_df[\"is_SI\"].values.astype(np.float32)\n",
    "    X_test = np.stack(test_df[\"X\"].values, axis=0)\n",
    "    y_test = test_df[\"is_SI\"].values.astype(np.float32)\n",
    "    \n",
    "    n_subjects, input_channels, seq_len = X_train.shape\n",
    "\n",
    "    # Define the local LSTM classifier for comb classification.\n",
    "    class LocalCombClassLSTM(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, dropout, bidirectional):\n",
    "            super(LocalCombClassLSTM, self).__init__()\n",
    "            self.lstm = nn.LSTM(input_size=input_size,\n",
    "                                hidden_size=hidden_size,\n",
    "                                num_layers=1,\n",
    "                                batch_first=True,\n",
    "                                dropout=dropout,\n",
    "                                bidirectional=bidirectional)\n",
    "            fc_input = hidden_size * (2 if bidirectional else 1)\n",
    "            self.fc = nn.Linear(fc_input, 1)\n",
    "        def forward(self, x):\n",
    "            x = x.permute(0, 2, 1)\n",
    "            out, _ = self.lstm(x)\n",
    "            out = out[:, -1, :]\n",
    "            out = self.fc(out)\n",
    "            return out\n",
    "\n",
    "    model = LocalCombClassLSTM(input_size=input_channels,\n",
    "                               hidden_size=params[\"hidden_size\"],\n",
    "                               dropout=params[\"dropout\"],\n",
    "                               bidirectional=params[\"bidirectional\"]).to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    num_epochs = params[\"num_epochs\"]\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    test_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.float32),\n",
    "        torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def get_preds(loader):\n",
    "        preds, truths = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "                truths.append(y_batch.cpu().numpy())\n",
    "        preds = np.concatenate(preds).flatten()\n",
    "        truths = np.concatenate(truths).flatten()\n",
    "        return truths, preds\n",
    "\n",
    "    model.eval()\n",
    "    y_train_true, y_train_pred = get_preds(train_loader)\n",
    "    y_test_true, y_test_pred = get_preds(test_loader)\n",
    "    \n",
    "    train_auc = roc_auc_score(y_train_true, y_train_pred)\n",
    "    test_auc = roc_auc_score(y_test_true, y_test_pred)\n",
    "    y_train_bin = (y_train_pred >= 0.5).astype(np.float32)\n",
    "    y_test_bin = (y_test_pred >= 0.5).astype(np.float32)\n",
    "    TP_train = np.sum((y_train_bin == 1) & (y_train_true == 1))\n",
    "    FN_train = np.sum((y_train_bin == 0) & (y_train_true == 1))\n",
    "    TN_train = np.sum((y_train_bin == 0) & (y_train_true == 0))\n",
    "    FP_train = np.sum((y_train_bin == 1) & (y_train_true == 0))\n",
    "    train_acc = np.mean(y_train_bin == y_train_true)\n",
    "    train_sens = TP_train / (TP_train + FN_train) if (TP_train + FN_train) > 0 else np.nan\n",
    "    train_spec = TN_train / (TN_train + FP_train) if (TN_train + FP_train) > 0 else np.nan\n",
    "\n",
    "    TP_test = np.sum((y_test_bin == 1) & (y_test_true == 1))\n",
    "    FN_test = np.sum((y_test_bin == 0) & (y_test_true == 1))\n",
    "    TN_test = np.sum((y_test_bin == 0) & (y_test_true == 0))\n",
    "    FP_test = np.sum((y_test_bin == 1) & (y_test_true == 0))\n",
    "    test_acc = np.mean(y_test_bin == y_test_true)\n",
    "    test_sens = TP_test / (TP_test + FN_test) if (TP_test + FN_test) > 0 else np.nan\n",
    "    test_spec = TN_test / (TN_test + FP_test) if (TN_test + FP_test) > 0 else np.nan\n",
    "    \n",
    "    perf_train_df = pd.DataFrame({\n",
    "        \"model\": [\"comb_classification\"],\n",
    "        \"type\": [\"train\"],\n",
    "        \"AUC\": [train_auc],\n",
    "        \"accuracy\": [train_acc],\n",
    "        \"sensitivity\": [train_sens],\n",
    "        \"specificity\": [train_spec]\n",
    "    })\n",
    "    perf_test_df = pd.DataFrame({\n",
    "        \"model\": [\"comb_classification\"],\n",
    "        \"type\": [\"test\"],\n",
    "        \"AUC\": [test_auc],\n",
    "        \"accuracy\": [test_acc],\n",
    "        \"sensitivity\": [test_sens],\n",
    "        \"specificity\": [test_spec]\n",
    "    })\n",
    "    performance_df = pd.concat([perf_train_df, perf_test_df], ignore_index=True)\n",
    "    \n",
    "    # Compute Integrated Gradients for SHAP.\n",
    "    set_seed(42)\n",
    "    model_for_attr = model.module if hasattr(model, 'module') else model\n",
    "    model_for_attr.train()\n",
    "    ig = IntegratedGradients(model_for_attr)\n",
    "    input_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    baseline = torch.zeros_like(input_tensor)\n",
    "    attributions = ig.attribute(input_tensor, baselines=baseline, n_steps=50, internal_batch_size=8)\n",
    "    mean_attr = attributions.mean(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    sd_attr = attributions.abs().std(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    if len(predictor_cols) != len(mean_attr):\n",
    "        min_len = min(len(predictor_cols), len(mean_attr))\n",
    "        predictor_names = predictor_cols[:min_len]\n",
    "        mean_attr = mean_attr[:min_len]\n",
    "        sd_attr = sd_attr[:min_len]\n",
    "    else:\n",
    "        predictor_names = predictor_cols\n",
    "    shap_df = pd.DataFrame({\n",
    "        \"predictor\": predictor_names,\n",
    "        \"mean_abs_integrated_gradients\": mean_attr,\n",
    "        \"sd_abs_integrated_gradients\": sd_attr\n",
    "    })\n",
    "    \n",
    "    #################################################################\n",
    "    # CROSS-VALIDATION for comb classification using LSTM\n",
    "    cv_splits = get_stratified_cv_splits(comb_dict['train'], subject_id=\"PatientID\", target_var=\"is_SI\", n_splits=5)\n",
    "    cv_fold_metrics = []\n",
    "    \n",
    "    for fold, (cv_train_raw, cv_val_raw) in enumerate(cv_splits, start=1):\n",
    "        cv_train_df, _ = create_subject_dataset(cv_train_raw, outcome_col=\"is_SI\")\n",
    "        cv_val_df, _ = create_subject_dataset(cv_val_raw, outcome_col=\"is_SI\")\n",
    "        X_cv_train = np.stack(cv_train_df[\"X\"].values, axis=0)\n",
    "        y_cv_train = cv_train_df[\"is_SI\"].values.astype(np.float32)\n",
    "        X_cv_val = np.stack(cv_val_df[\"X\"].values, axis=0)\n",
    "        y_cv_val = cv_val_df[\"is_SI\"].values.astype(np.float32)\n",
    "        \n",
    "        n_cv, input_channels_cv, seq_len_cv = X_cv_train.shape\n",
    "        \n",
    "        model_cv = LocalCombClassLSTM(input_size=input_channels_cv,\n",
    "                                      hidden_size=params[\"hidden_size\"],\n",
    "                                      dropout=params[\"dropout\"],\n",
    "                                      bidirectional=params[\"bidirectional\"]).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model_cv = nn.DataParallel(model_cv)\n",
    "        \n",
    "        optimizer_cv = optim.Adam(model_cv.parameters(), lr=params[\"lr\"])\n",
    "        loss_fn_cv = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        train_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_train, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        val_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_val, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_val, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader_cv = torch.utils.data.DataLoader(train_dataset_cv, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "        val_loader_cv = torch.utils.data.DataLoader(val_dataset_cv, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "        \n",
    "        model_cv.train()\n",
    "        for epoch in range(params[\"num_epochs\"]):\n",
    "            for X_batch, y_batch in train_loader_cv:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                optimizer_cv.zero_grad()\n",
    "                outputs = model_cv(X_batch)\n",
    "                loss = loss_fn_cv(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer_cv.step()\n",
    "        \n",
    "        def get_preds_cv(loader):\n",
    "            preds, truths = [], []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in loader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    outputs = model_cv(X_batch)\n",
    "                    preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "                    truths.append(y_batch.cpu().numpy())\n",
    "            preds = np.concatenate(preds).flatten()\n",
    "            truths = np.concatenate(truths).flatten()\n",
    "            return truths, preds\n",
    "        \n",
    "        model_cv.eval()\n",
    "        y_val_true_cv, y_val_pred_cv = get_preds_cv(val_loader_cv)\n",
    "        auc_cv = roc_auc_score(y_val_true_cv, y_val_pred_cv)\n",
    "        y_val_bin = (y_val_pred_cv >= 0.5).astype(np.float32)\n",
    "        TP = np.sum((y_val_bin == 1) & (y_val_true_cv == 1))\n",
    "        FN = np.sum((y_val_bin == 0) & (y_val_true_cv == 1))\n",
    "        TN = np.sum((y_val_bin == 0) & (y_val_true_cv == 0))\n",
    "        FP = np.sum((y_val_bin == 1) & (y_val_true_cv == 0))\n",
    "        acc = np.mean(y_val_bin == y_val_true_cv)\n",
    "        sens = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "        spec = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
    "        # Compute binary cross entropy for the fold\n",
    "        epsilon = 1e-7\n",
    "        bse = -np.mean(\n",
    "            y_val_true_cv * np.log(y_val_pred_cv + epsilon) +\n",
    "            (1 - y_val_true_cv) * np.log(1 - y_val_pred_cv + epsilon)\n",
    "        )\n",
    "        cv_fold_metrics.append({\"AUC\": auc_cv, \"accuracy\": acc, \"sensitivity\": sens, \"specificity\": spec, \"bse\": bse})\n",
    "    \n",
    "    keys = [\"AUC\", \"accuracy\", \"sensitivity\", \"specificity\", \"bse\"]\n",
    "    mean_metrics = {k: np.mean([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    sd_metrics = {k: np.std([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    cv_val_df = pd.DataFrame({\n",
    "        \"stat\": [\"mean\", \"sd\"],\n",
    "        \"model\": [\"comb_classification\", \"comb_classification\"],\n",
    "        \"AUC\": [mean_metrics[\"AUC\"], sd_metrics[\"AUC\"]],\n",
    "        \"accuracy\": [mean_metrics[\"accuracy\"], sd_metrics[\"accuracy\"]],\n",
    "        \"sensitivity\": [mean_metrics[\"sensitivity\"], sd_metrics[\"sensitivity\"]],\n",
    "        \"specificity\": [mean_metrics[\"specificity\"], sd_metrics[\"specificity\"]],\n",
    "        \"bse\": [mean_metrics[\"bse\"], sd_metrics[\"bse\"]]\n",
    "    })\n",
    "    \n",
    "    return performance_df, shap_df, cv_val_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cde785-92bc-48cb-8b10-0e2a841ca0f5",
   "metadata": {},
   "source": [
    "### Running the Models and Displaying Results for Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f16cc6b4-0b73-480f-b31c-cf2f7224c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the functions (each returns a tuple: (performance_df, shap_df))\n",
    "perf_fitbit_reg, shap_fitbit_reg, cv_fitbit_reg = run_fitbit_regression_best(fitbit_reg_dict)\n",
    "perf_fitbit_class, shap_fitbit_class, cv_fitbit_class = run_fitbit_classification_best(fitbit_class_dict)\n",
    "perf_comb_reg, shap_comb_reg, cv_comb_reg = run_comb_regression_best(comb_reg_dict)\n",
    "perf_comb_class, shap_comb_class, cv_comb_class = run_comb_classification_best(comb_class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb9b5167-63c8-4084-9e67-fccf56711c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the SHAP score dataframes separately as TSV files (without index)\n",
    "shap_fitbit_reg.to_csv(\"results/shap_fitbit_reg.tsv\", sep=\"\\t\", index=False)\n",
    "shap_fitbit_class.to_csv(\"results/shap_fitbit_class.tsv\", sep=\"\\t\", index=False)\n",
    "shap_comb_reg.to_csv(\"results/shap_comb_reg.tsv\", sep=\"\\t\", index=False)\n",
    "shap_comb_class.to_csv(\"results/shap_comb_class.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da26fcb-fc1c-4f5a-abae-ce6eebfff3e0",
   "metadata": {},
   "source": [
    "Unweighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "266be714-a716-423f-83ea-a80944285d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fitbit_regression_nw_best(fitbit_reg_dict):\n",
    "    set_seed(42)\n",
    "    # Use the original (LSTM) unweighted hyperparameters.\n",
    "    params = {\n",
    "        'batch_size': 32,\n",
    "        'num_epochs': 10,\n",
    "        'lr': 0.00037998740411755944,\n",
    "        'use_regularization': False,  # not used here\n",
    "        'n_fc_layers': 2,\n",
    "        'dropout': 0.2,\n",
    "        'bidirectional': False,\n",
    "        'hidden_size': 32  # chosen default for regression\n",
    "    }\n",
    "    # Create subject-level datasets WITHOUT sample weighting.\n",
    "    train_df, predictor_cols = create_subject_dataset(fitbit_reg_dict['train'], outcome_col=\"SI_mean\", weighted=False)\n",
    "    test_df, _ = create_subject_dataset(fitbit_reg_dict['test'], outcome_col=\"SI_mean\", weighted=False)\n",
    "\n",
    "    X_train = np.stack(train_df[\"X\"].values, axis=0)  # shape: (n_subjects, n_features, seq_len)\n",
    "    y_train = train_df[\"SI_mean\"].values.astype(np.float32)\n",
    "    \n",
    "    X_test = np.stack(test_df[\"X\"].values, axis=0)\n",
    "    y_test = test_df[\"SI_mean\"].values.astype(np.float32)\n",
    "    \n",
    "    n_subjects, input_channels, seq_len = X_train.shape\n",
    "\n",
    "    # Define the LSTM-based model.\n",
    "    class LocalFitRegLSTM(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, n_fc_layers, dropout, bidirectional):\n",
    "            super(LocalFitRegLSTM, self).__init__()\n",
    "            self.lstm = nn.LSTM(input_size=input_size,\n",
    "                                hidden_size=hidden_size,\n",
    "                                num_layers=1,\n",
    "                                batch_first=True,\n",
    "                                dropout=dropout,\n",
    "                                bidirectional=bidirectional)\n",
    "            fc_input = hidden_size * (2 if bidirectional else 1)\n",
    "            fc_layers = []\n",
    "            # Build fully connected layers based on n_fc_layers.\n",
    "            for _ in range(n_fc_layers):\n",
    "                fc_layers.append(nn.Linear(fc_input, 64))\n",
    "                fc_layers.append(nn.ReLU())\n",
    "                fc_layers.append(nn.Dropout(dropout))\n",
    "                fc_input = 64\n",
    "            fc_layers.append(nn.Linear(fc_input, 1))\n",
    "            self.fc_net = nn.Sequential(*fc_layers)\n",
    "        def forward(self, x):\n",
    "            # x shape: [batch, n_features, seq_len] -> permute to [batch, seq_len, n_features]\n",
    "            x = x.permute(0, 2, 1)\n",
    "            out, _ = self.lstm(x)\n",
    "            out = out[:, -1, :]  # take last time step\n",
    "            out = self.fc_net(out)\n",
    "            return out\n",
    "\n",
    "    model = LocalFitRegLSTM(input_size=input_channels,\n",
    "                            hidden_size=params[\"hidden_size\"],\n",
    "                            n_fc_layers=params[\"n_fc_layers\"],\n",
    "                            dropout=params[\"dropout\"],\n",
    "                            bidirectional=params[\"bidirectional\"]).to(device)\n",
    "\n",
    "    # Use DataParallel if multiple GPUs are available.\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "    # Here we use a standard MSELoss (mean reduction)\n",
    "    loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "    \n",
    "    # Create datasets WITHOUT sample weights.\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    test_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.float32),\n",
    "        torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    # Train the main model.\n",
    "    model.train()\n",
    "    for epoch in range(params[\"num_epochs\"]):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Helper function to get predictions.\n",
    "    def get_preds(loader, model_obj):\n",
    "        preds, truths = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                outputs = model_obj(X_batch)\n",
    "                preds.append(outputs.cpu().numpy())\n",
    "                truths.append(y_batch.cpu().numpy())\n",
    "        preds = np.concatenate(preds).flatten()\n",
    "        truths = np.concatenate(truths).flatten()\n",
    "        return truths, preds\n",
    "\n",
    "    model.eval()\n",
    "    y_train_true, y_train_pred = get_preds(train_loader, model)\n",
    "    y_test_true, y_test_pred = get_preds(test_loader, model)\n",
    "\n",
    "    perf_train = compute_regression_perf(y_train_true, y_train_pred)\n",
    "    perf_test  = compute_regression_perf(y_test_true, y_test_pred)\n",
    "\n",
    "    perf_train_df = pd.DataFrame({\n",
    "        \"model\": [\"fitbit_nw_regression\"],\n",
    "        \"type\": [\"train\"],\n",
    "        \"1\": [perf_train[\"1\"]],\n",
    "        \"2\": [perf_train[\"2\"]],\n",
    "        \"3\": [perf_train[\"3\"]],\n",
    "        \"4\": [perf_train[\"4\"]],\n",
    "        \"5\": [perf_train[\"5\"]],\n",
    "        \"overall\": [perf_train[\"overall\"]]\n",
    "    })\n",
    "    perf_test_df = pd.DataFrame({\n",
    "        \"model\": [\"fitbit_nw_regression\"],\n",
    "        \"type\": [\"test\"],\n",
    "        \"1\": [perf_test[\"1\"]],\n",
    "        \"2\": [perf_test[\"2\"]],\n",
    "        \"3\": [perf_test[\"3\"]],\n",
    "        \"4\": [perf_test[\"4\"]],\n",
    "        \"5\": [perf_test[\"5\"]],\n",
    "        \"overall\": [perf_test[\"overall\"]]\n",
    "    })\n",
    "    performance_df = pd.concat([perf_train_df, perf_test_df], ignore_index=True)\n",
    "\n",
    "    # Compute Integrated Gradients for SHAP.\n",
    "    set_seed(42)\n",
    "    model_for_attr = model.module if hasattr(model, 'module') else model\n",
    "    model_for_attr.train()  # for cuDNN LSTM backward\n",
    "    ig = IntegratedGradients(model_for_attr)\n",
    "    input_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    baseline = torch.zeros_like(input_tensor)\n",
    "    attributions = ig.attribute(input_tensor, baselines=baseline, n_steps=50, internal_batch_size=8)\n",
    "    mean_attr = attributions.mean(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    sd_attr = attributions.abs().std(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    if len(predictor_cols) != len(mean_attr):\n",
    "        min_len = min(len(predictor_cols), len(mean_attr))\n",
    "        predictor_names = predictor_cols[:min_len]\n",
    "        mean_attr = mean_attr[:min_len]\n",
    "        sd_attr = sd_attr[:min_len]\n",
    "    else:\n",
    "        predictor_names = predictor_cols\n",
    "    shap_df = pd.DataFrame({\n",
    "        \"predictor\": predictor_names,\n",
    "        \"mean_abs_integrated_gradients\": mean_attr,\n",
    "        \"sd_abs_integrated_gradients\": sd_attr\n",
    "    })\n",
    "\n",
    "    #################################################################\n",
    "    # CROSS-VALIDATION for non-weight Fitbit regression\n",
    "    cv_splits = get_stratified_cv_splits(fitbit_reg_dict['train'], subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5)\n",
    "    cv_fold_metrics = []\n",
    "    for fold, (cv_train_raw, cv_val_raw) in enumerate(cv_splits, start=1):\n",
    "        cv_train_df, _ = create_subject_dataset(cv_train_raw, outcome_col=\"SI_mean\", weighted=False)\n",
    "        cv_val_df, _   = create_subject_dataset(cv_val_raw, outcome_col=\"SI_mean\", weighted=False)\n",
    "        X_cv_train = np.stack(cv_train_df[\"X\"].values, axis=0)\n",
    "        y_cv_train = cv_train_df[\"SI_mean\"].values.astype(np.float32)\n",
    "        X_cv_val   = np.stack(cv_val_df[\"X\"].values, axis=0)\n",
    "        y_cv_val   = cv_val_df[\"SI_mean\"].values.astype(np.float32)\n",
    "        n_cv, input_channels_cv, seq_len_cv = X_cv_train.shape\n",
    "        \n",
    "        # Create a new instance of the LSTM model with the same hyperparameters.\n",
    "        model_cv = LocalFitRegLSTM(input_size=input_channels_cv,\n",
    "                                   hidden_size=params[\"hidden_size\"],\n",
    "                                   n_fc_layers=params[\"n_fc_layers\"],\n",
    "                                   dropout=params[\"dropout\"],\n",
    "                                   bidirectional=params[\"bidirectional\"]).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model_cv = nn.DataParallel(model_cv)\n",
    "        optimizer_cv = optim.Adam(model_cv.parameters(), lr=params[\"lr\"])\n",
    "        loss_fn_cv = nn.MSELoss(reduction=\"mean\")\n",
    "        \n",
    "        train_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_train, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        val_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_val, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_val, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader_cv = torch.utils.data.DataLoader(train_dataset_cv, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "        val_loader_cv   = torch.utils.data.DataLoader(val_dataset_cv, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "        \n",
    "        model_cv.train()\n",
    "        for epoch in range(params[\"num_epochs\"]):\n",
    "            for X_batch, y_batch in train_loader_cv:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                optimizer_cv.zero_grad()\n",
    "                outputs = model_cv(X_batch)\n",
    "                loss = loss_fn_cv(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer_cv.step()\n",
    "        \n",
    "        model_cv.eval()\n",
    "        y_val_true_cv, y_val_pred_cv = get_preds(val_loader_cv, model_cv)\n",
    "        fold_perf = compute_regression_perf(y_val_true_cv, y_val_pred_cv)\n",
    "        mse_val = np.mean((y_val_true_cv - y_val_pred_cv) ** 2)\n",
    "        fold_perf[\"mse\"] = mse_val\n",
    "        cv_fold_metrics.append(fold_perf)\n",
    "    \n",
    "    keys = [\"1\", \"2\", \"3\", \"4\", \"5\", \"overall\", \"mse\"]\n",
    "    mean_metrics = {k: np.mean([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    sd_metrics   = {k: np.std([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    cv_val_df = pd.DataFrame({\n",
    "        \"stat\": [\"mean\", \"sd\"],\n",
    "        \"model\": [\"fitbit_nw_regression\", \"fitbit_nw_regression\"],\n",
    "        \"1\": [mean_metrics[\"1\"], sd_metrics[\"1\"]],\n",
    "        \"2\": [mean_metrics[\"2\"], sd_metrics[\"2\"]],\n",
    "        \"3\": [mean_metrics[\"3\"], sd_metrics[\"3\"]],\n",
    "        \"4\": [mean_metrics[\"4\"], sd_metrics[\"4\"]],\n",
    "        \"5\": [mean_metrics[\"5\"], sd_metrics[\"5\"]],\n",
    "        \"overall\": [mean_metrics[\"overall\"], sd_metrics[\"overall\"]],\n",
    "        \"mse\": [mean_metrics[\"mse\"], sd_metrics[\"mse\"]]\n",
    "    })\n",
    "\n",
    "    return performance_df, shap_df, cv_val_df\n",
    "\n",
    "\n",
    "def run_fitbit_classification_nw_best(fitbit_dict):\n",
    "    set_seed(42)\n",
    "    # Use unweighted hyperparameters.\n",
    "    params = {\n",
    "        'lr': 0.001000968792210794,\n",
    "        'hidden_size': 32,\n",
    "        'dropout': 0.145910416042635,\n",
    "        'num_epochs': 10,\n",
    "        'batch_size': 64\n",
    "    }\n",
    "    # Create subject-level datasets WITHOUT sample weighting.\n",
    "    train_df, predictor_cols = create_subject_dataset(fitbit_dict['train'], outcome_col=\"is_SI\", weighted=False)\n",
    "    test_df, _ = create_subject_dataset(fitbit_dict['test'], outcome_col=\"is_SI\", weighted=False)\n",
    "    \n",
    "    X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "    y_train = train_df[\"is_SI\"].values.astype(np.float32)\n",
    "    X_test = np.stack(test_df[\"X\"].values, axis=0)\n",
    "    y_test = test_df[\"is_SI\"].values.astype(np.float32)\n",
    "    \n",
    "    n_subjects, input_channels, seq_len = X_train.shape\n",
    "\n",
    "    # Define the local LSTM classifier.\n",
    "    class LocalFitbitClassLSTM(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, dropout):\n",
    "            super(LocalFitbitClassLSTM, self).__init__()\n",
    "            self.lstm = nn.LSTM(input_size=input_size,\n",
    "                                hidden_size=hidden_size,\n",
    "                                num_layers=1,\n",
    "                                batch_first=True,\n",
    "                                dropout=dropout,\n",
    "                                bidirectional=False)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "        def forward(self, x):\n",
    "            x = x.permute(0, 2, 1)\n",
    "            out, _ = self.lstm(x)\n",
    "            out = out[:, -1, :]\n",
    "            out = self.dropout(out)\n",
    "            out = self.fc(out)\n",
    "            return out\n",
    "\n",
    "    model = LocalFitbitClassLSTM(input_size=input_channels,\n",
    "                                 hidden_size=params[\"hidden_size\"],\n",
    "                                 dropout=params[\"dropout\"]).to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    test_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.float32),\n",
    "        torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(params[\"num_epochs\"]):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def get_preds(loader, model_obj):\n",
    "        preds, truths = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                outputs = model_obj(X_batch)\n",
    "                preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "                truths.append(y_batch.cpu().numpy())\n",
    "        preds = np.concatenate(preds).flatten()\n",
    "        truths = np.concatenate(truths).flatten()\n",
    "        return truths, preds\n",
    "\n",
    "    model.eval()\n",
    "    y_train_true, y_train_pred = get_preds(train_loader, model)\n",
    "    y_test_true, y_test_pred = get_preds(test_loader, model)\n",
    "    \n",
    "    # Compute metrics on main splits.\n",
    "    TP_train = np.sum((y_train_pred >= 0.5) & (y_train_true == 1))\n",
    "    FN_train = np.sum((y_train_pred < 0.5) & (y_train_true == 1))\n",
    "    TN_train = np.sum((y_train_pred < 0.5) & (y_train_true == 0))\n",
    "    FP_train = np.sum((y_train_pred >= 0.5) & (y_train_true == 0))\n",
    "    train_acc = np.mean((y_train_pred >= 0.5) == y_train_true)\n",
    "    train_sens = TP_train / (TP_train + FN_train) if (TP_train + FN_train) > 0 else np.nan\n",
    "    train_spec = TN_train / (TN_train + FP_train) if (TN_train + FP_train) > 0 else np.nan\n",
    "\n",
    "    TP_test = np.sum((y_test_pred >= 0.5) & (y_test_true == 1))\n",
    "    FN_test = np.sum((y_test_pred < 0.5) & (y_test_true == 1))\n",
    "    TN_test = np.sum((y_test_pred < 0.5) & (y_test_true == 0))\n",
    "    FP_test = np.sum((y_test_pred >= 0.5) & (y_test_true == 0))\n",
    "    test_acc = np.mean((y_test_pred >= 0.5) == y_test_true)\n",
    "    test_sens = TP_test / (TP_test + FN_test) if (TP_test + FN_test) > 0 else np.nan\n",
    "    test_spec = TN_test / (TN_test + FP_test) if (TN_test + FP_test) > 0 else np.nan\n",
    "\n",
    "    perf_train_df = pd.DataFrame({\n",
    "        \"model\": [\"fitbit_nw_classification\"],\n",
    "        \"type\": [\"train\"],\n",
    "        \"accuracy\": [train_acc],\n",
    "        \"sensitivity\": [train_sens],\n",
    "        \"specificity\": [train_spec]\n",
    "    })\n",
    "    perf_test_df = pd.DataFrame({\n",
    "        \"model\": [\"fitbit_nw_classification\"],\n",
    "        \"type\": [\"test\"],\n",
    "        \"accuracy\": [test_acc],\n",
    "        \"sensitivity\": [test_sens],\n",
    "        \"specificity\": [test_spec]\n",
    "    })\n",
    "    performance_df = pd.concat([perf_train_df, perf_test_df], ignore_index=True)\n",
    "    \n",
    "    # Compute Integrated Gradients for SHAP.\n",
    "    set_seed(42)\n",
    "    model_for_attr = model.module if hasattr(model, 'module') else model\n",
    "    model_for_attr.train()  # for cuDNN LSTM backward\n",
    "    ig = IntegratedGradients(model_for_attr)\n",
    "    input_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    baseline = torch.zeros_like(input_tensor)\n",
    "    attributions = ig.attribute(input_tensor, baselines=baseline, n_steps=50, internal_batch_size=8)\n",
    "    mean_attr = attributions.mean(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    sd_attr = attributions.abs().std(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    if len(predictor_cols) != len(mean_attr):\n",
    "        min_len = min(len(predictor_cols), len(mean_attr))\n",
    "        predictor_names = predictor_cols[:min_len]\n",
    "        mean_attr = mean_attr[:min_len]\n",
    "        sd_attr = sd_attr[:min_len]\n",
    "    else:\n",
    "        predictor_names = predictor_cols\n",
    "    shap_df = pd.DataFrame({\n",
    "        \"predictor\": predictor_names,\n",
    "        \"mean_abs_integrated_gradients\": mean_attr,\n",
    "        \"sd_abs_integrated_gradients\": sd_attr\n",
    "    })\n",
    "    \n",
    "    #################################################################\n",
    "    # CROSS-VALIDATION for non-weight Fitbit classification\n",
    "    cv_splits = get_stratified_cv_splits(fitbit_dict['train'], subject_id=\"PatientID\", target_var=\"is_SI\", n_splits=5)\n",
    "    cv_fold_metrics = []\n",
    "    for fold, (cv_train_raw, cv_val_raw) in enumerate(cv_splits, start=1):\n",
    "        cv_train_df, _ = create_subject_dataset(cv_train_raw, outcome_col=\"is_SI\", weighted=False)\n",
    "        cv_val_df, _   = create_subject_dataset(cv_val_raw, outcome_col=\"is_SI\", weighted=False)\n",
    "        X_cv_train = np.stack(cv_train_df[\"X\"].values, axis=0)\n",
    "        y_cv_train = cv_train_df[\"is_SI\"].values.astype(np.float32)\n",
    "        X_cv_val   = np.stack(cv_val_df[\"X\"].values, axis=0)\n",
    "        y_cv_val   = cv_val_df[\"is_SI\"].values.astype(np.float32)\n",
    "        n_cv, input_channels_cv, seq_len_cv = X_cv_train.shape\n",
    "\n",
    "        model_cv = LocalFitbitClassLSTM(input_size=input_channels_cv,\n",
    "                                        hidden_size=params[\"hidden_size\"],\n",
    "                                        dropout=params[\"dropout\"]).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model_cv = nn.DataParallel(model_cv)\n",
    "        optimizer_cv = optim.Adam(model_cv.parameters(), lr=params[\"lr\"])\n",
    "        loss_fn_cv = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        train_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_train, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        val_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_val, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_val, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader_cv = torch.utils.data.DataLoader(train_dataset_cv, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "        val_loader_cv   = torch.utils.data.DataLoader(val_dataset_cv, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "        \n",
    "        model_cv.train()\n",
    "        for epoch in range(params[\"num_epochs\"]):\n",
    "            for X_batch, y_batch in train_loader_cv:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                optimizer_cv.zero_grad()\n",
    "                outputs = model_cv(X_batch)\n",
    "                loss = loss_fn_cv(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer_cv.step()\n",
    "        \n",
    "        model_cv.eval()\n",
    "        y_val_true_cv, y_val_pred_cv = get_preds(val_loader_cv, model_cv)\n",
    "        auc_cv = roc_auc_score(y_val_true_cv, y_val_pred_cv)\n",
    "        y_val_bin = (y_val_pred_cv >= 0.5).astype(np.float32)\n",
    "        TP = np.sum((y_val_bin == 1) & (y_val_true_cv == 1))\n",
    "        FN = np.sum((y_val_bin == 0) & (y_val_true_cv == 1))\n",
    "        TN = np.sum((y_val_bin == 0) & (y_val_true_cv == 0))\n",
    "        FP = np.sum((y_val_bin == 1) & (y_val_true_cv == 0))\n",
    "        acc = np.mean(y_val_bin == y_val_true_cv)\n",
    "        sens = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "        spec = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
    "        epsilon = 1e-7\n",
    "        bse_cv = -np.mean(y_val_true_cv * np.log(y_val_pred_cv + epsilon) + (1 - y_val_true_cv) * np.log(1 - y_val_pred_cv + epsilon))\n",
    "        cv_fold_metrics.append({\"AUC\": auc_cv, \"accuracy\": acc, \"sensitivity\": sens, \"specificity\": spec, \"bse\": bse_cv})\n",
    "    \n",
    "    keys = [\"AUC\", \"accuracy\", \"sensitivity\", \"specificity\", \"bse\"]\n",
    "    mean_metrics = {k: np.mean([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    sd_metrics = {k: np.std([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    cv_val_df = pd.DataFrame({\n",
    "        \"stat\": [\"mean\", \"sd\"],\n",
    "        \"model\": [\"fitbit_nw_classification\", \"fitbit_nw_classification\"],\n",
    "        \"AUC\": [mean_metrics[\"AUC\"], sd_metrics[\"AUC\"]],\n",
    "        \"accuracy\": [mean_metrics[\"accuracy\"], sd_metrics[\"accuracy\"]],\n",
    "        \"sensitivity\": [mean_metrics[\"sensitivity\"], sd_metrics[\"sensitivity\"]],\n",
    "        \"specificity\": [mean_metrics[\"specificity\"], sd_metrics[\"specificity\"]],\n",
    "        \"bse\": [mean_metrics[\"bse\"], sd_metrics[\"bse\"]]\n",
    "    })\n",
    "    \n",
    "    return performance_df, shap_df, cv_val_df\n",
    "\n",
    "\n",
    "def run_comb_regression_nw_best(comb_dict):\n",
    "    set_seed(42)\n",
    "    # Use unweighted hyperparameters.\n",
    "    params = {\n",
    "        'lr': 0.00018275118830783695,\n",
    "        'dropout': 0.21206897079668371,\n",
    "        'num_epochs': 10,\n",
    "        'batch_size': 32,\n",
    "        'hidden_size': 32\n",
    "    }\n",
    "    # Create subject-level datasets WITHOUT sample weighting.\n",
    "    train_df, predictor_cols = create_subject_dataset(comb_dict['train'], outcome_col=\"SI_mean\", weighted=False)\n",
    "    test_df, _ = create_subject_dataset(comb_dict['test'], outcome_col=\"SI_mean\", weighted=False)\n",
    "    \n",
    "    X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "    y_train = train_df[\"SI_mean\"].values.astype(np.float32)\n",
    "    \n",
    "    X_test = np.stack(test_df[\"X\"].values, axis=0)\n",
    "    y_test = test_df[\"SI_mean\"].values.astype(np.float32)\n",
    "    \n",
    "    n_subjects, input_channels, seq_len = X_train.shape\n",
    "\n",
    "    # Define the local LSTM model.\n",
    "    class LocalCombRegLSTM(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, dropout):\n",
    "            super(LocalCombRegLSTM, self).__init__()\n",
    "            self.lstm = nn.LSTM(input_size=input_size,\n",
    "                                hidden_size=hidden_size,\n",
    "                                num_layers=1,\n",
    "                                batch_first=True,\n",
    "                                dropout=dropout,\n",
    "                                bidirectional=False)\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "        def forward(self, x):\n",
    "            x = x.permute(0, 2, 1)\n",
    "            out, _ = self.lstm(x)\n",
    "            out = out[:, -1, :]\n",
    "            out = self.fc(out)\n",
    "            return out\n",
    "\n",
    "    model = LocalCombRegLSTM(input_size=input_channels,\n",
    "                             hidden_size=params[\"hidden_size\"],\n",
    "                             dropout=params[\"dropout\"]).to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "    loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    test_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.float32),\n",
    "        torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(params[\"num_epochs\"]):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def get_preds(loader, model_obj):\n",
    "        preds, truths = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                outputs = model_obj(X_batch)\n",
    "                preds.append(outputs.cpu().numpy())\n",
    "                truths.append(y_batch.cpu().numpy())\n",
    "        preds = np.concatenate(preds).flatten()\n",
    "        truths = np.concatenate(truths).flatten()\n",
    "        return truths, preds\n",
    "\n",
    "    model.eval()\n",
    "    y_train_true, y_train_pred = get_preds(train_loader, model)\n",
    "    y_test_true, y_test_pred = get_preds(test_loader, model)\n",
    "    \n",
    "    perf_train = compute_regression_perf(y_train_true, y_train_pred)\n",
    "    perf_test  = compute_regression_perf(y_test_true, y_test_pred)\n",
    "    \n",
    "    perf_train_df = pd.DataFrame({\n",
    "        \"model\": [\"comb_nw_regression\"],\n",
    "        \"type\": [\"train\"],\n",
    "        \"1\": [perf_train[\"1\"]],\n",
    "        \"2\": [perf_train[\"2\"]],\n",
    "        \"3\": [perf_train[\"3\"]],\n",
    "        \"4\": [perf_train[\"4\"]],\n",
    "        \"5\": [perf_train[\"5\"]],\n",
    "        \"overall\": [perf_train[\"overall\"]]\n",
    "    })\n",
    "    perf_test_df = pd.DataFrame({\n",
    "        \"model\": [\"comb_nw_regression\"],\n",
    "        \"type\": [\"test\"],\n",
    "        \"1\": [perf_test[\"1\"]],\n",
    "        \"2\": [perf_test[\"2\"]],\n",
    "        \"3\": [perf_test[\"3\"]],\n",
    "        \"4\": [perf_test[\"4\"]],\n",
    "        \"5\": [perf_test[\"5\"]],\n",
    "        \"overall\": [perf_test[\"overall\"]]\n",
    "    })\n",
    "    performance_df = pd.concat([perf_train_df, perf_test_df], ignore_index=True)\n",
    "    \n",
    "    # Compute Integrated Gradients for SHAP.\n",
    "    set_seed(42)\n",
    "    model_for_attr = model.module if hasattr(model, 'module') else model\n",
    "    model_for_attr.train()  # for cuDNN LSTM backward\n",
    "    ig = IntegratedGradients(model_for_attr)\n",
    "    input_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    baseline = torch.zeros_like(input_tensor)\n",
    "    attributions = ig.attribute(input_tensor, baselines=baseline, n_steps=50, internal_batch_size=8)\n",
    "    mean_attr = attributions.mean(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    sd_attr = attributions.abs().std(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    if len(predictor_cols) != len(mean_attr):\n",
    "        min_len = min(len(predictor_cols), len(mean_attr))\n",
    "        predictor_names = predictor_cols[:min_len]\n",
    "        mean_attr = mean_attr[:min_len]\n",
    "        sd_attr = sd_attr[:min_len]\n",
    "    else:\n",
    "        predictor_names = predictor_cols\n",
    "    shap_df = pd.DataFrame({\n",
    "        \"predictor\": predictor_names,\n",
    "        \"mean_abs_integrated_gradients\": mean_attr,\n",
    "        \"sd_abs_integrated_gradients\": sd_attr\n",
    "    })\n",
    "    \n",
    "    #################################################################\n",
    "    # CROSS-VALIDATION for non-weight Comb regression\n",
    "    cv_splits = get_stratified_cv_splits(comb_dict['train'], subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5)\n",
    "    cv_fold_metrics = []\n",
    "    for fold, (cv_train_raw, cv_val_raw) in enumerate(cv_splits, start=1):\n",
    "        cv_train_df, _ = create_subject_dataset(cv_train_raw, outcome_col=\"SI_mean\", weighted=False)\n",
    "        cv_val_df, _   = create_subject_dataset(cv_val_raw, outcome_col=\"SI_mean\", weighted=False)\n",
    "        X_cv_train = np.stack(cv_train_df[\"X\"].values, axis=0)\n",
    "        y_cv_train = cv_train_df[\"SI_mean\"].values.astype(np.float32)\n",
    "        X_cv_val   = np.stack(cv_val_df[\"X\"].values, axis=0)\n",
    "        y_cv_val   = cv_val_df[\"SI_mean\"].values.astype(np.float32)\n",
    "        n_cv, input_channels_cv, seq_len_cv = X_cv_train.shape\n",
    "        \n",
    "        model_cv = LocalCombRegLSTM(input_size=input_channels_cv,\n",
    "                                    hidden_size=params[\"hidden_size\"],\n",
    "                                    dropout=params[\"dropout\"]).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model_cv = nn.DataParallel(model_cv)\n",
    "        optimizer_cv = optim.Adam(model_cv.parameters(), lr=params[\"lr\"])\n",
    "        loss_fn_cv = nn.MSELoss(reduction=\"mean\")\n",
    "        \n",
    "        train_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_train, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        val_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_val, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_val, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader_cv = torch.utils.data.DataLoader(train_dataset_cv, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "        val_loader_cv   = torch.utils.data.DataLoader(val_dataset_cv, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "        \n",
    "        model_cv.train()\n",
    "        for epoch in range(params[\"num_epochs\"]):\n",
    "            for X_batch, y_batch in train_loader_cv:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                optimizer_cv.zero_grad()\n",
    "                outputs = model_cv(X_batch)\n",
    "                loss = loss_fn_cv(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer_cv.step()\n",
    "        \n",
    "        model_cv.eval()\n",
    "        y_val_true_cv, y_val_pred_cv = get_preds(val_loader_cv, model_cv)\n",
    "        fold_perf = compute_regression_perf(y_val_true_cv, y_val_pred_cv)\n",
    "        mse_val = np.mean((y_val_true_cv - y_val_pred_cv) ** 2)\n",
    "        fold_perf[\"mse\"] = mse_val\n",
    "        cv_fold_metrics.append(fold_perf)\n",
    "    \n",
    "    keys = [\"1\", \"2\", \"3\", \"4\", \"5\", \"overall\", \"mse\"]\n",
    "    mean_metrics = {k: np.mean([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    sd_metrics   = {k: np.std([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    cv_val_df = pd.DataFrame({\n",
    "        \"stat\": [\"mean\", \"sd\"],\n",
    "        \"model\": [\"comb_nw_regression\", \"comb_nw_regression\"],\n",
    "        \"1\": [mean_metrics[\"1\"], sd_metrics[\"1\"]],\n",
    "        \"2\": [mean_metrics[\"2\"], sd_metrics[\"2\"]],\n",
    "        \"3\": [mean_metrics[\"3\"], sd_metrics[\"3\"]],\n",
    "        \"4\": [mean_metrics[\"4\"], sd_metrics[\"4\"]],\n",
    "        \"5\": [mean_metrics[\"5\"], sd_metrics[\"5\"]],\n",
    "        \"overall\": [mean_metrics[\"overall\"], sd_metrics[\"overall\"]],\n",
    "        \"mse\": [mean_metrics[\"mse\"], sd_metrics[\"mse\"]]\n",
    "    })\n",
    "    \n",
    "    return performance_df, shap_df, cv_val_df\n",
    "\n",
    "\n",
    "def run_comb_classification_nw_best(comb_dict):\n",
    "    set_seed(42)\n",
    "    # Use unweighted hyperparameters.\n",
    "    params = {\n",
    "        'lr': 0.00013777799219093285,\n",
    "        'dropout': 0.2,\n",
    "        'num_epochs': 10,\n",
    "        'batch_size': 32,\n",
    "        'hidden_size': 32,\n",
    "        'bidirectional': False\n",
    "    }\n",
    "    # Create subject-level datasets WITHOUT sample weighting.\n",
    "    train_df, predictor_cols = create_subject_dataset(comb_dict['train'], outcome_col=\"is_SI\", weighted=False)\n",
    "    test_df, _ = create_subject_dataset(comb_dict['test'], outcome_col=\"is_SI\", weighted=False)\n",
    "    \n",
    "    X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "    y_train = train_df[\"is_SI\"].values.astype(np.float32)\n",
    "    X_test = np.stack(test_df[\"X\"].values, axis=0)\n",
    "    y_test = test_df[\"is_SI\"].values.astype(np.float32)\n",
    "    \n",
    "    n_subjects, input_channels, seq_len = X_train.shape\n",
    "\n",
    "    # Define the local LSTM classifier.\n",
    "    class LocalCombClassLSTM(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, dropout, bidirectional):\n",
    "            super(LocalCombClassLSTM, self).__init__()\n",
    "            self.lstm = nn.LSTM(input_size=input_size,\n",
    "                                hidden_size=hidden_size,\n",
    "                                num_layers=1,\n",
    "                                batch_first=True,\n",
    "                                dropout=dropout,\n",
    "                                bidirectional=bidirectional)\n",
    "            fc_input = hidden_size * (2 if bidirectional else 1)\n",
    "            self.fc = nn.Linear(fc_input, 1)\n",
    "        def forward(self, x):\n",
    "            x = x.permute(0, 2, 1)\n",
    "            out, _ = self.lstm(x)\n",
    "            out = out[:, -1, :]\n",
    "            out = self.fc(out)\n",
    "            return out\n",
    "\n",
    "    model = LocalCombClassLSTM(input_size=input_channels,\n",
    "                               hidden_size=params[\"hidden_size\"],\n",
    "                               dropout=params[\"dropout\"],\n",
    "                               bidirectional=params[\"bidirectional\"]).to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    test_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.float32),\n",
    "        torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(params[\"num_epochs\"]):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = loss_fn(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def get_preds(loader, model_obj):\n",
    "        preds, truths = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                outputs = model_obj(X_batch)\n",
    "                preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "                truths.append(y_batch.cpu().numpy())\n",
    "        preds = np.concatenate(preds).flatten()\n",
    "        truths = np.concatenate(truths).flatten()\n",
    "        return truths, preds\n",
    "\n",
    "    model.eval()\n",
    "    y_train_true, y_train_pred = get_preds(train_loader, model)\n",
    "    y_test_true, y_test_pred = get_preds(test_loader, model)\n",
    "    \n",
    "    TP_train = np.sum((y_train_pred >= 0.5) & (y_train_true == 1))\n",
    "    FN_train = np.sum((y_train_pred < 0.5) & (y_train_true == 1))\n",
    "    TN_train = np.sum((y_train_pred < 0.5) & (y_train_true == 0))\n",
    "    FP_train = np.sum((y_train_pred >= 0.5) & (y_train_true == 0))\n",
    "    train_acc = np.mean((y_train_pred >= 0.5) == y_train_true)\n",
    "    train_sens = TP_train / (TP_train + FN_train) if (TP_train + FN_train) > 0 else np.nan\n",
    "    train_spec = TN_train / (TN_train + FP_train) if (TN_train + FP_train) > 0 else np.nan\n",
    "    \n",
    "    TP_test = np.sum((y_test_pred >= 0.5) & (y_test_true == 1))\n",
    "    FN_test = np.sum((y_test_pred < 0.5) & (y_test_true == 1))\n",
    "    TN_test = np.sum((y_test_pred < 0.5) & (y_test_true == 0))\n",
    "    FP_test = np.sum((y_test_pred >= 0.5) & (y_test_true == 0))\n",
    "    test_acc = np.mean((y_test_pred >= 0.5) == y_test_true)\n",
    "    test_sens = TP_test / (TP_test + FN_test) if (TP_test + FN_test) > 0 else np.nan\n",
    "    test_spec = TN_test / (TN_test + FP_test) if (TN_test + FP_test) > 0 else np.nan\n",
    "    \n",
    "    perf_train_df = pd.DataFrame({\n",
    "        \"model\": [\"comb_nw_classification\"],\n",
    "        \"type\": [\"train\"],\n",
    "        \"accuracy\": [train_acc],\n",
    "        \"sensitivity\": [train_sens],\n",
    "        \"specificity\": [train_spec]\n",
    "    })\n",
    "    perf_test_df = pd.DataFrame({\n",
    "        \"model\": [\"comb_nw_classification\"],\n",
    "        \"type\": [\"test\"],\n",
    "        \"accuracy\": [test_acc],\n",
    "        \"sensitivity\": [test_sens],\n",
    "        \"specificity\": [test_spec]\n",
    "    })\n",
    "    performance_df = pd.concat([perf_train_df, perf_test_df], ignore_index=True)\n",
    "    \n",
    "    # Compute Integrated Gradients for SHAP.\n",
    "    set_seed(42)\n",
    "    model_for_attr = model.module if hasattr(model, 'module') else model\n",
    "    model_for_attr.train()  # for cuDNN LSTM backward\n",
    "    ig = IntegratedGradients(model_for_attr)\n",
    "    input_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    baseline = torch.zeros_like(input_tensor)\n",
    "    attributions = ig.attribute(input_tensor, baselines=baseline, n_steps=50, internal_batch_size=8)\n",
    "    mean_attr = attributions.mean(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    sd_attr = attributions.abs().std(dim=0).mean(dim=1).cpu().detach().numpy()\n",
    "    if len(predictor_cols) != len(mean_attr):\n",
    "        min_len = min(len(predictor_cols), len(mean_attr))\n",
    "        predictor_names = predictor_cols[:min_len]\n",
    "        mean_attr = mean_attr[:min_len]\n",
    "        sd_attr = sd_attr[:min_len]\n",
    "    else:\n",
    "        predictor_names = predictor_cols\n",
    "    shap_df = pd.DataFrame({\n",
    "        \"predictor\": predictor_names,\n",
    "        \"mean_abs_integrated_gradients\": mean_attr,\n",
    "        \"sd_abs_integrated_gradients\": sd_attr\n",
    "    })\n",
    "    \n",
    "    #################################################################\n",
    "    # CROSS-VALIDATION for non-weight Comb classification\n",
    "    cv_splits = get_stratified_cv_splits(comb_dict['train'], subject_id=\"PatientID\", target_var=\"is_SI\", n_splits=5)\n",
    "    cv_fold_metrics = []\n",
    "    for fold, (cv_train_raw, cv_val_raw) in enumerate(cv_splits, start=1):\n",
    "        cv_train_df, _ = create_subject_dataset(cv_train_raw, outcome_col=\"is_SI\", weighted=False)\n",
    "        cv_val_df, _   = create_subject_dataset(cv_val_raw, outcome_col=\"is_SI\", weighted=False)\n",
    "        X_cv_train = np.stack(cv_train_df[\"X\"].values, axis=0)\n",
    "        y_cv_train = cv_train_df[\"is_SI\"].values.astype(np.float32)\n",
    "        X_cv_val   = np.stack(cv_val_df[\"X\"].values, axis=0)\n",
    "        y_cv_val   = cv_val_df[\"is_SI\"].values.astype(np.float32)\n",
    "        n_cv, input_channels_cv, seq_len_cv = X_cv_train.shape\n",
    "\n",
    "        model_cv = LocalCombClassLSTM(input_size=input_channels_cv,\n",
    "                                      hidden_size=params[\"hidden_size\"],\n",
    "                                      dropout=params[\"dropout\"],\n",
    "                                      bidirectional=params[\"bidirectional\"]).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model_cv = nn.DataParallel(model_cv)\n",
    "        optimizer_cv = optim.Adam(model_cv.parameters(), lr=params[\"lr\"])\n",
    "        loss_fn_cv = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        train_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_train, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_train, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        val_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_val, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_val, dtype=torch.float32).view(-1, 1)\n",
    "        )\n",
    "        train_loader_cv = torch.utils.data.DataLoader(train_dataset_cv, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "        val_loader_cv   = torch.utils.data.DataLoader(val_dataset_cv, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "        \n",
    "        model_cv.train()\n",
    "        for epoch in range(params[\"num_epochs\"]):\n",
    "            for X_batch, y_batch in train_loader_cv:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                optimizer_cv.zero_grad()\n",
    "                outputs = model_cv(X_batch)\n",
    "                loss = loss_fn_cv(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer_cv.step()\n",
    "        \n",
    "        model_cv.eval()\n",
    "        y_val_true_cv, y_val_pred_cv = get_preds(val_loader_cv, model_cv)\n",
    "        auc_cv = roc_auc_score(y_val_true_cv, y_val_pred_cv)\n",
    "        y_val_bin = (y_val_pred_cv >= 0.5).astype(np.float32)\n",
    "        TP = np.sum((y_val_bin == 1) & (y_val_true_cv == 1))\n",
    "        FN = np.sum((y_val_bin == 0) & (y_val_true_cv == 1))\n",
    "        TN = np.sum((y_val_bin == 0) & (y_val_true_cv == 0))\n",
    "        FP = np.sum((y_val_bin == 1) & (y_val_true_cv == 0))\n",
    "        acc = np.mean(y_val_bin == y_val_true_cv)\n",
    "        sens = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "        spec = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
    "        epsilon = 1e-7\n",
    "        bse_cv = -np.mean(y_val_true_cv * np.log(y_val_pred_cv + epsilon) + (1 - y_val_true_cv) * np.log(1 - y_val_pred_cv + epsilon))\n",
    "        cv_fold_metrics.append({\"AUC\": auc_cv, \"accuracy\": acc, \"sensitivity\": sens, \"specificity\": spec, \"bse\": bse_cv})\n",
    "    \n",
    "    keys = [\"AUC\", \"accuracy\", \"sensitivity\", \"specificity\", \"bse\"]\n",
    "    mean_metrics = {k: np.mean([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    sd_metrics = {k: np.std([fold[k] for fold in cv_fold_metrics]) for k in keys}\n",
    "    cv_val_df = pd.DataFrame({\n",
    "        \"stat\": [\"mean\", \"sd\"],\n",
    "        \"model\": [\"comb_nw_classification\", \"comb_nw_classification\"],\n",
    "        \"AUC\": [mean_metrics[\"AUC\"], sd_metrics[\"AUC\"]],\n",
    "        \"accuracy\": [mean_metrics[\"accuracy\"], sd_metrics[\"accuracy\"]],\n",
    "        \"sensitivity\": [mean_metrics[\"sensitivity\"], sd_metrics[\"sensitivity\"]],\n",
    "        \"specificity\": [mean_metrics[\"specificity\"], sd_metrics[\"specificity\"]],\n",
    "        \"bse\": [mean_metrics[\"bse\"], sd_metrics[\"bse\"]]\n",
    "    })\n",
    "    \n",
    "    return performance_df, shap_df, cv_val_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641f359d-f517-469c-aec2-5fc8b77f5eb6",
   "metadata": {},
   "source": [
    "### Running the Models and Displaying Results for Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aefb3fc3-f72b-402d-9c9b-984f5ef74276",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_fitbit_reg_nw, shap_fitbit_reg_nw, cv_fitbit_reg_nw = run_fitbit_regression_nw_best(fitbit_reg_dict)\n",
    "perf_fitbit_class_nw, shap_fitbit_class_nw, cv_fitbit_class_nw = run_fitbit_classification_nw_best(fitbit_class_dict)\n",
    "perf_comb_reg_nw, shap_comb_reg_nw, cv_comb_reg_nw = run_comb_regression_nw_best(comb_reg_dict)\n",
    "perf_comb_class_nw, shap_comb_class_nw, cv_comb_class_nw = run_comb_classification_nw_best(comb_class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1307e4b-e356-49c7-8d27-449c64830cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the SHAP score dataframes separately as TSV files (without index)\n",
    "shap_fitbit_reg_nw.to_csv(\"results/shap_fitbit_reg_nw.tsv\", sep=\"\\t\", index=False)\n",
    "shap_fitbit_class_nw.to_csv(\"results/shap_fitbit_class_nw.tsv\", sep=\"\\t\", index=False)\n",
    "shap_comb_reg_nw.to_csv(\"results/shap_comb_reg_nw.tsv\", sep=\"\\t\", index=False)\n",
    "shap_comb_class_nw.to_csv(\"results/shap_comb_class_nw.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c2690a-916a-4c2d-b3b8-d5fae0b8a29f",
   "metadata": {},
   "source": [
    "### Save train/test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b65ff403-8114-4ad8-8896-df6355421524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the performance dataframes for regression and classification\n",
    "best_perf_reg = pd.concat([perf_fitbit_reg, perf_comb_reg, perf_fitbit_reg_nw, perf_comb_reg_nw])\n",
    "best_perf_class = pd.concat([perf_fitbit_class, perf_comb_class, perf_fitbit_class_nw, perf_comb_class_nw])\n",
    "\n",
    "\n",
    "# Save the combined performance dataframes as TSV files (without index)\n",
    "best_perf_reg.to_csv(\"results/best_perf_reg.tsv\", sep=\"\\t\", index=False)\n",
    "best_perf_class.to_csv(\"results/best_perf_class.tsv\", sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca81fea-d73d-44da-b01d-b6d3811d7af3",
   "metadata": {},
   "source": [
    "### Save Validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa5c0a0e-a78f-44e2-b1da-41bb7f7ef5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the val performance dataframes for regression and classification\n",
    "best_cv_perf_reg = pd.concat([cv_fitbit_reg, cv_comb_reg, cv_fitbit_reg_nw, cv_comb_reg_nw])\n",
    "best_cv_perf_class = pd.concat([cv_fitbit_class, cv_comb_class, cv_fitbit_class_nw, cv_comb_class_nw])\n",
    "\n",
    "# Save the combined val performance dataframes as TSV files (without index)\n",
    "best_cv_perf_reg.to_csv(\"results/best_val_perf_reg.tsv\", sep=\"\\t\", index=False)\n",
    "best_cv_perf_class.to_csv(\"results/best_val_perf_class.tsv\", sep=\"\\t\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL_py_env)",
   "language": "python",
   "name": "dl_py_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
