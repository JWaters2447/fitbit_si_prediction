{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d7cb036-8a29-4188-ba59-d74e8e25b9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda with 5 GPU(s)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold  # (Not used now, but kept if needed elsewhere)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set deterministic behavior for CUDA (set before torch imports)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import optuna\n",
    "import optuna.visualization\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # for multi-GPU\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    return seed\n",
    "\n",
    "# Global device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device} with {torch.cuda.device_count()} GPU(s)\")\n",
    "\n",
    "# Data directory (adjust as needed)\n",
    "DL_DIR = \"../../data/deep_learning\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254431e0-045d-44b4-8e3b-800dcdf86f8f",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6591c5b-a315-4fe9-b8d1-fceee51ae6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the regression split dictionary.\n",
    "with open(f'{DL_DIR}/comb_reg_dict.pkl', 'rb') as f:\n",
    "    comb_reg_dict = pickle.load(f)\n",
    "\n",
    "with open(f'{DL_DIR}/fitbit_reg_dict.pkl', 'rb') as f:\n",
    "    fitbit_reg_dict = pickle.load(f)\n",
    "\n",
    "# Load the classification split dictionary.\n",
    "with open(f'{DL_DIR}/comb_class_dict.pkl', 'rb') as f:\n",
    "    comb_class_dict = pickle.load(f)\n",
    "\n",
    "with open(f'{DL_DIR}/fitbit_class_dict.pkl', 'rb') as f:\n",
    "    fitbit_class_dict = pickle.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eea5ae9-055d-4ced-b7fb-2b0a36d284e2",
   "metadata": {},
   "source": [
    "### Utility Functions & Dynamic Model Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ec9f1e-b1b3-42a6-97d1-bbb14aa09cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_output_length(L_in, pool_kernel):\n",
    "    \"\"\"Assuming stride == pool_kernel for pooling.\"\"\"\n",
    "    return L_in // pool_kernel\n",
    "\n",
    "def create_subject_dataset(df, outcome_col=\"SI_mean\"):\n",
    "    \"\"\"\n",
    "    Aggregates records for each subject into a subject-level sample.\n",
    "    Returns a DataFrame with:\n",
    "      - 'X': predictors with shape (n_features, 39)\n",
    "      - the outcome (given by outcome_col)\n",
    "      - sample_weight (if provided)\n",
    "      - a stratification column based on the outcome.\n",
    "    \n",
    "    This function assumes that each subject already has exactly 39 timepoints.\n",
    "    For classification (when outcome_col==\"is_SI\"), if the SI_mean column is absent,\n",
    "    it is not added.\n",
    "    \"\"\"\n",
    "    exclude_cols = [\"PatientID\", \"timepoints\", \"si_kde_weight\", \"SI_mean\", \"is_SI\", \"SI_level\"]\n",
    "    predictor_cols = [col for col in df.columns if col not in (exclude_cols + [outcome_col])]\n",
    "    \n",
    "    subject_data = []\n",
    "    for pid, group in df.groupby(\"PatientID\"):\n",
    "        group_sorted = group.sort_values(\"timepoints\")\n",
    "        # Assume each subject has exactly 39 timepoints.\n",
    "        X = group_sorted[predictor_cols].values.T  # shape: (n_features, 39)\n",
    "        y = group_sorted[outcome_col].iloc[0]\n",
    "        weight = group_sorted[\"si_kde_weight\"].iloc[0] if \"si_kde_weight\" in group.columns else 1.0\n",
    "        record = {\"PatientID\": pid, \"X\": X, outcome_col: y, \"sample_weight\": weight}\n",
    "        if outcome_col == \"is_SI\" and \"SI_mean\" in group_sorted.columns:\n",
    "            record[\"SI_mean\"] = group_sorted[\"SI_mean\"].iloc[0]\n",
    "        subject_data.append(record)\n",
    "    subj_df = pd.DataFrame(subject_data)\n",
    "    subj_df[f\"{outcome_col}_bin\"] = np.round(subj_df[outcome_col]).astype(int)\n",
    "    return subj_df, predictor_cols\n",
    "\n",
    "def get_stratified_cv_splits(df, subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5):\n",
    "    \"\"\"\n",
    "    Performs stratified K-fold cross validation at the subject level.\n",
    "    \n",
    "    Parameters:\n",
    "      df : pandas.DataFrame\n",
    "          The original dataframe containing repeated measures.\n",
    "      subject_id : str\n",
    "          The column name for the subject ID (e.g., \"PatientID\").\n",
    "      target_var : str\n",
    "          The target variable; for regression use \"SI_mean\" and for classification use \"is_SI\".\n",
    "      n_splits : int\n",
    "          Number of folds for cross validation.\n",
    "    \n",
    "    Returns:\n",
    "      splits : list of tuples\n",
    "          A list where each element is a tuple (train_df, test_df) corresponding\n",
    "          to one fold. Each dataframe contains all rows (i.e. repeated measures) for the patients in that fold.\n",
    "    \n",
    "    Behavior:\n",
    "      - Isolates unique patient IDs and their target variable by dropping duplicates.\n",
    "      - If target_var is \"SI_mean\", creates a new column \"SI_mean_levels\" (rounded SI_mean).\n",
    "      - Uses the resulting column as the stratification column.\n",
    "      - Performs stratified K-fold CV and then subsets the original dataframe based on the patient IDs.\n",
    "    \"\"\"\n",
    "    # Create a subject-level dataframe (unique patient IDs with their target variable)\n",
    "    subject_df = df[[subject_id, target_var]].drop_duplicates(subset=[subject_id]).copy()\n",
    "    \n",
    "    # For regression: create a new column with the rounded SI_mean values.\n",
    "    if target_var == \"SI_mean\":\n",
    "        subject_df[\"SI_mean_levels\"] = subject_df[target_var].round().astype(int)\n",
    "        strat_col = \"SI_mean_levels\"\n",
    "    else:\n",
    "        strat_col = target_var  # For classification, use the target directly.\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    splits = []\n",
    "    \n",
    "    # Get the subject IDs and stratification labels\n",
    "    subjects = subject_df[subject_id].values\n",
    "    strat_labels = subject_df[strat_col].values\n",
    "    \n",
    "    # For each fold, retrieve patient IDs and then subset the original dataframe.\n",
    "    for train_idx, test_idx in skf.split(subjects, strat_labels):\n",
    "        train_patient_ids = subject_df.iloc[train_idx][subject_id].values\n",
    "        test_patient_ids  = subject_df.iloc[test_idx][subject_id].values\n",
    "        train_split = df[df[subject_id].isin(train_patient_ids)]\n",
    "        test_split  = df[df[subject_id].isin(test_patient_ids)]\n",
    "        splits.append((train_split, test_split))\n",
    "    \n",
    "    return splits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Updated Dynamic Models with Dropout Using LSTM\n",
    "\n",
    "class DynamicLSTMRegression(nn.Module):\n",
    "    \"\"\"\n",
    "    Builds a dynamic LSTM model for regression based on hyperparameters from an Optuna trial.\n",
    "    The model searches over the number of LSTM layers, hidden size, bidirectionality, dropout, \n",
    "    and additional fully connected layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, seq_len, trial):\n",
    "        super(DynamicLSTMRegression, self).__init__()\n",
    "        # Hyperparameter search for LSTM architecture\n",
    "        n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "        hidden_size = trial.suggest_int(\"hidden_size\", 16, 128, step=16)\n",
    "        bidirectional = trial.suggest_categorical(\"bidirectional\", [True, False])\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout if n_layers > 1 else 0.0,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Determine the size of the LSTM output for the fully connected layers.\n",
    "        fc_input_dim = hidden_size * 2 if bidirectional else hidden_size\n",
    "        \n",
    "        # Optionally add fully connected layers.\n",
    "        n_fc_layers = trial.suggest_int(\"n_fc_layers\", 0, 2)\n",
    "        fc_layers = []\n",
    "        in_features = fc_input_dim\n",
    "        for i in range(n_fc_layers):\n",
    "            fc_units = trial.suggest_int(f\"fc_units_{i}\", 16, 128, step=16)\n",
    "            fc_layers.append(nn.Linear(in_features, fc_units))\n",
    "            fc_layers.append(nn.ReLU())\n",
    "            fc_layers.append(nn.Dropout(dropout))\n",
    "            in_features = fc_units\n",
    "        fc_layers.append(nn.Linear(in_features, 1))\n",
    "        self.fc = nn.Sequential(*fc_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input x shape: (batch_size, n_features, seq_len)\n",
    "        # Transpose to (batch_size, seq_len, n_features) for LSTM\n",
    "        x = x.transpose(1, 2)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Use the output at the last time step for prediction.\n",
    "        out = lstm_out[:, -1, :]\n",
    "        return self.fc(out)\n",
    "\n",
    "class DynamicLSTMClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    Builds a dynamic LSTM model for binary classification based on hyperparameters from an Optuna trial.\n",
    "    The model searches over LSTM architecture parameters (layers, hidden size, bidirectionality, dropout)\n",
    "    and over the fully connected layers following the LSTM.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, seq_len, trial):\n",
    "        super(DynamicLSTMClassification, self).__init__()\n",
    "        n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "        hidden_size = trial.suggest_int(\"hidden_size\", 16, 128, step=16)\n",
    "        bidirectional = trial.suggest_categorical(\"bidirectional\", [True, False])\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout if n_layers > 1 else 0.0,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        fc_input_dim = hidden_size * 2 if bidirectional else hidden_size\n",
    "        \n",
    "        n_fc_layers = trial.suggest_int(\"n_fc_layers\", 0, 2)\n",
    "        fc_layers = []\n",
    "        in_features = fc_input_dim\n",
    "        for i in range(n_fc_layers):\n",
    "            fc_units = trial.suggest_int(f\"fc_units_{i}\", 16, 128, step=16)\n",
    "            fc_layers.append(nn.Linear(in_features, fc_units))\n",
    "            fc_layers.append(nn.ReLU())\n",
    "            fc_layers.append(nn.Dropout(dropout))\n",
    "            in_features = fc_units\n",
    "        # Final layer produces one logit for binary classification.\n",
    "        fc_layers.append(nn.Linear(in_features, 1))\n",
    "        self.fc = nn.Sequential(*fc_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input x shape: (batch_size, n_features, seq_len)\n",
    "        # Transpose to (batch_size, seq_len, n_features) for LSTM\n",
    "        x = x.transpose(1, 2)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = lstm_out[:, -1, :]\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0083479-6a47-4b33-9952-dba5eb3f13e2",
   "metadata": {},
   "source": [
    "### Objective Function with 5-Fold Stratified Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ee47511-1800-4449-a212-61936413212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_regression(trial, data_dict, use_sample_weights, model_name):\n",
    "    set_seed(42)\n",
    "    # Process only the training portion from the dictionary.\n",
    "    train_df, _ = create_subject_dataset(data_dict['train'], outcome_col=\"SI_mean\")\n",
    "    X = np.stack(train_df[\"X\"].values, axis=0)  # shape: (n_subjects, n_features, 39)\n",
    "    y = train_df[\"SI_mean\"].values\n",
    "    w = train_df[\"sample_weight\"].values if use_sample_weights else np.ones_like(y, dtype=np.float32)\n",
    "    n_subjects, input_channels, seq_len = X.shape\n",
    "\n",
    "    # Get the stratified CV splits from the helper function.\n",
    "    cv_splits = get_stratified_cv_splits(train_df, subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5)\n",
    "\n",
    "    # Suggest hyperparameters common to training.\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    use_reg = trial.suggest_categorical(\"use_regularization\", [True, False])\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3) if use_reg else 0.0\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 16, 64, step=16)\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 5, 10)\n",
    "\n",
    "    rmse_list = []\n",
    "    for cv_train_df, cv_val_df in cv_splits:\n",
    "        X_train_fold = np.stack(cv_train_df[\"X\"].values, axis=0)\n",
    "        y_train_fold = cv_train_df[\"SI_mean\"].values\n",
    "        w_train_fold = cv_train_df[\"sample_weight\"].values if use_sample_weights else np.ones_like(y_train_fold, dtype=np.float32)\n",
    "        X_val_fold   = np.stack(cv_val_df[\"X\"].values, axis=0)\n",
    "        y_val_fold   = cv_val_df[\"SI_mean\"].values\n",
    "\n",
    "        # Build the dynamic LSTM regression model with the trial’s hyperparameter choices.\n",
    "        model = DynamicLSTMRegression(input_channels, seq_len, trial).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "        train_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_train_fold, dtype=torch.float32),\n",
    "            torch.tensor(y_train_fold, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(w_train_fold, dtype=torch.float32)\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        \n",
    "        model.train()\n",
    "        for ep in range(num_epochs):\n",
    "            for X_batch, y_batch, weight_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                weight_batch = weight_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss_per_sample = loss_fn(outputs, y_batch).view(-1)\n",
    "                loss = (loss_per_sample * weight_batch).mean() if use_sample_weights else loss_per_sample.mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.tensor(X_val_fold, dtype=torch.float32).to(device)\n",
    "            preds = model(X_val_tensor).cpu().numpy()\n",
    "        fold_rmse = np.sqrt(np.mean((preds - y_val_fold.reshape(-1, 1)) ** 2))\n",
    "        rmse_list.append(fold_rmse)\n",
    "    \n",
    "    mean_rmse = np.mean(rmse_list)\n",
    "    return mean_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a0ebb1-b342-4387-89b5-27cc7b449cb3",
   "metadata": {},
   "source": [
    "###  Run the Superficial Optuna Search & Save Hyperparameter Importance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c8633d6-3d37-49ac-8cf7-aeeec03096df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_superficial_search_regression(data_dict, use_sample_weights, model_name, n_trials=5):\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    def print_progress(study, trial):\n",
    "        print(f\"{model_name} trial {len(study.trials)}/{n_trials}\\r\", end=\"\", flush=True)\n",
    "    \n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(lambda trial: objective_regression(trial, data_dict, use_sample_weights, model_name),\n",
    "                   n_trials=n_trials, callbacks=[print_progress])\n",
    "    \n",
    "    fig = optuna.visualization.plot_param_importances(study)\n",
    "    plot_filename = f\"search/{model_name}_hyperparam_importance.png\"\n",
    "    fig.write_image(plot_filename)\n",
    "    \n",
    "    rows = []\n",
    "    for t in study.trials:\n",
    "        row = {\"model\": model_name,\n",
    "               \"type\": \"weighted\" if use_sample_weights else \"not weighted\",\n",
    "               \"overall_rmse\": t.value,\n",
    "               \"config\": t.params}\n",
    "        rows.append(row)\n",
    "    result_df = pd.DataFrame(rows)\n",
    "    \n",
    "    best_trial = study.best_trial\n",
    "    optimal_configuration = {\"value\": best_trial.value,\n",
    "                             \"params\": best_trial.params,\n",
    "                             \"user_attrs\": best_trial.user_attrs}\n",
    "    \n",
    "    result_dict = {\"results\": result_df,\n",
    "                   \"optimal_configuration\": optimal_configuration,\n",
    "                   \"importance_plot\": plot_filename}\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbebba5-786c-4e3b-874d-6176a44a7c6e",
   "metadata": {},
   "source": [
    "### Run an Optuna study for a given dataset and weighting configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21c7947f-c8e2-4cc9-8edb-21e6a2fe4ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comb_reg_superficial trial 20/20\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'results':                    model      type  overall_rmse  \\\n",
       " 0   comb_reg_superficial  weighted      1.562145   \n",
       " 1   comb_reg_superficial  weighted      1.671992   \n",
       " 2   comb_reg_superficial  weighted      1.625948   \n",
       " 3   comb_reg_superficial  weighted      1.324943   \n",
       " 4   comb_reg_superficial  weighted      1.699000   \n",
       " 5   comb_reg_superficial  weighted      1.716119   \n",
       " 6   comb_reg_superficial  weighted      1.232757   \n",
       " 7   comb_reg_superficial  weighted      1.717029   \n",
       " 8   comb_reg_superficial  weighted      1.532924   \n",
       " 9   comb_reg_superficial  weighted      1.574785   \n",
       " 10  comb_reg_superficial  weighted      1.234916   \n",
       " 11  comb_reg_superficial  weighted      1.304396   \n",
       " 12  comb_reg_superficial  weighted      0.665981   \n",
       " 13  comb_reg_superficial  weighted      1.705783   \n",
       " 14  comb_reg_superficial  weighted      1.766074   \n",
       " 15  comb_reg_superficial  weighted      1.405695   \n",
       " 16  comb_reg_superficial  weighted      1.253953   \n",
       " 17  comb_reg_superficial  weighted      0.943573   \n",
       " 18  comb_reg_superficial  weighted      0.890813   \n",
       " 19  comb_reg_superficial  weighted      1.690625   \n",
       " \n",
       "                                                config  \n",
       " 0   {'lr': 0.0006204297287792125, 'use_regularizat...  \n",
       " 1   {'lr': 0.0010068577804446155, 'use_regularizat...  \n",
       " 2   {'lr': 0.000613995219269057, 'use_regularizati...  \n",
       " 3   {'lr': 1.2344554809508611e-05, 'use_regulariza...  \n",
       " 4   {'lr': 0.0051160314048413974, 'use_regularizat...  \n",
       " 5   {'lr': 0.0005401510238395647, 'use_regularizat...  \n",
       " 6   {'lr': 1.7009002090851563e-05, 'use_regulariza...  \n",
       " 7   {'lr': 0.00342424643853544, 'use_regularizatio...  \n",
       " 8   {'lr': 0.001710068803184453, 'use_regularizati...  \n",
       " 9   {'lr': 0.0025086554057105544, 'use_regularizat...  \n",
       " 10  {'lr': 4.498774577899626e-05, 'use_regularizat...  \n",
       " 11  {'lr': 3.1063204944636956e-05, 'use_regulariza...  \n",
       " 12  {'lr': 9.721841613075093e-05, 'use_regularizat...  \n",
       " 13  {'lr': 0.0001305296020010999, 'use_regularizat...  \n",
       " 14  {'lr': 0.0001479307402581733, 'use_regularizat...  \n",
       " 15  {'lr': 1.0024396000552915e-05, 'use_regulariza...  \n",
       " 16  {'lr': 4.479166005891897e-05, 'use_regularizat...  \n",
       " 17  {'lr': 0.0001591517905546012, 'use_regularizat...  \n",
       " 18  {'lr': 0.00016309585999637423, 'use_regulariza...  \n",
       " 19  {'lr': 0.0002710467091550024, 'use_regularizat...  ,\n",
       " 'optimal_configuration': {'value': 0.6659812100324526,\n",
       "  'params': {'lr': 9.721841613075093e-05,\n",
       "   'use_regularization': True,\n",
       "   'weight_decay': 0.0007840825684073759,\n",
       "   'batch_size': 48,\n",
       "   'num_epochs': 10,\n",
       "   'n_layers': 2,\n",
       "   'hidden_size': 32,\n",
       "   'bidirectional': False,\n",
       "   'dropout': 0.486393219859214,\n",
       "   'n_fc_layers': 0},\n",
       "  'user_attrs': {}},\n",
       " 'importance_plot': 'search/comb_reg_superficial_hyperparam_importance.png'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_superficial_search_regression(comb_reg_dict, use_sample_weights=True, model_name=\"comb_reg_superficial\", n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed39d7da-d990-4906-8355-f9824318e05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitbit_reg_superficial trial 20/20\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'results':                      model      type  overall_rmse  \\\n",
       " 0   fitbit_reg_superficial  weighted      1.607195   \n",
       " 1   fitbit_reg_superficial  weighted      1.640821   \n",
       " 2   fitbit_reg_superficial  weighted      1.315669   \n",
       " 3   fitbit_reg_superficial  weighted      1.308216   \n",
       " 4   fitbit_reg_superficial  weighted      1.715482   \n",
       " 5   fitbit_reg_superficial  weighted      1.788118   \n",
       " 6   fitbit_reg_superficial  weighted      1.180780   \n",
       " 7   fitbit_reg_superficial  weighted      1.424092   \n",
       " 8   fitbit_reg_superficial  weighted      1.634825   \n",
       " 9   fitbit_reg_superficial  weighted      1.568106   \n",
       " 10  fitbit_reg_superficial  weighted      1.766472   \n",
       " 11  fitbit_reg_superficial  weighted      1.397064   \n",
       " 12  fitbit_reg_superficial  weighted      1.563550   \n",
       " 13  fitbit_reg_superficial  weighted      1.160402   \n",
       " 14  fitbit_reg_superficial  weighted      1.268800   \n",
       " 15  fitbit_reg_superficial  weighted      1.318844   \n",
       " 16  fitbit_reg_superficial  weighted      1.668759   \n",
       " 17  fitbit_reg_superficial  weighted      0.897219   \n",
       " 18  fitbit_reg_superficial  weighted      1.013270   \n",
       " 19  fitbit_reg_superficial  weighted      1.836024   \n",
       " \n",
       "                                                config  \n",
       " 0   {'lr': 0.0010629376507396721, 'use_regularizat...  \n",
       " 1   {'lr': 0.0004997992747368873, 'use_regularizat...  \n",
       " 2   {'lr': 1.861686594229207e-05, 'use_regularizat...  \n",
       " 3   {'lr': 1.3676618692727845e-05, 'use_regulariza...  \n",
       " 4   {'lr': 0.0006209800118450807, 'use_regularizat...  \n",
       " 5   {'lr': 0.007721816045957088, 'use_regularizati...  \n",
       " 6   {'lr': 7.49894002984658e-05, 'use_regularizati...  \n",
       " 7   {'lr': 0.0003437931795074398, 'use_regularizat...  \n",
       " 8   {'lr': 0.0018085925978674475, 'use_regularizat...  \n",
       " 9   {'lr': 0.0015467299370521101, 'use_regularizat...  \n",
       " 10  {'lr': 6.765051289182273e-05, 'use_regularizat...  \n",
       " 11  {'lr': 1.0985663373712333e-05, 'use_regulariza...  \n",
       " 12  {'lr': 7.596695006087033e-05, 'use_regularizat...  \n",
       " 13  {'lr': 4.7261109744353376e-05, 'use_regulariza...  \n",
       " 14  {'lr': 9.244014477809343e-05, 'use_regularizat...  \n",
       " 15  {'lr': 3.3800043140088054e-05, 'use_regulariza...  \n",
       " 16  {'lr': 0.0001880084463109107, 'use_regularizat...  \n",
       " 17  {'lr': 0.00015981757873712114, 'use_regulariza...  \n",
       " 18  {'lr': 0.00017808360511869565, 'use_regulariza...  \n",
       " 19  {'lr': 0.0001750010831158489, 'use_regularizat...  ,\n",
       " 'optimal_configuration': {'value': 0.8972185997126877,\n",
       "  'params': {'lr': 0.00015981757873712114,\n",
       "   'use_regularization': True,\n",
       "   'weight_decay': 0.0002585908452608643,\n",
       "   'batch_size': 48,\n",
       "   'num_epochs': 7,\n",
       "   'n_layers': 1,\n",
       "   'hidden_size': 48,\n",
       "   'bidirectional': False,\n",
       "   'dropout': 0.4516931919367047,\n",
       "   'n_fc_layers': 1,\n",
       "   'fc_units_0': 96},\n",
       "  'user_attrs': {}},\n",
       " 'importance_plot': 'search/fitbit_reg_superficial_hyperparam_importance.png'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_superficial_search_regression(fitbit_reg_dict, use_sample_weights=True, model_name=\"fitbit_reg_superficial\", n_trials=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785b961f-23e3-4a56-a393-885d03d189db",
   "metadata": {},
   "source": [
    "#### No weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab37555c-ad9a-43ab-9fd4-9dcfa00800c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comb_reg_superficial_nw trial 20/20\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'results':                       model          type  overall_rmse  \\\n",
       " 0   comb_reg_superficial_nw  not weighted      0.620188   \n",
       " 1   comb_reg_superficial_nw  not weighted      0.596405   \n",
       " 2   comb_reg_superficial_nw  not weighted      1.193700   \n",
       " 3   comb_reg_superficial_nw  not weighted      1.115172   \n",
       " 4   comb_reg_superficial_nw  not weighted      0.596681   \n",
       " 5   comb_reg_superficial_nw  not weighted      1.093058   \n",
       " 6   comb_reg_superficial_nw  not weighted      0.629297   \n",
       " 7   comb_reg_superficial_nw  not weighted      1.319175   \n",
       " 8   comb_reg_superficial_nw  not weighted      0.627419   \n",
       " 9   comb_reg_superficial_nw  not weighted      0.673436   \n",
       " 10  comb_reg_superficial_nw  not weighted      0.607532   \n",
       " 11  comb_reg_superficial_nw  not weighted      0.628660   \n",
       " 12  comb_reg_superficial_nw  not weighted      0.594348   \n",
       " 13  comb_reg_superficial_nw  not weighted      0.600590   \n",
       " 14  comb_reg_superficial_nw  not weighted      0.601746   \n",
       " 15  comb_reg_superficial_nw  not weighted      0.592903   \n",
       " 16  comb_reg_superficial_nw  not weighted      0.601969   \n",
       " 17  comb_reg_superficial_nw  not weighted      0.610140   \n",
       " 18  comb_reg_superficial_nw  not weighted      0.596607   \n",
       " 19  comb_reg_superficial_nw  not weighted      0.600845   \n",
       " \n",
       "                                                config  \n",
       " 0   {'lr': 0.00618982129770469, 'use_regularizatio...  \n",
       " 1   {'lr': 0.00020462955726637298, 'use_regulariza...  \n",
       " 2   {'lr': 1.3826619802757098e-05, 'use_regulariza...  \n",
       " 3   {'lr': 2.1507379859362636e-05, 'use_regulariza...  \n",
       " 4   {'lr': 0.00015929827651257932, 'use_regulariza...  \n",
       " 5   {'lr': 5.2994674936096346e-05, 'use_regulariza...  \n",
       " 6   {'lr': 7.191999034660834e-05, 'use_regularizat...  \n",
       " 7   {'lr': 2.6630046697362307e-05, 'use_regulariza...  \n",
       " 8   {'lr': 0.002279901359347023, 'use_regularizati...  \n",
       " 9   {'lr': 0.005124505328267663, 'use_regularizati...  \n",
       " 10  {'lr': 0.0004663269742113057, 'use_regularizat...  \n",
       " 11  {'lr': 0.0003183053938264526, 'use_regularizat...  \n",
       " 12  {'lr': 0.00010893672069434237, 'use_regulariza...  \n",
       " 13  {'lr': 0.0009538383946567556, 'use_regularizat...  \n",
       " 14  {'lr': 0.00011222281047403993, 'use_regulariza...  \n",
       " 15  {'lr': 0.0007397940380189501, 'use_regularizat...  \n",
       " 16  {'lr': 0.0008612885284981993, 'use_regularizat...  \n",
       " 17  {'lr': 0.0019472540943603258, 'use_regularizat...  \n",
       " 18  {'lr': 0.0006247623907044399, 'use_regularizat...  \n",
       " 19  {'lr': 5.311043717689544e-05, 'use_regularizat...  ,\n",
       " 'optimal_configuration': {'value': 0.5929031371357374,\n",
       "  'params': {'lr': 0.0007397940380189501,\n",
       "   'use_regularization': False,\n",
       "   'batch_size': 64,\n",
       "   'num_epochs': 5,\n",
       "   'n_layers': 3,\n",
       "   'hidden_size': 64,\n",
       "   'bidirectional': False,\n",
       "   'dropout': 0.10082518827553788,\n",
       "   'n_fc_layers': 0},\n",
       "  'user_attrs': {}},\n",
       " 'importance_plot': 'search/comb_reg_superficial_nw_hyperparam_importance.png'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_superficial_search_regression(comb_reg_dict, use_sample_weights=False, model_name=\"comb_reg_superficial_nw\", n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9982c7c-6c18-4e4e-8f4e-6ba06e8b8930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitbit_reg_superficial_nw trial 20/20\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'results':                         model          type  overall_rmse  \\\n",
       " 0   fitbit_reg_superficial_nw  not weighted      0.620028   \n",
       " 1   fitbit_reg_superficial_nw  not weighted      0.605539   \n",
       " 2   fitbit_reg_superficial_nw  not weighted      0.627281   \n",
       " 3   fitbit_reg_superficial_nw  not weighted      0.598081   \n",
       " 4   fitbit_reg_superficial_nw  not weighted      1.242475   \n",
       " 5   fitbit_reg_superficial_nw  not weighted      1.257282   \n",
       " 6   fitbit_reg_superficial_nw  not weighted      0.609245   \n",
       " 7   fitbit_reg_superficial_nw  not weighted      1.305235   \n",
       " 8   fitbit_reg_superficial_nw  not weighted      0.615307   \n",
       " 9   fitbit_reg_superficial_nw  not weighted      0.625439   \n",
       " 10  fitbit_reg_superficial_nw  not weighted      0.598953   \n",
       " 11  fitbit_reg_superficial_nw  not weighted      0.598079   \n",
       " 12  fitbit_reg_superficial_nw  not weighted      0.597776   \n",
       " 13  fitbit_reg_superficial_nw  not weighted      0.597429   \n",
       " 14  fitbit_reg_superficial_nw  not weighted      0.598615   \n",
       " 15  fitbit_reg_superficial_nw  not weighted      0.597083   \n",
       " 16  fitbit_reg_superficial_nw  not weighted      0.595763   \n",
       " 17  fitbit_reg_superficial_nw  not weighted      0.600214   \n",
       " 18  fitbit_reg_superficial_nw  not weighted      0.595719   \n",
       " 19  fitbit_reg_superficial_nw  not weighted      0.601582   \n",
       " \n",
       "                                                config  \n",
       " 0   {'lr': 0.0028611496468716664, 'use_regularizat...  \n",
       " 1   {'lr': 0.0013152129789799822, 'use_regularizat...  \n",
       " 2   {'lr': 0.002249458652992545, 'use_regularizati...  \n",
       " 3   {'lr': 0.0003350931326483147, 'use_regularizat...  \n",
       " 4   {'lr': 1.44742175394377e-05, 'use_regularizati...  \n",
       " 5   {'lr': 2.765734949294271e-05, 'use_regularizat...  \n",
       " 6   {'lr': 0.0016827233963724242, 'use_regularizat...  \n",
       " 7   {'lr': 1.9973073450796836e-05, 'use_regulariza...  \n",
       " 8   {'lr': 0.0017001844965475212, 'use_regularizat...  \n",
       " 9   {'lr': 0.003703110961284166, 'use_regularizati...  \n",
       " 10  {'lr': 0.00018637741660493327, 'use_regulariza...  \n",
       " 11  {'lr': 0.00015979125841179525, 'use_regulariza...  \n",
       " 12  {'lr': 0.00015959206715057786, 'use_regulariza...  \n",
       " 13  {'lr': 9.764358169124981e-05, 'use_regularizat...  \n",
       " 14  {'lr': 6.209623537292185e-05, 'use_regularizat...  \n",
       " 15  {'lr': 0.0005691848021457963, 'use_regularizat...  \n",
       " 16  {'lr': 0.0005400908846635865, 'use_regularizat...  \n",
       " 17  {'lr': 0.0006613135825698836, 'use_regularizat...  \n",
       " 18  {'lr': 0.0005764059158406173, 'use_regularizat...  \n",
       " 19  {'lr': 0.009906635796199121, 'use_regularizati...  ,\n",
       " 'optimal_configuration': {'value': 0.5957189245716175,\n",
       "  'params': {'lr': 0.0005764059158406173,\n",
       "   'use_regularization': False,\n",
       "   'batch_size': 64,\n",
       "   'num_epochs': 7,\n",
       "   'n_layers': 3,\n",
       "   'hidden_size': 48,\n",
       "   'bidirectional': False,\n",
       "   'dropout': 0.24676475915872623,\n",
       "   'n_fc_layers': 0},\n",
       "  'user_attrs': {}},\n",
       " 'importance_plot': 'search/fitbit_reg_superficial_nw_hyperparam_importance.png'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_superficial_search_regression(fitbit_reg_dict, use_sample_weights=False, model_name=\"fitbit_reg_superficial_nw\", n_trials=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcfe371-b318-4515-901e-d565a20e061d",
   "metadata": {},
   "source": [
    "### Superficial search for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef7ea051-f4c4-4cad-851e-7025917d945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_classification(trial, data_dict, use_sample_weights, model_name):\n",
    "    set_seed(42)\n",
    "    # Process only the training portion.\n",
    "    train_df, _ = create_subject_dataset(data_dict['train'], outcome_col=\"is_SI\")\n",
    "    X = np.stack(train_df[\"X\"].values, axis=0)\n",
    "    y = train_df[\"is_SI\"].values.astype(np.float32)\n",
    "    w = train_df[\"sample_weight\"].values if use_sample_weights else np.ones_like(y, dtype=np.float32)\n",
    "    n_subjects, input_channels, seq_len = X.shape\n",
    "\n",
    "    # Get stratified CV splits using the helper function.\n",
    "    cv_splits = get_stratified_cv_splits(train_df, subject_id=\"PatientID\", target_var=\"is_SI\", n_splits=5)\n",
    "\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    use_reg = trial.suggest_categorical(\"use_regularization\", [True, False])\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3) if use_reg else 0.0\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 16, 64, step=16)\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 5, 10)\n",
    "\n",
    "    acc_list = []\n",
    "    sens_list = []\n",
    "    spec_list = []\n",
    "    for cv_train_df, cv_val_df in cv_splits:\n",
    "        X_train_fold = np.stack(cv_train_df[\"X\"].values, axis=0)\n",
    "        y_train_fold = cv_train_df[\"is_SI\"].values.astype(np.float32)\n",
    "        w_train_fold = cv_train_df[\"sample_weight\"].values if use_sample_weights else np.ones_like(y_train_fold, dtype=np.float32)\n",
    "        X_val_fold = np.stack(cv_val_df[\"X\"].values, axis=0)\n",
    "        y_val_fold = cv_val_df[\"is_SI\"].values.astype(np.float32)\n",
    "\n",
    "        # Build the dynamic LSTM classification model with the trial's hyperparameters.\n",
    "        model = DynamicLSTMClassification(input_channels, seq_len, trial).to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        \n",
    "        train_dataset = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_train_fold, dtype=torch.float32),\n",
    "            torch.tensor(y_train_fold, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(w_train_fold, dtype=torch.float32)\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        \n",
    "        model.train()\n",
    "        for ep in range(num_epochs):\n",
    "            for X_batch, y_batch, weight_batch in train_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                weight_batch = weight_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss_per_sample = loss_fn(outputs, y_batch).view(-1)\n",
    "                loss = (loss_per_sample * weight_batch).mean() if use_sample_weights else loss_per_sample.mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.tensor(X_val_fold, dtype=torch.float32).to(device)\n",
    "            logits = model(X_val_tensor)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().reshape(-1)\n",
    "        preds = (probs >= 0.5).astype(np.float32)\n",
    "        overall_acc = np.mean(preds == y_val_fold)\n",
    "        TP = np.sum((preds == 1) & (y_val_fold == 1))\n",
    "        FN = np.sum((preds == 0) & (y_val_fold == 1))\n",
    "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "        TN = np.sum((preds == 0) & (y_val_fold == 0))\n",
    "        FP = np.sum((preds == 1) & (y_val_fold == 0))\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
    "        \n",
    "        acc_list.append(overall_acc)\n",
    "        sens_list.append(sensitivity)\n",
    "        spec_list.append(specificity)\n",
    "    \n",
    "    mean_acc = np.mean(acc_list)\n",
    "    mean_sens = np.mean(sens_list)\n",
    "    mean_spec = np.mean(spec_list)\n",
    "    \n",
    "    # For the objective function, we return 1 - mean accuracy (to minimize).\n",
    "    return 1 - mean_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ffb5f19-30b9-4f29-a407-1165f8c97086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_superficial_search_classification(data_dict, use_sample_weights, model_name, n_trials=5):\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    def print_progress(study, trial):\n",
    "        print(f\"{model_name} trial {len(study.trials)}/{n_trials}\\r\", end=\"\", flush=True)\n",
    "    \n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(lambda trial: objective_classification(trial, data_dict, use_sample_weights, model_name),\n",
    "                   n_trials=n_trials, callbacks=[print_progress])\n",
    "    \n",
    "    fig = optuna.visualization.plot_param_importances(study)\n",
    "    plot_filename = f\"search/{model_name}_hyperparam_importance.png\"\n",
    "    fig.write_image(plot_filename)\n",
    "    \n",
    "    rows = []\n",
    "    for t in study.trials:\n",
    "        row = {\"model\": model_name,\n",
    "               \"type\": \"weighted\" if use_sample_weights else \"not weighted\",\n",
    "               \"1 - accuracy\": 1 - t.value,  # since objective is 1-accuracy\n",
    "               \"config\": t.params}\n",
    "        rows.append(row)\n",
    "    result_df = pd.DataFrame(rows)\n",
    "    \n",
    "    best_trial = study.best_trial\n",
    "    optimal_configuration = {\"value\": best_trial.value,\n",
    "                             \"params\": best_trial.params,\n",
    "                             \"user_attrs\": best_trial.user_attrs}\n",
    "    \n",
    "    result_dict = {\"results\": result_df,\n",
    "                   \"optimal_configuration\": optimal_configuration,\n",
    "                   \"importance_plot\": plot_filename}\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fda3e0b-6b1c-4cb3-a481-a3bd3edc41b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-04-05 16:39:59,924] Trial 0 failed with parameters: {'lr': 0.008578525404781486, 'use_regularization': False, 'batch_size': 32, 'num_epochs': 6, 'n_layers': 3, 'hidden_size': 64, 'bidirectional': True, 'dropout': 0.15719888348738867, 'n_fc_layers': 2, 'fc_units_0': 32, 'fc_units_1': 48} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jeffrw/BIOSTAT629/main_project/modeling/DL_py_env/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_3571150/2524403166.py\", line 7, in <lambda>\n",
      "    study.optimize(lambda trial: objective_classification(trial, data_dict, use_sample_weights, model_name),\n",
      "  File \"/tmp/ipykernel_3571150/1236772839.py\", line 54, in objective_classification\n",
      "    optimizer.step()\n",
      "  File \"/home/jeffrw/BIOSTAT629/main_project/modeling/DL_py_env/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 493, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/home/jeffrw/BIOSTAT629/main_project/modeling/DL_py_env/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/home/jeffrw/BIOSTAT629/main_project/modeling/DL_py_env/lib/python3.10/site-packages/torch/optim/adam.py\", line 244, in step\n",
      "    adam(\n",
      "  File \"/home/jeffrw/BIOSTAT629/main_project/modeling/DL_py_env/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 154, in maybe_fallback\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jeffrw/BIOSTAT629/main_project/modeling/DL_py_env/lib/python3.10/site-packages/torch/optim/adam.py\", line 876, in adam\n",
      "    func(\n",
      "  File \"/home/jeffrw/BIOSTAT629/main_project/modeling/DL_py_env/lib/python3.10/site-packages/torch/optim/adam.py\", line 703, in _multi_tensor_adam\n",
      "    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)\n",
      "KeyboardInterrupt\n",
      "[W 2025-04-05 16:39:59,931] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_superficial_search_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomb_class_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_sample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomb_class_superficial\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m, in \u001b[0;36mrun_superficial_search_classification\u001b[0;34m(data_dict, use_sample_weights, model_name, n_trials)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m trial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(study\u001b[38;5;241m.\u001b[39mtrials)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_trials\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_sample_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m               \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprint_progress\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m fig \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mvisualization\u001b[38;5;241m.\u001b[39mplot_param_importances(study)\n\u001b[1;32m     11\u001b[0m plot_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_hyperparam_importance.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/BIOSTAT629/main_project/modeling/DL_py_env/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/BIOSTAT629/main_project/modeling/DL_py_env/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/BIOSTAT629/main_project/modeling/DL_py_env/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/BIOSTAT629/main_project/modeling/DL_py_env/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/BIOSTAT629/main_project/modeling/DL_py_env/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m, in \u001b[0;36mrun_superficial_search_classification.<locals>.<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m trial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(study\u001b[38;5;241m.\u001b[39mtrials)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_trials\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_sample_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      8\u001b[0m                n_trials\u001b[38;5;241m=\u001b[39mn_trials, callbacks\u001b[38;5;241m=\u001b[39m[print_progress])\n\u001b[1;32m     10\u001b[0m fig \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mvisualization\u001b[38;5;241m.\u001b[39mplot_param_importances(study)\n\u001b[1;32m     11\u001b[0m plot_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_hyperparam_importance.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[10], line 54\u001b[0m, in \u001b[0;36mobjective_classification\u001b[0;34m(trial, data_dict, use_sample_weights, model_name)\u001b[0m\n\u001b[1;32m     52\u001b[0m         loss \u001b[38;5;241m=\u001b[39m (loss_per_sample \u001b[38;5;241m*\u001b[39m weight_batch)\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;28;01mif\u001b[39;00m use_sample_weights \u001b[38;5;28;01melse\u001b[39;00m loss_per_sample\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     53\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 54\u001b[0m         \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/BIOSTAT629/main_project/modeling/DL_py_env/lib/python3.10/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/BIOSTAT629/main_project/modeling/DL_py_env/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/BIOSTAT629/main_project/modeling/DL_py_env/lib/python3.10/site-packages/torch/optim/adam.py:244\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    235\u001b[0m         group,\n\u001b[1;32m    236\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m         state_steps,\n\u001b[1;32m    242\u001b[0m     )\n\u001b[0;32m--> 244\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/BIOSTAT629/main_project/modeling/DL_py_env/lib/python3.10/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/BIOSTAT629/main_project/modeling/DL_py_env/lib/python3.10/site-packages/torch/optim/adam.py:876\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    874\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 876\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/BIOSTAT629/main_project/modeling/DL_py_env/lib/python3.10/site-packages/torch/optim/adam.py:703\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    701\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_sqrt(device_max_exp_avg_sqs)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 703\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[1;32m    706\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_superficial_search_classification(comb_class_dict, use_sample_weights=True, model_name=\"comb_class_superficial\", n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57b8d81-8ed7-4efb-8956-734174e64181",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_superficial_search_classification(fitbit_class_dict, use_sample_weights=True, model_name=\"fitbit_class_superficial\", n_trials=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6142fab-0cb9-4dcd-a1a4-9b4fcfbe3809",
   "metadata": {},
   "source": [
    "### No weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2570aeb3-a80e-45ad-9d4c-9d679a66d174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comb_class_superficial_nw trial 20/20\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'results':                         model          type  1 - accuracy  \\\n",
       " 0   comb_class_superficial_nw  not weighted      0.737175   \n",
       " 1   comb_class_superficial_nw  not weighted      0.657542   \n",
       " 2   comb_class_superficial_nw  not weighted      0.753340   \n",
       " 3   comb_class_superficial_nw  not weighted      0.713705   \n",
       " 4   comb_class_superficial_nw  not weighted      0.737166   \n",
       " 5   comb_class_superficial_nw  not weighted      0.752223   \n",
       " 6   comb_class_superficial_nw  not weighted      0.761720   \n",
       " 7   comb_class_superficial_nw  not weighted      0.761720   \n",
       " 8   comb_class_superficial_nw  not weighted      0.749467   \n",
       " 9   comb_class_superficial_nw  not weighted      0.761720   \n",
       " 10  comb_class_superficial_nw  not weighted      0.762279   \n",
       " 11  comb_class_superficial_nw  not weighted      0.762279   \n",
       " 12  comb_class_superficial_nw  not weighted      0.761163   \n",
       " 13  comb_class_superficial_nw  not weighted      0.762279   \n",
       " 14  comb_class_superficial_nw  not weighted      0.761720   \n",
       " 15  comb_class_superficial_nw  not weighted      0.765072   \n",
       " 16  comb_class_superficial_nw  not weighted      0.756694   \n",
       " 17  comb_class_superficial_nw  not weighted      0.733249   \n",
       " 18  comb_class_superficial_nw  not weighted      0.756698   \n",
       " 19  comb_class_superficial_nw  not weighted      0.761720   \n",
       " \n",
       "                                                config  \n",
       " 0   {'lr': 0.00044364336759249276, 'use_regulariza...  \n",
       " 1   {'lr': 1.8126862252194028e-05, 'use_regulariza...  \n",
       " 2   {'lr': 0.0008011368338257187, 'use_regularizat...  \n",
       " 3   {'lr': 0.0035959007933320086, 'use_regularizat...  \n",
       " 4   {'lr': 0.0004899070792866214, 'use_regularizat...  \n",
       " 5   {'lr': 1.3438793351648942e-05, 'use_regulariza...  \n",
       " 6   {'lr': 1.376783317762319e-05, 'use_regularizat...  \n",
       " 7   {'lr': 0.00015286527229344728, 'use_regulariza...  \n",
       " 8   {'lr': 0.0011842353030135679, 'use_regularizat...  \n",
       " 9   {'lr': 9.661424075811461e-05, 'use_regularizat...  \n",
       " 10  {'lr': 4.318205313838231e-05, 'use_regularizat...  \n",
       " 11  {'lr': 4.0936178381603185e-05, 'use_regulariza...  \n",
       " 12  {'lr': 6.290078284918534e-05, 'use_regularizat...  \n",
       " 13  {'lr': 4.616528462132649e-05, 'use_regularizat...  \n",
       " 14  {'lr': 4.1032501254554705e-05, 'use_regulariza...  \n",
       " 15  {'lr': 0.00013219466784021534, 'use_regulariza...  \n",
       " 16  {'lr': 0.000193657250047809, 'use_regularizati...  \n",
       " 17  {'lr': 0.009730561603113677, 'use_regularizati...  \n",
       " 18  {'lr': 0.00020726294800419302, 'use_regulariza...  \n",
       " 19  {'lr': 9.755196581015265e-05, 'use_regularizat...  ,\n",
       " 'optimal_configuration': {'value': 0.23492787227089518,\n",
       "  'params': {'lr': 0.00013219466784021534,\n",
       "   'use_regularization': False,\n",
       "   'batch_size': 48,\n",
       "   'num_epochs': 9,\n",
       "   'n_layers': 2,\n",
       "   'hidden_size': 112,\n",
       "   'bidirectional': True,\n",
       "   'dropout': 0.22923160035257717,\n",
       "   'n_fc_layers': 1,\n",
       "   'fc_units_0': 96},\n",
       "  'user_attrs': {}},\n",
       " 'importance_plot': 'search/comb_class_superficial_nw_hyperparam_importance.png'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_superficial_search_classification(comb_class_dict, use_sample_weights=False, model_name=\"comb_class_superficial_nw\", n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70e287b5-a2ba-40fd-9fc7-5fc05f87d9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitbit_class_superficial_nw trial 20/20\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'results':                           model          type  1 - accuracy  \\\n",
       " 0   fitbit_class_superficial_nw  not weighted      0.758370   \n",
       " 1   fitbit_class_superficial_nw  not weighted      0.761720   \n",
       " 2   fitbit_class_superficial_nw  not weighted      0.752805   \n",
       " 3   fitbit_class_superficial_nw  not weighted      0.570830   \n",
       " 4   fitbit_class_superficial_nw  not weighted      0.756144   \n",
       " 5   fitbit_class_superficial_nw  not weighted      0.761163   \n",
       " 6   fitbit_class_superficial_nw  not weighted      0.748325   \n",
       " 7   fitbit_class_superficial_nw  not weighted      0.760044   \n",
       " 8   fitbit_class_superficial_nw  not weighted      0.757810   \n",
       " 9   fitbit_class_superficial_nw  not weighted      0.747217   \n",
       " 10  fitbit_class_superficial_nw  not weighted      0.760604   \n",
       " 11  fitbit_class_superficial_nw  not weighted      0.762837   \n",
       " 12  fitbit_class_superficial_nw  not weighted      0.605731   \n",
       " 13  fitbit_class_superficial_nw  not weighted      0.761720   \n",
       " 14  fitbit_class_superficial_nw  not weighted      0.761720   \n",
       " 15  fitbit_class_superficial_nw  not weighted      0.755016   \n",
       " 16  fitbit_class_superficial_nw  not weighted      0.761162   \n",
       " 17  fitbit_class_superficial_nw  not weighted      0.761163   \n",
       " 18  fitbit_class_superficial_nw  not weighted      0.761720   \n",
       " 19  fitbit_class_superficial_nw  not weighted      0.761720   \n",
       " \n",
       "                                                config  \n",
       " 0   {'lr': 0.0010954336828514573, 'use_regularizat...  \n",
       " 1   {'lr': 0.006766224370737214, 'use_regularizati...  \n",
       " 2   {'lr': 0.007148772250748036, 'use_regularizati...  \n",
       " 3   {'lr': 3.363932225058606e-05, 'use_regularizat...  \n",
       " 4   {'lr': 0.0005919541091079127, 'use_regularizat...  \n",
       " 5   {'lr': 8.041116068796287e-05, 'use_regularizat...  \n",
       " 6   {'lr': 0.0012203763194603443, 'use_regularizat...  \n",
       " 7   {'lr': 0.0016992465945921045, 'use_regularizat...  \n",
       " 8   {'lr': 0.006246890422961257, 'use_regularizati...  \n",
       " 9   {'lr': 0.0013427106833758344, 'use_regularizat...  \n",
       " 10  {'lr': 0.00026098284765808656, 'use_regulariza...  \n",
       " 11  {'lr': 7.228504449792424e-05, 'use_regularizat...  \n",
       " 12  {'lr': 1.3007098639710995e-05, 'use_regulariza...  \n",
       " 13  {'lr': 0.0001594585899377398, 'use_regularizat...  \n",
       " 14  {'lr': 6.781960694915567e-05, 'use_regularizat...  \n",
       " 15  {'lr': 1.2106964226805783e-05, 'use_regulariza...  \n",
       " 16  {'lr': 0.0036341859452458203, 'use_regularizat...  \n",
       " 17  {'lr': 0.0004138817559309196, 'use_regularizat...  \n",
       " 18  {'lr': 2.7673295957477852e-05, 'use_regulariza...  \n",
       " 19  {'lr': 0.00014026390739187635, 'use_regulariza...  ,\n",
       " 'optimal_configuration': {'value': 0.23716250914240367,\n",
       "  'params': {'lr': 7.228504449792424e-05,\n",
       "   'use_regularization': False,\n",
       "   'batch_size': 16,\n",
       "   'num_epochs': 10,\n",
       "   'n_layers': 1,\n",
       "   'hidden_size': 64,\n",
       "   'bidirectional': True,\n",
       "   'dropout': 0.39579575453261584,\n",
       "   'n_fc_layers': 2,\n",
       "   'fc_units_0': 16,\n",
       "   'fc_units_1': 32},\n",
       "  'user_attrs': {}},\n",
       " 'importance_plot': 'search/fitbit_class_superficial_nw_hyperparam_importance.png'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_superficial_search_classification(fitbit_class_dict, use_sample_weights=False, model_name=\"fitbit_class_superficial_nw\", n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb44ec69-2bcd-4645-a938-582bea2de9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL_py_env)",
   "language": "python",
   "name": "dl_py_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
