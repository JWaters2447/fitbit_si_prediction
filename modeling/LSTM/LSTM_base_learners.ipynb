{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e99154b4-96a2-460b-b067-678bb4749a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, precision_recall_fscore_support, accuracy_score, roc_auc_score\n",
    "\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\" # set before any torch imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DL_DIR = \"../../data/deep_learning\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d61da12-cc45-4132-a810-f652dcdeb478",
   "metadata": {},
   "source": [
    "### Function for setting the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aa95d45-f10f-4cf3-977b-9b7ed3d4f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    \n",
    "    \n",
    "    return seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e237af3d-92ea-4cc1-9d16-05817b2e5060",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fb99506-5747-460d-8932-3ad53439ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the regression split dictionary.\n",
    "with open(f'{DL_DIR}/comb_reg_dict.pkl', 'rb') as f:\n",
    "    comb_reg_dict = pickle.load(f)\n",
    "\n",
    "with open(f'{DL_DIR}/fitbit_reg_dict.pkl', 'rb') as f:\n",
    "    fitbit_reg_dict = pickle.load(f)\n",
    "\n",
    "# Load the classification split dictionary.\n",
    "with open(f'{DL_DIR}/comb_class_dict.pkl', 'rb') as f:\n",
    "    comb_class_dict = pickle.load(f)\n",
    "\n",
    "with open(f'{DL_DIR}/fitbit_class_dict.pkl', 'rb') as f:\n",
    "    fitbit_class_dict = pickle.load(f) \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa76d33-9179-4ff3-be86-05d2239cbb2a",
   "metadata": {},
   "source": [
    "### Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2919c232-f541-417d-88cb-19c33d548e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRegression(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, dropout_prob=0.5):\n",
    "        super(LSTMRegression, self).__init__()\n",
    "        # Define an LSTM layer with default parameters (hidden_size set to 16)\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=16, num_layers=1, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(16, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, input_size, seq_len]\n",
    "        # Transpose to [batch, seq_len, input_size] for LSTM\n",
    "        x = x.transpose(1, 2)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        # Use the last hidden state from the LSTM (from the final layer)\n",
    "        h_last = h_n[-1]  # shape: [batch, hidden_size]\n",
    "        h_last = self.dropout(h_last)\n",
    "        out = self.fc(h_last)\n",
    "        return out\n",
    "\n",
    "class LSTMClassification(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, dropout_prob=0.5):\n",
    "        super(LSTMClassification, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=16, num_layers=1, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(16, 1)  # single logit for binary classification\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # convert to [batch, seq_len, input_size]\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        h_last = h_n[-1]\n",
    "        h_last = self.dropout(h_last)\n",
    "        out = self.fc(h_last)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742ece70-1a75-4685-b510-52e378388c74",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27f4fce5-8c85-4af2-9241-142b9ca11f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subject_dataset(df, outcome_col=\"SI_mean\"):\n",
    "    \"\"\"\n",
    "    Aggregates records for each subject into a subject-level sample.\n",
    "    Excludes meta/outcome columns and returns a DataFrame with:\n",
    "      - PatientID, outcome (SI_mean or is_SI), sample_weight,\n",
    "      - X: a numpy array of predictors with shape (n_features, 39).\n",
    "    \n",
    "    This function assumes that each subject already has exactly 39 timepoints.\n",
    "    For classification (when outcome_col==\"is_SI\"), if the original DataFrame contains a SI_mean column, it is included.\n",
    "    A stratification column is created by rounding the outcome (used for later splitting).\n",
    "    \"\"\"\n",
    "    exclude_cols = [\"PatientID\", \"timepoints\", \"si_kde_weight\", \"SI_mean\", \"is_SI\", \"SI_level\"]\n",
    "    predictor_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    subject_data = []\n",
    "    for pid, group in df.groupby(\"PatientID\"):\n",
    "        group_sorted = group.sort_values(\"timepoints\")\n",
    "        # Assume each subject already has exactly 39 timepoints.\n",
    "        X = group_sorted[predictor_cols].values.T  # shape: (n_features, 39)\n",
    "        y = group_sorted[outcome_col].iloc[0]\n",
    "        weight = group_sorted[\"si_kde_weight\"].iloc[0] if \"si_kde_weight\" in group.columns else 1.0\n",
    "        record = {\"PatientID\": pid, \"X\": X, outcome_col: y, \"sample_weight\": weight}\n",
    "        if outcome_col == \"is_SI\" and \"SI_mean\" in group_sorted.columns:\n",
    "            record[\"SI_mean\"] = group_sorted[\"SI_mean\"].iloc[0]\n",
    "        subject_data.append(record)\n",
    "    subj_df = pd.DataFrame(subject_data)\n",
    "    subj_df[f\"{outcome_col}_bin\"] = np.round(subj_df[outcome_col]).astype(int)\n",
    "    return subj_df, predictor_cols\n",
    "\n",
    "def get_stratified_cv_splits(df, subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5):\n",
    "    \"\"\"\n",
    "    Performs stratified K-fold cross validation at the subject level.\n",
    "    \n",
    "    Parameters:\n",
    "      df : pandas.DataFrame\n",
    "          The original dataframe containing repeated measures.\n",
    "      subject_id : str\n",
    "          The column name for the subject ID (e.g., \"PatientID\").\n",
    "      target_var : str\n",
    "          The target variable; for regression use \"SI_mean\" and for classification use \"is_SI\".\n",
    "      n_splits : int\n",
    "          Number of folds for cross validation.\n",
    "    \n",
    "    Returns:\n",
    "      splits : list of tuples\n",
    "          A list where each element is a tuple (train_df, test_df) corresponding\n",
    "          to one fold. Each dataframe contains all rows (i.e. repeated measures) for the patients in that fold.\n",
    "    \n",
    "    Behavior:\n",
    "      - Isolates unique patient IDs and their target variable by dropping duplicates.\n",
    "      - If target_var is \"SI_mean\", creates a new column \"SI_mean_levels\" (rounded SI_mean).\n",
    "      - Uses the resulting column as the stratification column.\n",
    "      - Performs stratified K-fold CV and then subsets the original dataframe based on the patient IDs.\n",
    "    \"\"\"\n",
    "    # Create a subject-level dataframe (unique patient IDs with their target variable)\n",
    "    subject_df = df[[subject_id, target_var]].drop_duplicates(subset=[subject_id]).copy()\n",
    "    \n",
    "    # For regression: create a new column with the rounded SI_mean values.\n",
    "    if target_var == \"SI_mean\":\n",
    "        subject_df[\"SI_mean_levels\"] = subject_df[target_var].round().astype(int)\n",
    "        strat_col = \"SI_mean_levels\"\n",
    "    else:\n",
    "        strat_col = target_var  # For classification, use the target directly.\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    splits = []\n",
    "    \n",
    "    # Get the subject IDs and stratification labels\n",
    "    subjects = subject_df[subject_id].values\n",
    "    strat_labels = subject_df[strat_col].values\n",
    "    \n",
    "    # For each fold, retrieve patient IDs and then subset the original dataframe.\n",
    "    for train_idx, test_idx in skf.split(subjects, strat_labels):\n",
    "        train_patient_ids = subject_df.iloc[train_idx][subject_id].values\n",
    "        test_patient_ids  = subject_df.iloc[test_idx][subject_id].values\n",
    "        train_split = df[df[subject_id].isin(train_patient_ids)]\n",
    "        test_split  = df[df[subject_id].isin(test_patient_ids)]\n",
    "        splits.append((train_split, test_split))\n",
    "    \n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e3e55-b2f5-4c53-b724-2c40ed00b6d9",
   "metadata": {},
   "source": [
    "### SI_mean regression CNN base learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a9050b3-de3c-4b47-afdc-65b7af17a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_learner_SI_mean(data_dict, model_name, num_epochs, batch_size, use_sample_weights=False):\n",
    "    \"\"\"\n",
    "    Trains an LSTM regression model to predict SI_mean.\n",
    "    Uses subject-level training and testing DataFrames provided in data_dict.\n",
    "    Each split is processed with create_subject_dataset (using outcome_col=\"SI_mean\")\n",
    "    so that each subject has an \"X\" array of shape (n_features, timepoints).\n",
    "    \n",
    "    If use_sample_weights=True, sample-level weights are used during training.\n",
    "    (Weights are not used during evaluation.)\n",
    "    \n",
    "    Also performs 5-fold stratified cross validation (based on PatientID and SI_mean_levels)\n",
    "    on the training set and returns the mean and standard deviation of the RMSE (both overall and per-bin).\n",
    "    Additionally, overall mean squared error (MSE) is computed and returned in a column named \"mse\".\n",
    "    \n",
    "    Returns two DataFrames:\n",
    "      1. metrics_df: with columns: model, data, \"1\", \"2\", \"3\", \"4\", \"5\", overall \n",
    "         for the full training and test splits.\n",
    "      2. cv_metrics_df: with columns: stat, model, \"1\", \"2\", \"3\", \"4\", \"5\", overall, mse.\n",
    "         There will be one row for the mean RMSE values (stat = \"mean\") and one row for the standard deviation\n",
    "         values (stat = \"sd\"), computed via 5-fold stratified cross validation.\n",
    "    \"\"\"\n",
    "    # Process subject-level training and testing sets.\n",
    "    train_df, _ = create_subject_dataset(data_dict['train'], outcome_col=\"SI_mean\")\n",
    "    test_df, _  = create_subject_dataset(data_dict['test'], outcome_col=\"SI_mean\")\n",
    "    \n",
    "    X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "    y_train = train_df[\"SI_mean\"].values\n",
    "    if use_sample_weights:\n",
    "        w_train = train_df[\"sample_weight\"].values\n",
    "    else:\n",
    "        w_train = np.ones_like(y_train, dtype=np.float32)\n",
    "    \n",
    "    X_test = np.stack(test_df[\"X\"].values, axis=0)\n",
    "    y_test = test_df[\"SI_mean\"].values\n",
    "    \n",
    "    input_size = X_train.shape[1]  # number of features\n",
    "    seq_len = X_train.shape[2]     # number of timepoints\n",
    "    \n",
    "    # --- Full Training on the Provided Training Set ---\n",
    "    model = LSTMRegression(input_size, seq_len).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).view(-1, 1),\n",
    "        torch.tensor(w_train, dtype=torch.float32)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    model.train()\n",
    "    for ep in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch, weight_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            weight_batch = weight_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss_per_sample = loss_fn(outputs, y_batch).view(-1)\n",
    "            loss = (loss_per_sample * weight_batch).mean() if use_sample_weights else loss_per_sample.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * X_batch.size(0)\n",
    "        print(f\"SI_mean - Epoch {ep+1}/{num_epochs}, Loss: {epoch_loss/len(train_dataset):.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        train_preds = model(X_train_tensor).cpu().numpy()\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        test_preds = model(X_test_tensor).cpu().numpy()\n",
    "        \n",
    "        overall_train_rmse = np.sqrt(np.mean((train_preds - y_train.reshape(-1, 1))**2))\n",
    "        overall_test_rmse  = np.sqrt(np.mean((test_preds - y_test.reshape(-1, 1))**2))\n",
    "    \n",
    "    # Calculate per-bin metrics for train and test sets.\n",
    "    train_bins = np.round(y_train).astype(int)\n",
    "    test_bins  = np.round(y_test).astype(int)\n",
    "    bin_rmse_train = {}\n",
    "    for b in range(1, 6):\n",
    "        idx = np.where(train_bins == b)[0]\n",
    "        bin_rmse_train[str(b)] = np.sqrt(np.mean((train_preds[idx] - y_train[idx].reshape(-1, 1))**2)) if len(idx) > 0 else np.nan\n",
    "    bin_rmse_test = {}\n",
    "    for b in range(1, 6):\n",
    "        idx = np.where(test_bins == b)[0]\n",
    "        bin_rmse_test[str(b)] = np.sqrt(np.mean((test_preds[idx] - y_test[idx].reshape(-1, 1))**2)) if len(idx) > 0 else np.nan\n",
    "    \n",
    "    train_metrics = {\"model\": model_name, \"data\": \"train\"}\n",
    "    for b in range(1, 6):\n",
    "        train_metrics[str(b)] = bin_rmse_train.get(str(b), np.nan)\n",
    "    train_metrics[\"overall\"] = overall_train_rmse\n",
    "    \n",
    "    test_metrics = {\"model\": model_name, \"data\": \"test\"}\n",
    "    for b in range(1, 6):\n",
    "        test_metrics[str(b)] = bin_rmse_test.get(str(b), np.nan)\n",
    "    test_metrics[\"overall\"] = overall_test_rmse\n",
    "    \n",
    "    metrics_df = pd.DataFrame([train_metrics, test_metrics])\n",
    "    \n",
    "    # --- 5-fold Stratified Cross Validation on the Training Set ---\n",
    "    cv_splits = get_stratified_cv_splits(train_df, subject_id=\"PatientID\", target_var=\"SI_mean\", n_splits=5)\n",
    "    cv_results = []\n",
    "    for cv_train_df, cv_val_df in cv_splits:\n",
    "        X_cv_train = np.stack(cv_train_df[\"X\"].values, axis=0)\n",
    "        y_cv_train = cv_train_df[\"SI_mean\"].values\n",
    "        if use_sample_weights:\n",
    "            w_cv_train = cv_train_df[\"sample_weight\"].values\n",
    "        else:\n",
    "            w_cv_train = np.ones_like(y_cv_train, dtype=np.float32)\n",
    "        X_cv_val   = np.stack(cv_val_df[\"X\"].values, axis=0)\n",
    "        y_cv_val   = cv_val_df[\"SI_mean\"].values\n",
    "        \n",
    "        model_cv = LSTMRegression(input_size, seq_len).to(device)\n",
    "        optimizer_cv = optim.Adam(model_cv.parameters(), lr=0.001)\n",
    "        train_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_train, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_train, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(w_cv_train, dtype=torch.float32)\n",
    "        )\n",
    "        train_loader_cv = torch.utils.data.DataLoader(train_dataset_cv, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        \n",
    "        model_cv.train()\n",
    "        for ep in range(num_epochs):\n",
    "            for X_batch, y_batch, weight_batch in train_loader_cv:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                weight_batch = weight_batch.to(device)\n",
    "                optimizer_cv.zero_grad()\n",
    "                outputs = model_cv(X_batch)\n",
    "                loss_per_sample = loss_fn(outputs, y_batch).view(-1)\n",
    "                loss = (loss_per_sample * weight_batch).mean() if use_sample_weights else loss_per_sample.mean()\n",
    "                loss.backward()\n",
    "                optimizer_cv.step()\n",
    "        model_cv.eval()\n",
    "        with torch.no_grad():\n",
    "            X_cv_val_tensor = torch.tensor(X_cv_val, dtype=torch.float32).to(device)\n",
    "            val_preds = model_cv(X_cv_val_tensor).cpu().numpy()\n",
    "        \n",
    "        # Compute overall RMSE and MSE for the fold.\n",
    "        overall_cv_rmse = np.sqrt(np.mean((val_preds - y_cv_val.reshape(-1, 1))**2))\n",
    "        overall_cv_mse  = np.mean((val_preds - y_cv_val.reshape(-1, 1))**2)\n",
    "        \n",
    "        cv_bin_rmse = {}\n",
    "        cv_bins = np.round(y_cv_val).astype(int)\n",
    "        for b in range(1, 6):\n",
    "            idx = np.where(cv_bins == b)[0]\n",
    "            cv_bin_rmse[str(b)] = np.sqrt(np.mean((val_preds[idx] - y_cv_val[idx].reshape(-1, 1))**2)) if len(idx) > 0 else np.nan\n",
    "        \n",
    "        cv_results.append({\n",
    "            \"overall\": overall_cv_rmse,\n",
    "            \"mse\": overall_cv_mse,   # New overall MSE metric\n",
    "            \"1\": cv_bin_rmse[\"1\"],\n",
    "            \"2\": cv_bin_rmse[\"2\"],\n",
    "            \"3\": cv_bin_rmse[\"3\"],\n",
    "            \"4\": cv_bin_rmse[\"4\"],\n",
    "            \"5\": cv_bin_rmse[\"5\"]\n",
    "        })\n",
    "    \n",
    "    cv_results_df = pd.DataFrame(cv_results)\n",
    "    cv_mean = cv_results_df.mean()\n",
    "    cv_std  = cv_results_df.std()\n",
    "\n",
    "    # Modified structure for cv_metrics_df:\n",
    "    # Two rows: one for the mean (\"stat\" = \"mean\") and one for the sd (\"stat\" = \"sd\")\n",
    "    mean_row = {\n",
    "        \"stat\": \"mean\",\n",
    "        \"model\": model_name,\n",
    "        \"1\": cv_mean[\"1\"],\n",
    "        \"2\": cv_mean[\"2\"],\n",
    "        \"3\": cv_mean[\"3\"],\n",
    "        \"4\": cv_mean[\"4\"],\n",
    "        \"5\": cv_mean[\"5\"],\n",
    "        \"overall\": cv_mean[\"overall\"],\n",
    "        \"mse\": cv_mean[\"mse\"]\n",
    "    }\n",
    "    sd_row = {\n",
    "        \"stat\": \"sd\",\n",
    "        \"model\": model_name,\n",
    "        \"1\": cv_std[\"1\"],\n",
    "        \"2\": cv_std[\"2\"],\n",
    "        \"3\": cv_std[\"3\"],\n",
    "        \"4\": cv_std[\"4\"],\n",
    "        \"5\": cv_std[\"5\"],\n",
    "        \"overall\": cv_std[\"overall\"],\n",
    "        \"mse\": cv_std[\"mse\"]\n",
    "    }\n",
    "    cv_metrics_df = pd.DataFrame([mean_row, sd_row])\n",
    "    \n",
    "    return metrics_df, cv_metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f82dd02-cf1c-45cb-b42e-fe11ebef58c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SI_mean - Epoch 1/10, Loss: 10.0719\n",
      "SI_mean - Epoch 2/10, Loss: 7.3971\n",
      "SI_mean - Epoch 3/10, Loss: 4.9292\n",
      "SI_mean - Epoch 4/10, Loss: 2.9679\n",
      "SI_mean - Epoch 5/10, Loss: 2.0076\n",
      "SI_mean - Epoch 6/10, Loss: 1.8832\n",
      "SI_mean - Epoch 7/10, Loss: 1.6193\n",
      "SI_mean - Epoch 8/10, Loss: 1.8980\n",
      "SI_mean - Epoch 9/10, Loss: 1.9266\n",
      "SI_mean - Epoch 10/10, Loss: 1.7592\n",
      "SI_mean - Epoch 1/10, Loss: 1.0879\n",
      "SI_mean - Epoch 2/10, Loss: 0.5766\n",
      "SI_mean - Epoch 3/10, Loss: 0.4997\n",
      "SI_mean - Epoch 4/10, Loss: 0.4726\n",
      "SI_mean - Epoch 5/10, Loss: 0.4523\n",
      "SI_mean - Epoch 6/10, Loss: 0.4501\n",
      "SI_mean - Epoch 7/10, Loss: 0.4415\n",
      "SI_mean - Epoch 8/10, Loss: 0.4362\n",
      "SI_mean - Epoch 9/10, Loss: 0.4078\n",
      "SI_mean - Epoch 10/10, Loss: 0.4112\n",
      "SI_mean - Epoch 1/10, Loss: 9.6233\n",
      "SI_mean - Epoch 2/10, Loss: 7.7155\n",
      "SI_mean - Epoch 3/10, Loss: 6.0528\n",
      "SI_mean - Epoch 4/10, Loss: 3.6338\n",
      "SI_mean - Epoch 5/10, Loss: 2.0604\n",
      "SI_mean - Epoch 6/10, Loss: 2.2771\n",
      "SI_mean - Epoch 7/10, Loss: 1.7369\n",
      "SI_mean - Epoch 8/10, Loss: 1.6627\n",
      "SI_mean - Epoch 9/10, Loss: 1.7799\n",
      "SI_mean - Epoch 10/10, Loss: 2.0356\n",
      "SI_mean - Epoch 1/10, Loss: 1.7470\n",
      "SI_mean - Epoch 2/10, Loss: 0.8171\n",
      "SI_mean - Epoch 3/10, Loss: 0.5517\n",
      "SI_mean - Epoch 4/10, Loss: 0.5216\n",
      "SI_mean - Epoch 5/10, Loss: 0.4932\n",
      "SI_mean - Epoch 6/10, Loss: 0.4700\n",
      "SI_mean - Epoch 7/10, Loss: 0.4746\n",
      "SI_mean - Epoch 8/10, Loss: 0.4670\n",
      "SI_mean - Epoch 9/10, Loss: 0.4618\n",
      "SI_mean - Epoch 10/10, Loss: 0.4347\n"
     ]
    }
   ],
   "source": [
    "comb_reg_base_w = base_learner_SI_mean(comb_reg_dict, num_epochs = 10, batch_size = 32, use_sample_weights=True, model_name=\"comb_base_weighted\")\n",
    "comb_reg_base_nw = base_learner_SI_mean(comb_reg_dict, num_epochs = 10, batch_size = 32, use_sample_weights=False, model_name=\"comb_base\")\n",
    "\n",
    "fitbit_reg_base_w = base_learner_SI_mean(fitbit_reg_dict, num_epochs = 10, batch_size = 32, use_sample_weights=True, model_name=\"fitbit_base_weighted\")\n",
    "fitbit_reg_base_nw = base_learner_SI_mean(fitbit_reg_dict, num_epochs = 10, batch_size = 32, use_sample_weights=False, model_name=\"fitbit_base\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dc7dcb-0cdf-4e08-81a3-dc0ef4d3042e",
   "metadata": {},
   "source": [
    "### Save base learner results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8df88d53-4382-481c-9cc9-299e4c116fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_base_results = pd.concat([comb_reg_base_w[0], comb_reg_base_nw[0], fitbit_reg_base_w[0], fitbit_reg_base_nw[0]], axis=0)\n",
    "reg_base_results.to_csv(\"results/reg_base_learner.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67debad-b70b-41e0-b16f-4c9e147972ae",
   "metadata": {},
   "source": [
    "### Save base learner val results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e12f7ab4-0c3d-41cb-aa1c-04b65362c56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_base_val_results = pd.concat([comb_reg_base_w[1], comb_reg_base_nw[1], fitbit_reg_base_w[1], fitbit_reg_base_nw[1]], axis=0)\n",
    "reg_base_val_results.to_csv(\"results/reg_base_val_learner.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4ed9c5-70c3-4904-89f6-5cdaca5f85fd",
   "metadata": {},
   "source": [
    "### Classification Model for base learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94605348-76d5-4152-8cf8-4859b635a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_learner_is_SI(data_dict, model_name, num_epochs, batch_size, use_sample_weights=False):\n",
    "    \"\"\"\n",
    "    Trains an LSTM classifier to predict is_SI.\n",
    "    Uses subject-level training and testing DataFrames provided in data_dict.\n",
    "    Each split is processed with create_subject_dataset (using outcome_col=\"is_SI\")\n",
    "    so that each subject has an \"X\" array of shape (n_features, timepoints).\n",
    "    \n",
    "    If use_sample_weights=True, sample-level weights are used during training\n",
    "    (weights are not used during evaluation).\n",
    "    \n",
    "    Also performs 5-fold stratified cross validation on the training set (stratified\n",
    "    by PatientID and the is_SI column) and returns the mean and standard deviation of\n",
    "    the performance metrics (accuracy, sensitivity, specificity, AUC) across folds.\n",
    "    \n",
    "    Returns two DataFrames:\n",
    "      1. metrics_df: with columns: model, data, accuracy, sensitivity, specificity, AUC,\n",
    "         computed for the full training and testing sets.\n",
    "      2. cv_metrics_df: with columns: stat, model, accuracy, sensitivity, specificity, AUC, bse.\n",
    "         There will be one row for the mean CV metrics (stat = \"mean\") and one row for their\n",
    "         standard deviation (stat = \"sd\").\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Prepare the Data ---\n",
    "    train_df, _ = create_subject_dataset(data_dict['train'], outcome_col=\"is_SI\")\n",
    "    test_df, _  = create_subject_dataset(data_dict['test'], outcome_col=\"is_SI\")\n",
    "    \n",
    "    X_train = np.stack(train_df[\"X\"].values, axis=0)\n",
    "    y_train = train_df[\"is_SI\"].values.astype(np.float32)\n",
    "    if use_sample_weights:\n",
    "        w_train = train_df[\"sample_weight\"].values\n",
    "    else:\n",
    "        w_train = np.ones_like(y_train, dtype=np.float32)\n",
    "        \n",
    "    X_test = np.stack(test_df[\"X\"].values, axis=0)\n",
    "    y_test = test_df[\"is_SI\"].values.astype(np.float32)\n",
    "    \n",
    "    input_size = X_train.shape[1]\n",
    "    seq_len = X_train.shape[2]\n",
    "    \n",
    "    # --- Full Training on the Provided Training Set ---\n",
    "    model = LSTMClassification(input_size, seq_len).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_train, dtype=torch.float32).view(-1, 1),\n",
    "        torch.tensor(w_train, dtype=torch.float32)\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                                 shuffle=True, num_workers=0)\n",
    "    \n",
    "    model.train()\n",
    "    for ep in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch, weight_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            weight_batch = weight_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss_per_sample = loss_fn(outputs, y_batch).view(-1)\n",
    "            loss = (loss_per_sample * weight_batch).mean() if use_sample_weights else loss_per_sample.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * X_batch.size(0)\n",
    "        print(f\"is_SI - Epoch {ep+1}/{num_epochs}, Loss: {epoch_loss/len(train_dataset):.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        train_logits = model(X_train_tensor)\n",
    "        train_probs = torch.sigmoid(train_logits).cpu().numpy()\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "        test_logits = model(X_test_tensor)\n",
    "        test_probs = torch.sigmoid(test_logits).cpu().numpy()\n",
    "        \n",
    "        train_preds = (train_probs >= 0.5).astype(np.float32)\n",
    "        test_preds = (test_probs >= 0.5).astype(np.float32)\n",
    "        \n",
    "        # Compute full-set accuracy, sensitivity, and specificity for training set.\n",
    "        overall_acc_train = np.mean(train_preds == y_train.reshape(-1, 1))\n",
    "        TP = np.sum((train_preds == 1) & (y_train.reshape(-1, 1) == 1))\n",
    "        FN = np.sum((train_preds == 0) & (y_train.reshape(-1, 1) == 1))\n",
    "        TN = np.sum((train_preds == 0) & (y_train.reshape(-1, 1) == 0))\n",
    "        FP = np.sum((train_preds == 1) & (y_train.reshape(-1, 1) == 0))\n",
    "        overall_sens_train = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "        overall_spec_train = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
    "\n",
    "        # Compute full-set accuracy, sensitivity, and specificity for testing set.\n",
    "        overall_acc_test = np.mean(test_preds == y_test.reshape(-1, 1))\n",
    "        TP = np.sum((test_preds == 1) & (y_test.reshape(-1, 1) == 1))\n",
    "        FN = np.sum((test_preds == 0) & (y_test.reshape(-1, 1) == 1))\n",
    "        TN = np.sum((test_preds == 0) & (y_test.reshape(-1, 1) == 0))\n",
    "        FP = np.sum((test_preds == 1) & (y_test.reshape(-1, 1) == 0))\n",
    "        overall_sens_test = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "        overall_spec_test = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
    "\n",
    "        # --- Compute AUC for full training and testing sets ---\n",
    "        try:\n",
    "            overall_auc_train = roc_auc_score(y_train, train_probs.flatten())\n",
    "        except ValueError:\n",
    "            overall_auc_train = np.nan\n",
    "        try:\n",
    "            overall_auc_test = roc_auc_score(y_test, test_probs.flatten())\n",
    "        except ValueError:\n",
    "            overall_auc_test = np.nan\n",
    "    \n",
    "    train_metrics = {\"model\": model_name, \"data\": \"train\",\n",
    "                     \"accuracy\": overall_acc_train,\n",
    "                     \"sensitivity\": overall_sens_train,\n",
    "                     \"specificity\": overall_spec_train,\n",
    "                     \"AUC\": overall_auc_train}\n",
    "    test_metrics = {\"model\": model_name, \"data\": \"test\",\n",
    "                    \"accuracy\": overall_acc_test,\n",
    "                    \"sensitivity\": overall_sens_test,\n",
    "                    \"specificity\": overall_spec_test,\n",
    "                    \"AUC\": overall_auc_test}\n",
    "    \n",
    "    metrics_df = pd.DataFrame([train_metrics, test_metrics])\n",
    "    \n",
    "    # --- 5-fold Stratified Cross Validation on the Training Set ---\n",
    "    cv_splits = get_stratified_cv_splits(train_df, subject_id=\"PatientID\", target_var=\"is_SI\", n_splits=5)\n",
    "    cv_fold_metrics = []\n",
    "    for cv_train_df, cv_val_df in cv_splits:\n",
    "        X_cv_train = np.stack(cv_train_df[\"X\"].values, axis=0)\n",
    "        y_cv_train = cv_train_df[\"is_SI\"].values.astype(np.float32)\n",
    "        if use_sample_weights:\n",
    "            w_cv_train = cv_train_df[\"sample_weight\"].values\n",
    "        else:\n",
    "            w_cv_train = np.ones_like(y_cv_train, dtype=np.float32)\n",
    "        X_cv_val   = np.stack(cv_val_df[\"X\"].values, axis=0)\n",
    "        y_cv_val   = cv_val_df[\"is_SI\"].values.astype(np.float32)\n",
    "        \n",
    "        model_cv = LSTMClassification(input_size, seq_len).to(device)\n",
    "        optimizer_cv = optim.Adam(model_cv.parameters(), lr=0.001)\n",
    "        train_dataset_cv = torch.utils.data.TensorDataset(\n",
    "            torch.tensor(X_cv_train, dtype=torch.float32),\n",
    "            torch.tensor(y_cv_train, dtype=torch.float32).view(-1, 1),\n",
    "            torch.tensor(w_cv_train, dtype=torch.float32)\n",
    "        )\n",
    "        train_loader_cv = torch.utils.data.DataLoader(train_dataset_cv, batch_size=batch_size,\n",
    "                                                      shuffle=True, num_workers=0)\n",
    "        \n",
    "        model_cv.train()\n",
    "        for ep in range(num_epochs):\n",
    "            for X_batch, y_batch, weight_batch in train_loader_cv:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                weight_batch = weight_batch.to(device)\n",
    "                optimizer_cv.zero_grad()\n",
    "                outputs = model_cv(X_batch)\n",
    "                loss_per_sample = loss_fn(outputs, y_batch).view(-1)\n",
    "                loss = (loss_per_sample * weight_batch).mean() if use_sample_weights else loss_per_sample.mean()\n",
    "                loss.backward()\n",
    "                optimizer_cv.step()\n",
    "        model_cv.eval()\n",
    "        with torch.no_grad():\n",
    "            X_cv_val_tensor = torch.tensor(X_cv_val, dtype=torch.float32).to(device)\n",
    "            cv_logits = model_cv(X_cv_val_tensor)\n",
    "            # --- Compute binary cross entropy (bse) on the CV validation set ---\n",
    "            cv_bce_loss_tensor = loss_fn(cv_logits, torch.tensor(y_cv_val, dtype=torch.float32).view(-1, 1).to(device))\n",
    "            cv_bse = cv_bce_loss_tensor.mean().item()\n",
    "            \n",
    "            cv_probs = torch.sigmoid(cv_logits).cpu().numpy()\n",
    "            cv_preds = (cv_probs >= 0.5).astype(np.float32)\n",
    "        \n",
    "        cv_acc = np.mean(cv_preds == y_cv_val.reshape(-1, 1))\n",
    "        TP = np.sum((cv_preds == 1) & (y_cv_val.reshape(-1, 1) == 1))\n",
    "        FN = np.sum((cv_preds == 0) & (y_cv_val.reshape(-1, 1) == 1))\n",
    "        TN = np.sum((cv_preds == 0) & (y_cv_val.reshape(-1, 1) == 0))\n",
    "        FP = np.sum((cv_preds == 1) & (y_cv_val.reshape(-1, 1) == 0))\n",
    "        cv_sens = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "        cv_spec = TN / (TN + FP) if (TN + FP) > 0 else np.nan\n",
    "        try:\n",
    "            cv_auc = roc_auc_score(y_cv_val, cv_probs.flatten())\n",
    "        except ValueError:\n",
    "            cv_auc = np.nan\n",
    "        \n",
    "        cv_fold_metrics.append({\n",
    "            \"accuracy\": cv_acc,\n",
    "            \"sensitivity\": cv_sens,\n",
    "            \"specificity\": cv_spec,\n",
    "            \"AUC\": cv_auc,\n",
    "            \"bse\": cv_bse\n",
    "        })\n",
    "    \n",
    "    cv_results_df = pd.DataFrame(cv_fold_metrics)\n",
    "    cv_mean = cv_results_df.mean()\n",
    "    cv_std  = cv_results_df.std()\n",
    "    \n",
    "    # Build the cross validation metrics DataFrame with two rows:\n",
    "    # one row for the mean metrics and one row for the standard deviation.\n",
    "    mean_row = {\n",
    "        \"stat\": \"mean\",\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": cv_mean[\"accuracy\"],\n",
    "        \"sensitivity\": cv_mean[\"sensitivity\"],\n",
    "        \"specificity\": cv_mean[\"specificity\"],\n",
    "        \"AUC\": cv_mean[\"AUC\"],\n",
    "        \"bse\": cv_mean[\"bse\"]\n",
    "    }\n",
    "    sd_row = {\n",
    "        \"stat\": \"sd\",\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": cv_std[\"accuracy\"],\n",
    "        \"sensitivity\": cv_std[\"sensitivity\"],\n",
    "        \"specificity\": cv_std[\"specificity\"],\n",
    "        \"AUC\": cv_std[\"AUC\"],\n",
    "        \"bse\": cv_std[\"bse\"]\n",
    "    }\n",
    "    cv_metrics_df = pd.DataFrame([mean_row, sd_row])\n",
    "    \n",
    "    return metrics_df, cv_metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c13b828-0a16-4332-9077-5ae1a012e101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_SI - Epoch 1/10, Loss: 0.6720\n",
      "is_SI - Epoch 2/10, Loss: 0.5074\n",
      "is_SI - Epoch 3/10, Loss: 0.3732\n",
      "is_SI - Epoch 4/10, Loss: 0.3023\n",
      "is_SI - Epoch 5/10, Loss: 0.2839\n",
      "is_SI - Epoch 6/10, Loss: 0.2682\n",
      "is_SI - Epoch 7/10, Loss: 0.2588\n",
      "is_SI - Epoch 8/10, Loss: 0.2731\n",
      "is_SI - Epoch 9/10, Loss: 0.2638\n",
      "is_SI - Epoch 10/10, Loss: 0.2611\n",
      "is_SI - Epoch 1/10, Loss: 0.6111\n",
      "is_SI - Epoch 2/10, Loss: 0.5508\n",
      "is_SI - Epoch 3/10, Loss: 0.5332\n",
      "is_SI - Epoch 4/10, Loss: 0.5321\n",
      "is_SI - Epoch 5/10, Loss: 0.5224\n",
      "is_SI - Epoch 6/10, Loss: 0.5106\n",
      "is_SI - Epoch 7/10, Loss: 0.5126\n",
      "is_SI - Epoch 8/10, Loss: 0.5059\n",
      "is_SI - Epoch 9/10, Loss: 0.5038\n",
      "is_SI - Epoch 10/10, Loss: 0.4927\n",
      "is_SI - Epoch 1/10, Loss: 0.5862\n",
      "is_SI - Epoch 2/10, Loss: 0.4545\n",
      "is_SI - Epoch 3/10, Loss: 0.3214\n",
      "is_SI - Epoch 4/10, Loss: 0.2866\n",
      "is_SI - Epoch 5/10, Loss: 0.2758\n",
      "is_SI - Epoch 6/10, Loss: 0.2760\n",
      "is_SI - Epoch 7/10, Loss: 0.2722\n",
      "is_SI - Epoch 8/10, Loss: 0.2624\n",
      "is_SI - Epoch 9/10, Loss: 0.2615\n",
      "is_SI - Epoch 10/10, Loss: 0.2604\n",
      "is_SI - Epoch 1/10, Loss: 0.7157\n",
      "is_SI - Epoch 2/10, Loss: 0.6132\n",
      "is_SI - Epoch 3/10, Loss: 0.5627\n",
      "is_SI - Epoch 4/10, Loss: 0.5425\n",
      "is_SI - Epoch 5/10, Loss: 0.5416\n",
      "is_SI - Epoch 6/10, Loss: 0.5334\n",
      "is_SI - Epoch 7/10, Loss: 0.5393\n",
      "is_SI - Epoch 8/10, Loss: 0.5339\n",
      "is_SI - Epoch 9/10, Loss: 0.5248\n",
      "is_SI - Epoch 10/10, Loss: 0.5257\n"
     ]
    }
   ],
   "source": [
    "comb_class_base_w = base_learner_is_SI(comb_class_dict, model_name=\"comb_base_weighted\", num_epochs=10, batch_size=32, use_sample_weights=True)\n",
    "comb_class_base_nw = base_learner_is_SI(comb_class_dict, model_name=\"comb_base\", num_epochs = 10, batch_size = 32, use_sample_weights=False)\n",
    "\n",
    "fitbit_class_base_w = base_learner_is_SI(fitbit_class_dict, model_name=\"fitbit_base_weighted\", num_epochs=10, batch_size=32, use_sample_weights=True)\n",
    "fitbit_class_base_nw = base_learner_is_SI(fitbit_class_dict, model_name=\"fitbit_base\", num_epochs = 10, batch_size = 32, use_sample_weights=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec85ad82-24b3-4313-b723-9a1f5647631c",
   "metadata": {},
   "source": [
    "### Save class base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5fd91fa-943f-4ec8-909f-75f7013798bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_base_results = pd.concat([comb_class_base_w[0], comb_class_base_nw[0], fitbit_class_base_w[0], fitbit_class_base_nw[0]], axis=0)\n",
    "class_base_results.to_csv(\"results/class_base_learner.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e561b-d98d-4878-a407-56c83d7afdf2",
   "metadata": {},
   "source": [
    "### Save Class val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87c5233c-fb3a-49a8-86a6-c2d19256a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_base_val_results = pd.concat([comb_class_base_w[1], comb_class_base_nw[1], fitbit_class_base_w[1], fitbit_class_base_nw[1]], axis=0)\n",
    "class_base_val_results.to_csv(\"results/class_base_val_learner.tsv\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL_py_env)",
   "language": "python",
   "name": "dl_py_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
